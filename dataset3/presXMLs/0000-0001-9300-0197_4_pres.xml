<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /mnt/sda1/Dissertation/grobid/grobid-0.5.6/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Multi-agent reinforcement learning</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2019-12-22">December 22, 2019</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Schnebli</forename><surname>Zoltan</surname></persName>
						</author>
						<title level="a" type="main">Multi-agent reinforcement learning</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2019-12-22">December 22, 2019</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.5.6" ident="GROBID" when="2019-12-24T15:31+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>		<profileDesc>
			<abstract/>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>RECURRENT NEURAL NETWORK IN NATURAL
LANGUAGE PROCESSING
.=</p>
<p>Sütő Evelyne 1
.=</p>
<p>1Babeş-Bolyai University, Computer Science, Cluj-Napoca.=</p>
</div>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>NATURAL LANGUAGE PROCESSING
.=</p>
<p>∙ Purpose: Present a model that is appropriate for sequence
generation and processing
.=</p>
<p>∙ Selected architecture: Recurrent neural network and more
precisely Long Short-Term memory network
.=</p>
<p>∙ Shortcoming of RNN architecture
.=</p>
<p>∙ Why LSTM?
.=</p>
<p>∙ How does LSTM work?
.=</p>
<p>∙ Improvements of LSTM over the years
.=</p>
<p>1.=</p>
</div>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>NEURAL NETWORKS
.=</p>
<p>Figure 1: The representation of a simple Artificial Neural Network with one
hidden layer. Image source:
https://en.wikipedia.org/wiki/Artificial_neural_network
.=</p>
<p>∑N
output = σ( (wi ∗ xi + bi)) (1)
.=</p>
<p>n=1
.=</p>
<p>2.=</p>
</div>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>RECURRENT NETWORK
.=</p>
<p>Figure 2: Recurrent Neural Network .
Image source: https://magenta.tensorflow.org
.=</p>
<p>ht = σ(Wxhxt +Whhht−1 + bh) (2)
.=</p>
<p>yt = Whyht + by (3)
3.=</p>
</div>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>BACKPROPAGATION
.=</p>
<p>4
.=</p>
<p>Figure 3: The backpropgation algorithm Image source:
https://en.wikipedia.org/wiki/Artificial_neural_network.=</p>
</div>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>LSTM
.=</p>
<p>5.=</p>
</div>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>LSTM
.=</p>
<p>Figure 4: Image source:
http:
//colah.github.io/posts/2015-08-Understanding-LSTMs/
.=</p>
<p>6.=</p>
</div>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>LSTM
.=</p>
<p>Figure 5: Image source:
http:
//colah.github.io/posts/2015-08-Understanding-LSTMs/
.=</p>
<p>7.=</p>
</div>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>LSTM
.=</p>
<p>Figure 6: Image source:
http:
//colah.github.io/posts/2015-08-Understanding-LSTMs/
.=</p>
<p>8.=</p>
</div>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>LSTM
.=</p>
<p>Figure 7: Image source:
http:
//colah.github.io/posts/2015-08-Understanding-LSTMs/
.=</p>
<p>9.=</p>
</div>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>VARIANTS OF LSTM
.=</p>
<p>∙ The first variation of LSTM introduced in 1997 included only input
and output gate withan internal state.
.=</p>
<p>∙ LSTM was not able to reset its own internal state. To solve this
the forget gate has been introduced.
.=</p>
<p>∙ Later this model was expanded with peephole connections,
connections from the cells to the gates, which can control the
gates in order to make precise timings easier to learn.
.=</p>
<p>∙ LSTM networks were improved further by replacing the mixture of
Real Time Recurrent Learning and Backpropagation Through
Time with full Backpropagation Trough Time training.
.=</p>
<p>∙ Generating new LSTM cells with evolutionary algorithms.
.=</p>
<p>∙ Hybrid networks e.g. LSTM with Convolutional networks.
.=</p>
<p>10.=</p>
</div>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>NATURAL LANGUAGE PROCESSING
.=</p>
<p>∙ Neural Machine translation
.=</p>
<p>∙ Text summarization
.=</p>
<p>∙ Speech to text
.=</p>
<p>∙ Paraphrase generation
.=</p>
<p>11.=</p>
</div>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>LSTM
.=</p>
<p>Figure 8: Image source:
https://medium.com/botsupply/generative-model-chatbots/
.=</p>
<p>12.=</p>
</div>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>DECODING ALGORITHMS
.=</p>
<p>Greedy Decoder
.=</p>
<p>Figure 9: Image source:
https://www.packtpub.com/mapt/book/big_data_and_
business_intelligence/ 13.=</p>
</div>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>ENCODER-DECODER (SEQUENCE TO SEQUENCE) MODEL
.=</p>
<p>Figure 10: Image source:
https://geekyisawesome.blogspot.com/2016/10/
.=</p>
<p>14.=</p>
</div>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>NEURAL MACHINE TRANSLATION
.=</p>
<p>∙ Researchers have applied this model to English-French
translation [Sutskever et al., 2014] in 2014
.=</p>
<p>∙ Conclusions
∙ LSTM sequence to sequence models are able to map even very long
sentences to the translation language
.=</p>
<p>∙ Deep LSTMs can significantly outperform shallow LSTMs
∙ Unoptimized model was able to produce state of the art results with
relatively short training
.=</p>
<p>∙ State of the art result: 37 BLEU score
∙ This model’s result: 36.5
.=</p>
<p>∙ Researchers improved this unoptimized version [Wu et al., 2016]
in 2016 to use for Google’s translation system
.=</p>
<p>15.=</p>
</div>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>SPEECH RECOGNITION
.=</p>
<p>∙ Another application is shown in [Graves and Jaitly, 2014] done in
2014 for speech recognition system.
.=</p>
<p>∙ Conclusions
∙ Bi-Directional LSTMs are needed to exploit future context as well.
∙ Connectionist Temporal Classification
∙ Model that does not need data preprocessing.
∙ Their best score on 81 hour dataset (Wall street journal) is 8.2 while
the baseline was 7.8 (word error rate/character error rate)
.=</p>
<p>16.=</p>
</div>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>TEXT SUMMARIZATION
.=</p>
<p>∙ Sequence to sequence model was used for text summarization in
[Nallapati et al., 2016] in 2016
.=</p>
<p>∙ Conclusions
∙ Encoder-Decoder model with a bi-directional GRU-RNN as the
encoder and a uni-directional GRU-RNN.
.=</p>
<p>∙ Address the problem of rarewords
∙ They have reached state of the art results.
.=</p>
<p>17.=</p>
</div>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>PARAPHRASE GENERATION
.=</p>
<p>∙ Paraphrasing, the act to express the same meaning in different
possible ways.
.=</p>
<p>∙ Paraphrase generation has been researched in
[Prakash et al., 2016] in 2016.
.=</p>
<p>∙ Conclusions
∙ Encoder-Decoder model with deep LSTM networks.
∙ They have reached state of the art results.
.=</p>
<p>18.=</p>
</div>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>CONCLUSION
.=</p>
<p>∙ Recurrent networks are ideal for sequence modeling.
.=</p>
<p>∙ LSTM is an appropriate approach when long term dependencies
have to be modeled.
.=</p>
<p>∙ LSTMs are still popular research topics.
.=</p>
<p>∙ LSTMs sometimes can be too complicated to train.
.=</p>
<p>∙ The literature is still looking for simpler architectures to replace
them e.g. 2D Convolutional networks.
.=</p>
<p>19.=</p>
</div>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Graves, A. and Jaitly, N. (2014).
Towards end-to-end speech recognition with recurrent neural
networks.
In International Conference on Machine Learning, pages
1764–1772.
Nallapati, R., Zhou, B., Gulcehre, C., Xiang, B., et al. (2016).
Abstractive text summarization using sequence-to-sequence
rnns and beyond.
arXiv preprint arXiv:1602.06023.
.=</p>
<p>Prakash, A., Hasan, S. A., Lee, K., Datla, V., Qadir, A., Liu, J., and
Farri, O. (2016).
Neural paraphrase generation with stacked residual lstm
networks.
arXiv preprint arXiv:1610.03098.
.=</p>
<p>Sutskever, I., Vinyals, O., and Le, Q. V. (2014).
Sequence to sequence learning with neural networks.
.=</p>
<p>19.=</p>
</div>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>In Advances in neural information processing systems, pages
3104–3112.
Wu, Y., Schuster, M., Chen, Z., Le, Q. V., Norouzi, M., Macherey, W.,
Krikun, M., Cao, Y., Gao, Q., Macherey, K., et al. (2016).
Google’s neural machine translation system: Bridging the gap
between human and machine translation.
arXiv preprint arXiv:1609.08144.
.=</p>
<p>19.=</p>
</div>
</body>
		<back>
			<div type="references">

				<listBibl>


				</listBibl>
			</div>
		</back>
	</text>
</TEI>
