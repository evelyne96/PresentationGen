<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /mnt/sda1/Dissertation/grobid/grobid-0.5.6/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">New Ensemble Methods For Evolving Data Streams</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Albert</forename><surname>Bifet</surname></persName>
							<email>abifet@lsi.upc.edu</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoff</forename><surname>Holmes</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernhard</forename><surname>Pfahringer</surname></persName>
							<email>bernhard@cs.waikato.ac.nz</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Kirkby</surname></persName>
							<email>rkirkby@cs.waikato.ac.nz</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ricard</forename><surname>Gavaldà</surname></persName>
							<email>gavalda@lsi.upc.edu</email>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">UPC-Barcelona Tech Barcelona</orgName>
								<orgName type="institution" key="instit2">University of Waikato</orgName>
								<address>
									<settlement>Hamilton</settlement>
									<region>Catalonia</region>
									<country key="NZ">New Zealand</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="institution">University of Waikato</orgName>
								<address>
									<settlement>Hamilton</settlement>
									<country key="NZ">New Zealand</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff2">
								<orgName type="institution">University of Waikato</orgName>
								<address>
									<settlement>Hamilton</settlement>
									<country key="NZ">New Zealand</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff3">
								<orgName type="institution">UPC-Barcelona Tech</orgName>
								<address>
									<settlement>Barcelona</settlement>
									<region>Catalonia</region>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">New Ensemble Methods For Evolving Data Streams</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.5.6" ident="GROBID" when="2019-12-24T15:32+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>		<profileDesc>
			<textClass>
				<keywords>
					<term>H28 [Database applications]: Database Applications- Data Mining General Terms Algorithms Keywords Data streams</term>
					<term>ensemble methods</term>
					<term>concept drift</term>
					<term>decision trees</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Advanced analysis of data streams is quickly becoming a key area of data mining research as the number of applications demanding such processing increases. Online mining when such data streams evolve over time, that is when concepts drift or change completely, is becoming one of the core issues. When tackling non-stationary concepts, ensembles of classifiers have several advantages over single classifier methods: they are easy to scale and parallelize, they can adapt to change quickly by pruning under-performing parts of the ensemble, and they therefore usually also generate more accurate concept descriptions. This paper proposes a new experimental data stream framework for studying concept drift, and two new variants of Bagging: ADWIN Bagging and Adaptive-Size Hoeffding Tree (ASHT) Bagging. Using the new experimental framework, an evaluation study on synthetic and real-world datasets comprising up to ten million examples shows that the new ensemble methods perform very well compared to several known methods.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">INTRODUCTION</head><p>Conventional knowledge discovery tools assume that the volume of data is such that we can store all data in memory Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. To copy otherwise, to republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. or local secondary storage, and there is no limitation on processing time. In the Data Stream model, we have space and time restrictions. Examples of data streams are sensoring video streams, network event logs, telephone call records, credit card transactional flows, etc. An important fact is that data may be evolving over time, so we need methods that adapt automatically. Under the constraints of the Data Stream model, the main properties of an ideal classification method are the following: high accuracy and fast adaption to change, low computational cost in both space and time, theoretical performance guarantees, and minimal number of parameters.</p><p>These properties may be interdependent: adjusting the time and space used by an algorithm can influence accuracy. By storing more pre-computed information, such as look up tables, an algorithm can run faster at the expense of space. An algorithm can also run faster by processing less information, either by stopping early or storing less, thus having less data to process. The more time an algorithm has, the more likely it is that accuracy can be increased.</p><p>Ensemble methods are combinations of several models whose individual predictions are combined in some manner (e.g., averaging or voting) to form a final prediction. Ensemble learning classifiers often have better accuracy and they are easier to scale and parallelize than single classifier methods.</p><p>A majority of concept drift research in data streams mining is done using traditional data mining frameworks such as WEKA <ref type="bibr" target="#b25">[26]</ref>. As the data stream setting has constraints that a traditional data mining environment does not, we believe that a new framework is needed to help to improve the empirical evaluation of these methods.</p><p>We present in Section 2 a novel framework for evaluation of concept drift. Sections 3 and 4 present two novel ensemble methods for handling concept drift, and Section 5 shows a first comprehensive cross-method comparison. We present conclusions in Section 6. Source code and datasets will be made available at http://sourceforge.net/projects/moadatastream.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">EXPERIMENTAL FRAMEWORK FOR CONCEPT DRIFT</head><p>A data stream environment has different requirements from the traditional setting <ref type="bibr" target="#b14">[15]</ref>. The most significant are the following: Requirement 1 Process an example at a time, and inspect We have to consider these requirements in order to design a new experimental framework for data streams. <ref type="figure" target="#fig_1">Figure 1</ref> illustrates the typical use of a data stream classification algorithm, and how the requirements fit in a repeating cycle:</p><p>1. The algorithm is passed the next available example from the stream (requirement 1).</p><p>2. The algorithm processes the example, updating its data structures. It does so without exceeding the memory bounds set on it (requirement 2), and as quickly as possible (requirement 3).</p><p>3. The algorithm is ready to accept the next example. On request it is able to predict the class of unseen examples (requirement 4).</p><p>In traditional batch learning the problem of limited data is overcome by analyzing and averaging multiple models produced with different random arrangements of training and test data. In the stream setting the problem of (effectively) unlimited data poses different challenges. One solution involves taking snapshots at different times during the induction of a model to see how much the model improves.</p><p>The evaluation procedure of a learning algorithm determines which examples are used for training the algorithm, and which are used to test the model output by the algorithm. The procedure used historically in batch learning has partly depended on data size. As data sizes increase, practical time limitations prevent procedures that repeat training too many times. It is commonly accepted with considerably larger data sources that it is necessary to reduce the numbers of repetitions or folds to allow experiments to complete in reasonable time. When considering what procedure to use in the data stream setting, one of the unique concerns is how to build a picture of accuracy over time. Two main approaches arise:</p><p>• Holdout: When traditional batch learning reaches a scale where cross-validation is too time consuming, it is often accepted to instead measure performance on a single holdout set. This is most useful when the division between train and test sets have been predefined, so that results from different studies can be directly compared.</p><p>• Interleaved Test-Then-Train: Each individual example can be used to test the model before it is used for training, and from this the accuracy can be incrementally updated <ref type="bibr" target="#b14">[15]</ref>. When intentionally performed in this order, the model is always being tested on examples it has not seen. This scheme has the advantage that no holdout set is needed for testing, making maximum use of the available data. It also ensures a smooth plot of accuracy over time, as each individual example will become increasingly less significant to the overall average.</p><p>As data stream classification is a relatively new field, such evaluation practices are not nearly as well researched and established as they are in the traditional batch setting. The majority of experimental evaluations use less than one million training examples. Some papers use more than this, up to ten million examples, and only very rarely is there any study like Domingos and Hulten <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b13">14]</ref> that is in the order of tens of millions of examples. In the context of data streams this is disappointing, because to be truly useful at data stream classification the algorithms need to be capable of handling very large (potentially infinite) streams of examples. Demonstrating systems only on small amounts of data does not build a convincing case for capacity to solve more demanding data stream applications. A claim of this paper is that in order to adequately evaluate data stream classification algorithms they need to be tested on large streams, in the order of tens of millions of examples where possible, and under explicit memory limits. Any less than this does not actually test algorithms in a realistically challenging setting.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Concept Drift Framework</head><p>We present a new experimental framework for concept drift. Our goal is to introduce artificial drift to data stream generators in a straightforward way.</p><p>The framework approach most similar to the one presented in this paper is the one proposed by Narasimhamurthy et al. <ref type="bibr" target="#b17">[18]</ref>. They proposed a general framework to generate data simulating changing environments. Their framework accommodates the STAGGER and Moving Hyperplane generation strategies. They consider a set of k data sources with known distributions. As these distributions at the sources are fixed, the data distribution at time t, D (t) is specified through vi(t), where vi(t) ∈ [0, 1] specify the extent of the influence of data source i at time t:</p><formula xml:id="formula_0">D (t) = {v1(t), v2(t), . . . , v k (t)}, X i vi(t) = 1</formula><p>Their framework covers gradual and abrupt changes. Our approach is more concrete, we begin by dealing with a simple scenario: a data stream and two different concepts. Later, we will consider the general case with more than one concept drift events. Considering data streams as data generated from pure distributions, we can model a concept drift event as a weighted combination of two pure distributions that characterizes the target concepts before and after the drift. In our framework, we need to define the probability that every new instance of the stream belongs to the new concept after the drift. We will use the sigmoid function, as an elegant and practical solution.</p><p>We see from <ref type="figure" target="#fig_2">Figure 2</ref> that the sigmoid function</p><formula xml:id="formula_1">f (t) = 1/(1 + e −s(t−t 0 ) )</formula><p>has a derivative at the point t0 equal to f (t0) = s/4. The tangent of angle α is equal to this derivative, tan α = s/4. We observe that tan α = 1/W , and as s = 4 tan α then s = 4/W . So the parameter s in the sigmoid gives the length of W and the angle α. In this sigmoid model we only need to specify two parameters : t0 the point of change, and W the length of change. Note that for any positive real number β</p><formula xml:id="formula_2">f (t0 + β · W ) = 1 − f (t0 − β · W ),</formula><p>and that f (t0 + β · W ) and f (t0 − β · W ) are constant values that don't depend on t0 and W :</p><formula xml:id="formula_3">f (t0 + W/2) = 1 − f (t0 − W/2) = 1/(1 + e −2 ) ≈ 88.08% f (t0 + W ) = 1 − f (t0 − W ) = 1/(1 + e −4 ) ≈ 98.20% f (t0 + 2W ) = 1 − f (t0 − 2W ) = 1/(1 + e −8 ) ≈ 99.97%</formula><p>Definition 1. Given two data streams a, b, we define c = a ⊕ W t 0 b as the data stream built joining the two data streams a and b, where t0 is the point of change, W is the length of change and We observe the following properties, if a = b:</p><formula xml:id="formula_4">• a ⊕ W t 0 b = b ⊕ W t 0 a • a ⊕ W t 0 a = a • a ⊕ 0 0 b = b • a ⊕ W t 0 (b ⊕ W t 0 c) = (a ⊕ W t 0 b) ⊕ W t 0 c • a ⊕ W t 0 (b ⊕ W t 1 c) ≈ (a ⊕ W t 0 b) ⊕ W t 1 c if t0 &lt; t1 and W |t1 − t0|</formula><p>In order to create a data stream with multiple concept changes, we can build new data streams joining different concept drifts:</p><formula xml:id="formula_5">(((a ⊕ W 0 t 0 b) ⊕ W 1 t 1 c) ⊕ W 2 t 2 d) . . .</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Datasets for concept drift</head><p>Synthetic data has several advantages -it is easier to reproduce and there is little cost in terms of storage and transmission. For this paper and framework, the data generators most commonly found in the literature have been collected.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>SEA Concepts Generator This artificial dataset contains</head><p>abrupt concept drift, first introduced in <ref type="bibr" target="#b24">[25]</ref>. It is generated using three attributes, where only the two first attributes are relevant. All three attributes have values between 0 and 10. The points of the dataset are divided into 4 blocks with different concepts. In each block, the classification is done using f1+f2 ≤ θ, where f1 and f2 represent the first two attributes and θ is a threshold value. The most frequent values are 9, 8, 7 and 9.5 for the data blocks. In our framework, SEA concepts are defined as follows:</p><formula xml:id="formula_6">(((SEA9 ⊕ W t 0 SEA8) ⊕ W 2t 0 SEA7) ⊕ W 3t 0 SEA9.5)</formula><p>STAGGER Concepts Generator They were introduced by Schlimmer and Granger in <ref type="bibr" target="#b22">[23]</ref>. The STAGGER Concepts are boolean functions of three attributes encoding objects: size (small, medium, and large), shape (circle, triangle, and rectangle), and colour (red,blue, and green). A concept description covering either green rectangles or red triangles is represented by (shape= rectangle and colour=green) or (shape=triangle and colour=red).</p><p>Rotating Hyperplane It was used as testbed for CVFDT versus VFDT in <ref type="bibr" target="#b13">[14]</ref>.</p><formula xml:id="formula_7">A hyperplane in d-dimensional space is the set of points x that satisfy d X i=1 wixi = w0 = d X i=1 wi</formula><p>where xi, is the ith coordinate of x. Examples for which P d i=1 wixi ≥ w0 are labeled positive, and examples for which P d i=1 wixi &lt; w0 are labeled negative. Hyperplanes are useful for simulating time-changing concepts, because we can change the orientation and position of the hyperplane in a smooth manner by changing the relative size of the weights. We introduce change to this dataset adding drift to each weight attribute wi = wi + dσ, where σ is the probability that the direction of change is reversed and d is the change applied to every example.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Random RBF Generator This generator was devised to</head><p>offer an alternate complex concept type that is not straightforward to approximate with a decision tree model. The RBF (Radial Basis Function) generator works as follows: A fixed number of random centroids are generated. Each center has a random position, a single standard deviation, class label and weight. New examples are generated by selecting a center at random, taking weights into consideration so that centers with higher weight are more likely to be chosen. A random direction is chosen to offset the attribute values from the central point. The length of the displacement is randomly drawn from a Gaussian distribution with standard deviation determined by the chosen centroid. The chosen centroid also determines the class label of the example. This effectively creates a normally distributed hypersphere of examples surrounding each central point with varying densities. Only numeric attributes are generated. Drift is introduced by moving the centroids with constant speed. This speed is initialized by a drift parameter.</p><p>LED Generator This data source originates from the CART book <ref type="bibr" target="#b5">[6]</ref>. An implementation in C was donated to the UCI <ref type="bibr" target="#b2">[3]</ref> machine learning repository by David Aha. The goal is to predict the digit displayed on a sevensegment LED display, where each attribute has a 10% chance of being inverted. It has an optimal Bayes classification rate of 74%. The particular configuration of the generator used for experiments (led) produces 24 binary attributes, 17 of which are irrelevant.</p><p>Waveform Generator It shares its origins with LED, and was also donated by David Aha to the UCI repository. The goal of the task is to differentiate between three different classes of waveform, each of which is generated from a combination of two or three base waves. The optimal Bayes classification rate is known to be 86%. There are two versions of the problem, wave21 which has 21 numeric attributes, all of which include noise, and wave40 which introduces an additional 19 irrelevant attributes.</p><p>Function Generator It was introduced by Agrawal et al. in <ref type="bibr" target="#b0">[1]</ref>, and was a common source of data for early work on scaling up decision tree learners <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b10">11]</ref>. The generator produces a stream containing nine attributes, six numeric and three categorical. Although not explicitly stated by the authors, a sensible conclusion is that these attributes describe hypothetical loan applications. There are ten functions defined for generating binary class labels from the attributes. Presumably these determine whether the loan should be approved.</p><p>Data streams may be considered infinite sequences of (x, y) where x is the feature vector and y the class label. Zhang et al. <ref type="bibr" target="#b27">[27]</ref> observe that p(x, y) = p(x|t) · p(y|x) and categorize concept drift in two types:</p><p>• Loose Concept Drifting (LCD) when concept drift is caused only by the change of the class prior probability p(y|x),</p><p>• Rigorous Concept Drifting (RCD) when concept drift is caused by the change of the class prior probability p(y|x) and the conditional probability p(x|t)</p><p>Note that the Random RBF Generator has RCD drift, and the rest of the dataset generators have LCD drift.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.1">Real-World Data</head><p>It is not easy to find large real-world datasets for public benchmarking, especially with substantial concept change. The UCI machine learning repository <ref type="bibr" target="#b2">[3]</ref> contains some realworld benchmark data for evaluating machine learning techniques. We will consider three : Forest Covertype, Poker-Hand, and Electricity.</p><p>Forest Covertype dataset It contains the forest cover type for 30 x 30 meter cells obtained from US Forest Service (USFS) Region 2 Resource Information System (RIS) data. It contains 581, 012 instances and 54 attributes, and it has been used in several papers on data stream classification <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b19">20]</ref>.</p><p>Poker-Hand dataset It consists of 1, 000, 000 instances and 11 attributes. Each record of the Poker-Hand dataset is an example of a hand consisting of five playing cards drawn from a standard deck of 52. Each card is described using two attributes (suit and rank), for a total of 10 predictive attributes. There is one Class attribute that describes the "Poker Hand". The order of cards is important, which is why there are 480 possible Royal Flush hands instead of 4.</p><p>Electricity dataset Another widely used dataset is the Electricity Market Dataset described by M. Harries <ref type="bibr" target="#b11">[12]</ref> and used by Gama <ref type="bibr" target="#b8">[9]</ref>. This data was collected from the Australian New South Wales Electricity Market. In this market, the prices are not fixed and are affected by demand and supply of the market. The prices in this market are set every five minutes. As all examples need to have the same number of attributes, we simple concatenate all the attributes, and we set a number of classes that is the maximum number of classes of all the datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">NEW METHOD OF BAGGING USING TREES OF DIFFERENT SIZE</head><p>In this section, we present a new method of bagging using Hoeffding Trees of different sizes.</p><p>A Hoeffding tree <ref type="bibr" target="#b7">[8]</ref> is an incremental, anytime decision tree induction algorithm that is capable of learning from massive data streams, assuming that the distribution generating examples does not change over time. Hoeffding trees exploit the fact that a small sample can often be enough to choose an optimal splitting attribute. This idea is supported mathematically by the Hoeffding bound, which quantifies the number of observations (in our case, examples) needed to estimate some statistics within a prescribed precision (in our case, the goodness of an attribute). More precisely, the Hoeffding bound states that with probability 1 − δ, the true mean of a random variable of range R will not differ from the estimated mean after n independent observations by more A theoretically appealing feature of Hoeffding Trees not shared by other incremental decision tree learners is that it has sound guarantees of performance. Using the Hoeffding bound one can show that its output is asymptotically nearly identical to that of a non-incremental learner using infinitely many examples. See <ref type="bibr" target="#b7">[8]</ref> for details. In this paper, we introduce the Adaptive-Size Hoeffding Tree (ASHT). It is derived from the Hoeffding Tree algorithm with the following differences:</p><p>• it has a maximum number of split nodes, or size • after one node splits, if the number of split nodes of the ASHT tree is higher than the maximum value, then it deletes some nodes to reduce its size</p><p>The intuition behind this method is as follows: smaller trees adapt more quickly to changes, and larger trees do better during periods with no or little change, simply because they were built on more data. Trees limited to size s will be reset about twice as often as trees with a size limit of 2s. This creates a set of different reset-speeds for an ensemble of such trees, and therefore a subset of trees that are a good approximation for the current rate of change. It is important to note that resets will happen all the time, even for stationary datasets, but this behaviour should not have a negative impact on the ensemble's predictive performance.</p><p>When the tree size exceeds the maximun size value, there are two different delete options:</p><p>• delete the oldest node, the root, and all of its children except the one where the split has been made. After that, the root of the child not deleted becomes the new root</p><p>• delete all the nodes of the tree, i.e., restart from a new root.</p><p>We present a new bagging method that uses these Adaptive-Size Hoeffding Trees and that sets the size for each tree. The maximum allowed size for the n-th ASHT tree is twice the maximum allowed size for the (n−1)-th tree. Moreover, each tree has a weight proportional to the inverse of the square of its error, and it monitors its error with an exponential weighted moving average (EWMA) with α = .01. The size of the first tree is 2.</p><p>With this new method, we attempt to improve bagging performance by increasing tree diversity. It has been observed that boosting tends to produce a more diverse set of classifiers than bagging, and this has been cited as a factor in increased performance <ref type="bibr" target="#b15">[16]</ref>.</p><p>We use the Kappa statistic κ to show how using trees of different size, we increase the diversity of the ensemble. Let </p><formula xml:id="formula_8">P L i=1 Cii m Θ2 = L X i=1 L X j=1 Cij m · L X j=1</formula><p>Cji m ! We could use Θ1 as a measure of agreement, but in problems where one class is much more common than others, all classifiers will agree by chance, so all pair of classifiers will obtain high values for Θ1. To correct this, the κ statistic is defined as follows:</p><formula xml:id="formula_9">κ = Θ1 − Θ2 1 − Θ2</formula><p>κ uses Θ2, the probability that two classifiers agree by chance, given the observed counts in the table. If two classifiers agree on every example then κ = 1, and if their predictions coincide purely by chance, then κ = 0. We use the Kappa-Error diagram to compare the diversity of normal bagging with bagging using trees of different size. The Kappa-Error diagram is a scatterplot where each point corresponds to a pair of classifiers. The x coordinate of the pair is the κ value for the two classifiers. The y coordinate is the average of the error rates of the two classifiers. <ref type="figure" target="#fig_4">Figure 3</ref> shows the Kappa-Error diagram for the Random RBF dataset with drift parameter or change speed equal to 0.001.We observe that bagging classifiers are very similar to one another and that the decision tree classifiers of different size are very diferent from one another.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">NEW METHOD OF BAGGING USING ADWIN</head><p>ADWIN <ref type="bibr" target="#b4">[5]</ref> is a change detector and estimator that solves in a well-specified way the problem of tracking the average of a stream of bits or real-valued numbers. ADWIN keeps a variable-length window of recently seen items, with the property that the window has the maximal length statistically consistent with the hypothesis "there has been no change in the average value inside the window".</p><p>ADWIN is parameter-and assumption-free in the sense that it automatically detects and adapts to the current rate of change. Its only parameter is a confidence bound δ, indicating how confident we want to be in the algorithm's output, inherent to all algorithms dealing with random processes.</p><p>Also important for our purposes, ADWIN does not maintain the window explicitly, but compresses it using a variant of the exponential histogram technique. This means that it keeps a window of length W using only O(log W ) memory and O(log W ) processing time per item.</p><p>ADWIN Bagging is the online bagging method of Oza and Rusell <ref type="bibr" target="#b18">[19]</ref> with the addition of the ADWIN algorithm as a change detector and as an estimator for the weights of the boosting method. When a change is detected, the worst classifier of the ensemble of classifiers is removed and a new classifier is added to the ensemble.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">COMPARATIVE EXPERIMENTAL EVALUATION</head><p>Massive Online Analysis (MOA) <ref type="bibr" target="#b12">[13]</ref> is a software environment for implementing algorithms and running experiments for online learning from data streams. The data stream evaluation framework and all algorithms evaluated in this paper were implemented in the Java programming language extending the MOA software. MOA includes a collection of offline and online methods as well as tools for evaluation. In particular, it implements boosting, bagging, and Hoeffding Trees, all with and without Naïve Bayes classifiers at the leaves.</p><p>One of the key data structures used in MOA is the description of an example from a data stream. This structure borrows from WEKA, where an example is represented by an array of double precision floating point values. This provides freedom to store all necessary types of value -numeric attribute values can be stored directly, and discrete attribute values and class labels are represented by integer index values that are stored as floating point values in the array. Double precision floating point values require storage space of 64 bits, or 8 bytes. This detail can have implications for memory usage.</p><p>We compare the following methods: Hoeffding Option Trees, bagging and boosting, and DDM. We review them and their main properties briefly.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Bagging and Boosting</head><p>Bagging and Boosting are two of the best known ensemble learning algorithms. In <ref type="bibr" target="#b18">[19]</ref> Oza and Russell developed online versions of bagging and boosting for Data Streams. They show how the process of sampling bootstrap replicates from training data can be simulated in a data stream context. They observe that the probability that any individual example will be chosen for a replicate tends to a Poisson(1) distribution.</p><p>For the boosting method, Oza and Russell note that the weighting procedure of AdaBoost actually divides the total example weight into two halves -half of the weight is assigned to the correctly classified examples, and the other half goes to the misclassified examples. They use the Poisson distribution for deciding the random probability that an example is used for training, only this time the parameter changes according to the boosting weight of the example as it is passed through each model in sequence.</p><p>Pelossof et al. presented in <ref type="bibr" target="#b20">[21]</ref> Online Coordinate Boost-  ing, a new online boosting algorithm for adapting the weights of a boosted classifier, which yields a closer approximation to Freund and Schapire's AdaBoost algorithm. The weight update procedure is derived by minimizing AdaBoost's loss when viewed in an incremental form. This boosting method may be reduced to a form similar to Oza and Russell's algorithm.</p><p>Chu and Zaniolo proposed in <ref type="bibr" target="#b6">[7]</ref> Fast and Light Boosting for adaptive mining of data streams. It is based on a dynamic sample-weight assignment scheme that is extended to handle concept drift via change detection. The change detection approach aims at significant data changes that could cause serious deterioration of the ensemble performance, and replaces the obsolete ensemble with one built from scratch.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Adaptive Hoeffding Option Trees</head><p>Hoeffding Option Trees <ref type="bibr" target="#b21">[22]</ref> are regular Hoeffding trees containing additional option nodes that allow several tests to be applied, leading to multiple Hoeffding trees as separate paths. They consist of a single structure that efficiently represents multiple trees. A particular example can travel down multiple paths of the tree, contributing, in different ways, to different options.</p><p>An Adaptive Hoeffding Option Tree is a Hoeffding Option Tree with the following improvement: each leaf stores an estimation of the current error. It uses an EWMA estimator with α = .2. The weight of each node in the voting process is proportional to the square of the inverse of the error.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Drift Detection Method</head><p>The drift detection method (DDM) proposed by Gama et al. <ref type="bibr" target="#b8">[9]</ref> controls the number of errors produced by the learning model during prediction. It compares the statistics of two windows: the first one contains all the data, and the second one contains only the data from the beginning until the number of errors increases. Their method doesn't store these windows in memory. It keeps only statistics and a window of recent errors.</p><p>They consider that the number of errors in a sample of examples is modeled by a binomial distribution. A significant increase in the error of the algorithm, suggests that the class distribution is changing and, hence, the actual decision model is supposed to be inappropriate. They check for a warning level and a drift level. Beyond these levels, change of context is considered.</p><p>Baena-García et al. proposed a new method EDDM <ref type="bibr" target="#b3">[4]</ref> in order to improve DDM. It is based on the estimated distribution of the distances between classification errors. The window resize procedure is governed by the same heuristics.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4">Results</head><p>We use a variety of datasets for evaluation, as explained in Section 2.2. The experiments were performed on a 2.0 GHz Intel Core Duo PC machine with 2 Gigabyte main memory, running Ubuntu 8.10. The evaluation methodology used was Interleaved Test-Then-Train: every example was used for testing the model before using it to train. This interleaved test followed by train procedure was carried out on 10 million examples from the hyperplane and Random-RBF datasets, and one million examples from the LED and SEA datasets. <ref type="table" target="#tab_2">Tables 1 and 2</ref> reports the final accuracy, and speed of the classification models induced on synthetic data. <ref type="table">Table 3</ref> shows the results for real datasets: Forest CoverType, Poker Hand, Electricity and CovPokElec. Additionally, the learning curves and model growth curves for LED dataset are plotted <ref type="figure">(Figure 4</ref>). For some datasets the differences in accuracy, as seen in <ref type="table" target="#tab_2">Tables 1, 2 and 3, are</ref> marginal.</p><p>The first, and baseline, algorithm (HT) is a single Hoeffding tree, enhanced with adaptive Naive Bayes leaf predictions. Parameter settings are nmin = 1000, δ = 10 −8 and τ = 0.05, used in <ref type="bibr" target="#b7">[8]</ref>. The HT DDM and HT EDDM are Hoeffding Trees with drift detection methods as explained in Section 5.3. HOT, is the Hoeffding option tree algorithm, restricted to a maximum of five option paths (HOT5) or fifty option paths (HOT50). AdaHOT is the Adaptive Hoeffding   <ref type="figure">Figure 4</ref>: Accuracy and size on dataset LED with three concept drifts.</p><p>Tree explained in Section 5.2. Bag10 is Oza and Russell online bagging using ten classifiers and Bag5 only five. BagADWIN is the online bagging version using ADWIN explained in Section 4. We implemented the following variants of bagging with Hoeffding trees of different size (ASHT):</p><p>• Bag ASHT is the base method, which deletes its root node and all its children except the one where the last split occurred,</p><p>• Bag ASHT W uses weighted classifiers,</p><p>• Bag ASHT R replaces oversized trees with new ones,</p><p>• Bag ASHT W+R uses both weighted classifiers and replaces oversized trees with new ones.</p><p>And finally, we tested three methods of boosting: Oza Boosting, Online Coordinate Boosting, and Fast and Light Boosting. The parameters used in the experimental evaluation were found to work well across a range of problems during the PhD of the first author. Bagging is clearly the best method in terms of accuracy. This superior position is, however, achieved at high cost in terms of memory and time. ADWIN Bagging and ASHT Bagging are the most accurate methods for most datasets, but they are slow. ADWIN Bagging is slower than ASHT Bagging and for some datasets it needs more memory. ASHT Bagging using weighted classifiers and replacing oversized trees with new ones seems to be the most accurate ASHT bagging method. We observe that bagging using 5 trees of different size may be sufficient, as its error is not much higher than for 10 trees, but it is nearly twice as fast. Also Hoeffding trees using drift detection methods are faster but less accurate methods.</p><p>In <ref type="bibr" target="#b21">[22]</ref>, a range of option limits were tested and averaged across all datasets without concept drift to determine the optimal number of paths. This optimal number of options was five. Dealing with concept drift, we observe that increasing the number of options to 50, we obtain a significant improvement in accuracy for some datasets.</p><p>A summary of the best results from the synthetic and real datasets in Tables 1-3 show that of the two novel methods presented here Bag10 ASHT W+R wins five times, BagADWIN 10 HT wins four times, and Bag10 HT, OzaBoost, and OC-Boost win once each. This confirms that the variants proposed in this paper are superior across this collection of datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">CONCLUSION</head><p>Our goal is to build an experimental framework for data streams similar to the WEKA framework, so that it will be easy for researchers to run experimental data stream benchmarks. New bagging methods were presented: ASHT Bagging using trees of different sizes, and ADWIN Bagging using a change detector to decide when to discard underperforming ensemble members. These methods compared favorably in a comprehensive cross-method comparison. Data stream evaluation is fundamentally three-dimensional. These comparisons, given your specific resource limitations, indicate the method of preference. For example, on the SEA Concepts and Forest Covertype datasets the best performing method across all three dimensions are arguably HT DDM and HT EDDM, as they are almost the fastest, and almost the most accurate and, by at least an order of magnitude, easily the most memory-efficient methods. On the other hand, if both runtime and memory consumption are less of a concern, then variants of bagging usually produce excellent accuracies.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.">ACKNOWLEDGMENTS</head><p>Partially supported by the EU PASCAL2 Network of Excellence (FP7-ICT-216886), and by projects SESAAME-BAR (TIN2008-06582-C03-01), MOISES-BAR (TIN2005-08832-C03-03). Albert Bifet is supported by a FI grant through the SGR program of Generalitat de Catalunya.  <ref type="table">Table 3</ref>: Comparison of algorithms on real data sets. Time is measured in seconds, and memory in MB.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>KDD'09, June 28-July 1, 2009, Paris, France. Copyright 2009 ACM 978-1-60558-495-9/09/06 ...$10.00.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 1 :</head><label>1</label><figDesc>The data stream classification cycle it only once (at most) Requirement 2 Use a limited amount of memory Requirement 3 Work in a limited amount of time Requirement 4 Be ready to predict at any time</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 2 :</head><label>2</label><figDesc>A sigmoid function f (t) = 1/(1 + e −s(t−t 0 ) ).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>•</head><label></label><figDesc>Pr[c(t) = a(t)] = e −4(t−t 0 )/W /(1 + e −4(t−t 0 )/W ) • Pr[c(t) = b(t)] = 1/(1 + e −4(t−t 0 )/W ).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 3 :</head><label>3</label><figDesc>Kappa-Error diagrams for ASHT bagging (top) and bagging (bottom) on dataset RandomRBF with drift, plotting 90 pairs of classifiers.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head></head><label></label><figDesc>'s consider two classifiers ha and h b , a data set containing m examples, and a contingency table where cell Cij contains the number of examples for which ha(x) = i and h b (x) = j. If ha and h b are identical on the data set, then all non-zero counts will appear along the diagonal. If ha and h b are very different, then there should be a large number of counts off the diagonal. We define Θ1 =</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" validated="false"><head></head><label></label><figDesc>The ELEC2 dataset contains 45, 312 instances. Each example of the dataset refers to a period of 30 minutes, i.e. there are 48 instances for each time period of one day. The class label identifies the change of the price related to a moving average of the last 24 hours. The class level only reflect deviations of the price on a one day average and removes the impact of longer term price trends.The size of these datasets is small, compared to tens of millions of training examples of synthetic datasets: 45, 312 for ELEC2 dataset, 581, 012 for CoverType, and 1, 000, 000 for Poker-Hand. Another important fact is that we do not know when drift occurs or if there is any drift. We may simulate RCD concept drift, joining the three datasets, merging attributes, and supposing that each dataset corresponds to a different concept.</figDesc><table>CovPokElec = (CoverType ⊕ 5,000 
581,012 Poker) ⊕ 5,000 
1,000,000 ELEC2 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" validated="false"><head>Table 1 :</head><label>1</label><figDesc>Comparison of algorithms. Accuracy is measured as the final percentage of examples correctly classified over the 1 or 10 million test/train interleaved evaluation. Time is measured in seconds, and memory in MB. The best individual accuracies are indicated in boldface. Note that due to the large number of test examples all differences are statistically significant, but these differences may not be meaningful in practise.</figDesc><table></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" validated="false"><head>Table 2 :</head><label>2</label><figDesc>Comparison of algorithms. Accuracy is measured as the final percentage of examples correctly classified over the 1 or 10 million test/train interleaved evaluation. Time is measured in seconds, and memory in MB.</figDesc><table></table></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">An interval classifier for database mining applications</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Agrawal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">P</forename><surname>Ghosh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Imielinski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">R</forename><surname>Iyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">N</forename><surname>Swami</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">VLDB &apos;92</title>
		<imprint>
			<date type="published" when="1992" />
			<biblScope unit="page" from="560" to="573" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Database mining: A performance perspective</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Agrawal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Imielinski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Swami</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. on Knowl. and Data Eng</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="914" to="925" />
			<date type="published" when="1993" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">UCI machine learning repository</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Asuncion</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Newman</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Early drift detection method</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Baena-García</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">D</forename><surname>Campo-Ávila</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Fidalgo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bifet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Gavaldà</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Morales-Bueno</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Fourth International Workshop on Knowledge Discovery from Data Streams</title>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Learning from time-changing data with adaptive windowing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bifet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Gavaldà</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIAM International Conference on Data Mining</title>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page" from="443" to="448" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Classification and Regression Trees</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Breiman</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1984" />
			<publisher>Chapman &amp; Hall</publisher>
			<pubPlace>New York</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Fast and light boosting for adaptive mining of data streams</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Chu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zaniolo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">PAKDD</title>
		<imprint>
			<publisher>Springer Verlag</publisher>
			<date type="published" when="2004" />
			<biblScope unit="page" from="282" to="292" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Mining high-speed data streams</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Domingos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hulten</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Knowledge Discovery and Data Mining</title>
		<imprint>
			<date type="published" when="2000" />
			<biblScope unit="page" from="71" to="80" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Learning with drift detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gama</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Medas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Castillo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Rodrigues</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SBIA Brazilian Symposium on Artificial Intelligence</title>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="page" from="286" to="295" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Accurate decision trees for mining high-speed data streams</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gama</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Rocha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Medas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">KDD &apos;03</title>
		<imprint>
			<date type="published" when="2003-08" />
			<biblScope unit="page" from="523" to="528" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">RainForest -a framework for fast decision tree construction of large datasets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gehrke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Ramakrishnan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Ganti</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">VLDB &apos;98</title>
		<imprint>
			<date type="published" when="1998" />
			<biblScope unit="page" from="416" to="427" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Splice-2 comparative evaluation: Electricity pricing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Harries</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1999" />
		</imprint>
		<respStmt>
			<orgName>The University of South Wales</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Technical report</note>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Holmes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Kirkby</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Pfahringer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Moa</surname></persName>
		</author>
		<ptr target="http://sourceforge.net/projects/moa-datastream" />
		<title level="m">Massive Online Analysis</title>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Mining time-changing data streams</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hulten</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Spencer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Domingos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">KDD&apos;01</title>
		<meeting><address><addrLine>San Francisco, CA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM Press</publisher>
			<date type="published" when="2001" />
			<biblScope unit="page" from="97" to="106" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Improving Hoeffding Trees</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Kirkby</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2007-11" />
		</imprint>
		<respStmt>
			<orgName>University of Waikato</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">PhD thesis</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Pruning adaptive boosting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">D</forename><surname>Margineantu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">G</forename><surname>Dietterich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML &apos;97</title>
		<imprint>
			<date type="published" when="1997" />
			<biblScope unit="page" from="211" to="218" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">SLIQ: A fast scalable classifier for data mining</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mehta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Agrawal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Rissanen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EDBT &apos;96</title>
		<meeting><address><addrLine>London, UK</addrLine></address></meeting>
		<imprint>
			<publisher>Springer-Verlag</publisher>
			<date type="published" when="1996" />
			<biblScope unit="page" from="18" to="32" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">A framework for generating data to simulate changing environments</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Narasimhamurthy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">I</forename><surname>Kuncheva</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AIAP&apos;07</title>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page" from="384" to="389" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Online bagging and boosting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Oza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Russell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Artificial Intelligence and Statistics</title>
		<imprint>
			<publisher>Morgan Kaufmann</publisher>
			<date type="published" when="2001" />
			<biblScope unit="page" from="105" to="112" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Experimental comparisons of online and batch versions of bagging and boosting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">C</forename><surname>Oza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Russell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">KDD &apos;01</title>
		<imprint>
			<date type="published" when="2001-08" />
			<biblScope unit="page" from="359" to="364" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Online coordinate boosting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Pelossof</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Vovsha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Rudin</surname></persName>
		</author>
		<ptr target="http://arxiv.org/abs/0810.4553" />
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">New options for hoeffding trees</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Pfahringer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Holmes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Kirkby</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AI</title>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page" from="90" to="99" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Incremental learning from noisy data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">C</forename><surname>Schlimmer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">H</forename><surname>Granger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Machine Learning</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="317" to="354" />
			<date type="published" when="1986" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">SPRINT: A scalable parallel classifier for data mining</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">C</forename><surname>Shafer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Agrawal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mehta</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">VLDB &apos;96</title>
		<imprint>
			<date type="published" when="1996" />
			<biblScope unit="page" from="544" to="555" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">A streaming ensemble algorithm (SEA) for large-scale classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">N</forename><surname>Street</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">KDD &apos;01</title>
		<meeting><address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM Press</publisher>
			<date type="published" when="2001" />
			<biblScope unit="page" from="377" to="382" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><forename type="middle">H</forename><surname>Witten</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Frank</surname></persName>
		</author>
		<title level="m">Data Mining: Practical Machine Learning Tools and Techniques. Morgan Kaufmann Series in Data Management Systems</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Morgan</forename><surname>Kaufmann</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2005-06" />
		</imprint>
	</monogr>
	<note>second edition</note>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Categorizing and mining concept drifting data streams</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Shi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">KDD &apos;08</title>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2008" />
			<biblScope unit="page" from="812" to="820" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
