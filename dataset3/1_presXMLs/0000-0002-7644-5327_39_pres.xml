<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /mnt/sda1/Dissertation/grobid/grobid-0.5.6/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Multi-agent reinforcement learning</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2019-12-22">December 22, 2019</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Schnebli</forename><surname>Zoltan</surname></persName>
						</author>
						<title level="a" type="main">Multi-agent reinforcement learning</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2019-12-22">December 22, 2019</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.5.6" ident="GROBID" when="2019-12-24T15:31+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>		<profileDesc>
			<abstract/>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Boosting as a Metaphor For Algorithm Design
Kevin Leyton-Brown   Eugene Nudelman   James McFadden   Galen Andrew  Yoav Shoham
</p>
<p>Department of Computer Science, Stanford University, USA
</p>
<p>The Combinatorial Auction Boosting Extensions
Winner Determination Problem 1. Trade off time taken to compute features and 
</p>
<p>1. Combine uncorrelated weak classifiers into a stronger aggregate time taken to run the selected algorithm
Find revenue-maximizing non-conflicting 2. Train new classifiers on instances that are hard for the aggregate
allocation of submitted bids
</p>
<p>Smart
</p>
<p>1. Algorithm Portfolios
Hardness models can be used to select an algorithm to run on a per-instance basis
</p>
<p>Regular
</p>
<p>6000 800 0 50 100 150 200 250 300
Time (s)
</p>
<p>700 2. Transform response variable to achieve 
5000
</p>
<p>tradeoffs between absolute and relative 
600
</p>
<p>4000 prediction error
Complete heuristic search algorithms we used: 500
</p>
<p>CPLEX [ILOG Inc.] 3000 400 5 Response = Actual Runtime
4
</p>
<p>CASS [Leyton-Brown et.al] 3
300 2
</p>
<p>GL [Gonen and Lehman] 2000 1
0
</p>
<p>200 -1
-2
</p>
<p>1000 -2 -1 0 1 2 3 4 5
Data 100 log(Actual Runtim e)
</p>
<p>We generated instances from: 0 0
5
</p>
<p>Response = (Actual Runtime)ˆ(1/3)
</p>
<p>Weighted Random (L2), Uniform (L3), GL CASS CPLEX CPLEX Optimal Portfolio 4
3
</p>
<p>Decay (L4) [Sandholm] 2
Optimal Algorithm Selection    Portfolio Algorithm Selection 1
</p>
<p>Exponential (L6), Binomial (L7) [Fujishima] 0
-1
</p>
<p>CATS: Regions, Arbitrary, Matching, -2 -2 -1 0 1 2 3 4 5
Scheduling log(Actua l Runtime)[Leyton-Brown et al.]
</p>
<p>Randomly sampled generator’s parameters for 
each instance 3. Cap runtimes to significantly reduce the 
</p>
<p>amount of time required for collecting data
Took more than 3 years of CPU time just to 
collect CPLEX runtimes
</p>
<p>Discussion: Other Approaches
Algorithm selection has received some 
</p>
<p>100% previous study; e.g., [Rice], [Lobjois & Lemaitre]
</p>
<p>2. Distribution Induction Classification  [Horvitz et al.]
80%
</p>
<p>To evaluate new algorithms, use portfolio hardness model as a PDF and generate error measure often inappropriate
</p>
<p>60% problems in proportion to the time our portfolio spends on them class boundary effects
</p>
<p>40% D: original distribution of instances; Hf: model of portfolio runtime (hf normalized) Run n algorithms in parallel [Gomes & Selman]
</p>
<p>Generate instances from D × hf using rejection sampling running time always n · min-time20%
80% in our case study, we did much better 
</p>
<p>0%
L (1.05 · min-time)
</p>
<p>L 3 70%7
-1 A0 L4 rbit Original
</p>
<p>Sequential algorithm selection using 
1 RL e r2 g ar
</p>
<p>60%
2 io y Harder MDP formalism [Lagoudakis & Littman]L
</p>
<p>3 6
n
</p>
<p>S s
</p>
<p>Running 4 P c 50%a he algorithms must be reimplemented
5 M th dul Distribution
</p>
<p>Time at s ic nh g 500 instances 40% computing a good value function at every in
log10(sec) g in each recursive branch can be very expensive 30%
</p>
<p>(our value function averaged 27 secs)
</p>
<p>Empirical Hardness Models 20%
</p>
<p>In past work, we found that quadratic 10% Future Directions
regression can yield very accurate models 0%
</p>
<p>Apply these ideas to other NP-hard problems 
predicting log of CPLEX runtime 0.1 1 10 100 1000 10000 10000010 such as SAT
root mean squared error: 0.216 (test data) Runtime (s)Matching Scheduling our preliminary SAT portfolio (“SATzilla”) 
5 100% 40% showed very encouraging results at the 
4 Original
</p>
<p>80% Original SAT-2003 competition (2nd on random 
3 Hard 30% Hard data; 3rd on “handmade” data)
</p>
<p>60%
2
</p>
<p>20% Study the use of SVM regression rather than 
1 40%
</p>
<p>least squares regression
0 10%
</p>
<p>20% our initial results show that SVMs
-1
</p>
<p>0% 0% outperform least-squares models, albeit by 
-2
</p>
<p>-2 -1 0 1 2 3 4 5 0.05 0.1 0.15 0.2 0.25 0.3 0.35 0.4 0.45 0.5 5 10 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 5 31 a fairly small “margin”
log(Actual Runtime) Runtime (s) Runtime (s)
</p>
<p>We would like to acknowledge Ryan Porter, Carla Gomes and Bart Selman for their assistance .  This work was supported by DARPA grant F30602-00-2-0598 and a Stanford Graduate Fellowship.
</p>
<p>Predicted log(Runtime)
</p>
<p>Time (s)
</p>
<p>Predicted log (Runtim e) P r e d i c ted  lo g (Ru n tim e)</p>
</div>
</body>
		<back>
			<div type="references">

				<listBibl>


				</listBibl>
			</div>
		</back>
	</text>
</TEI>
