<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /mnt/sda1/Dissertation/grobid/grobid-0.5.6/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Improving Adaptive Bagging Methods for Evolving Data Streams Motivation</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2009-11-04">4 November 2009</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Catalonia</forename><surname>Upc-Barcelona Tech</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Nanjing</surname></persName>
						</author>
						<title level="a" type="main">Improving Adaptive Bagging Methods for Evolving Data Streams Motivation</title>
					</analytic>
					<monogr>
						<title level="m">1st Asian Conference on Machine Learning (ACML&apos;09)</title>
						<imprint>
							<date type="published" when="2009-11-04">4 November 2009</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.5.6" ident="GROBID" when="2019-12-23T23:49+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>		<profileDesc>
			<abstract/>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>It is closely related to WEKA It includes a collection of offline and online as well as tools for evaluation: boosting and bagging Hoeffding Trees with and without Naïve Bayes classifiers at the leaves.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">/ 26</head><p>Waikato Environment for Knowledge Analysis Collection of state-of-the-art machine learning algorithms and data processing tools implemented in Java Released under the GPL Support for the whole process of experimental data mining Preparation of input data Statistical evaluation of learning schemes Visualization of input data and the result of learning Used for education, research and applications Complements "Data Mining" by Witten &amp; Frank</p><formula xml:id="formula_0">6 / 26 t f (t) f (t) α α t 0 W 0.5 1 Definition</formula><p>Given two data streams a, b, we define c = a ⊕ W t 0 b as the data stream built joining the two data streams a and b </p><formula xml:id="formula_1">Pr[c(t) = b(t)] = 1/(1 + e −4(t−t 0 )/W ). Pr[c(t) = a(t)] = 1 − Pr[c(t) = b(t)] 9 / 26 t f (t) f (t) α α t 0 W 0.5 1 Example (((a ⊕ W 0 t 0 b) ⊕ W 1 t 1 c) ⊕ W 2 t 2 d) . . . (((SEA 9 ⊕ W t 0 SEA 8 ) ⊕ W 2t 0 SEA 7 ) ⊕ W 3t 0 SEA 9.5 ) CovPokElec = (CoverType ⊕ 5,</formula><formula xml:id="formula_2">T 1 T 2 T 3 T 4</formula><p>Ensemble of trees of different size each tree has a maximum size after one node splits, it deletes some nodes to reduce its size if the size of the tree is higher than the maximum value</p><formula xml:id="formula_3">T 1 T 2 T 3 T 4</formula><p>Ensemble of trees of different size smaller trees adapt more quickly to changes,  Extend MOA to more data mining and learning methods.</p><formula xml:id="formula_4">W = 101010110111111 W 0 = 1 ADWIN: ADAPTIVE WINDOWING ALGORITHM 1 Initialize Window W 2 for each t &gt; 0 3 do W ← W ∪ {x t } (i.e., add x t to the head of W ) 4 repeat Drop elements from the tail of W 5 until |μ W 0 −μ W 1 | &lt; ε c holds 6 for every split of W into W = W 0 · W 1 7 Outputμ W ADWIN Example W = 101010110111111 W 0 = 1 W 1 = 01010110111111 ADWIN: ADAPTIVE WINDOWING ALGORITHM 1 Initialize Window W 2 for each t &gt; 0 3 do W ← W ∪ {x t } (i.e.,</formula></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Faster</head><label></label><figDesc>Mining Software using less resources Instant mining: more for less {M}assive {O}nline {A}nalysis is a framework for online learning from data streams.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure:</head><label></label><figDesc>Kappa-Error diagrams for ASHT bagging (left) and bagging (right) on dataset RandomRBF with drift, plotting 90 pairs of classifiers. Improvement for ASHT Bagging ensemble method Bagging using trees of different size add a change detector for each tree in the ensemble DDM: Gama et al. EDDM: Baena, del Campo, Fidalgo et al. sliding window whose size is recomputed online according to the rate of change observed.ADWIN has rigorous guarantees (theorems) On ratio of false positives and negatives On the relation of the size of the current window and change ratesADWIN BaggingWhen a change is detected, the worst classifier is removed and a new classifier is added.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" validated="false"><head></head><label></label><figDesc>Figure:Accuracy on dataset LED with three concept drifts.http://www.cs.waikato.ac.nz/∼abifet/MOA/ConclusionsNew improvements for ensemble bagging methods:Adaptive-Size Hoeffding Tree bagging using change detection methods ADWIN bagging using Hoeffding Adaptive Trees MOA is easy to use and extend</figDesc><table>add x t to the head of W ) 
4 
repeat Drop elements from the tail of W 
5 
until |μ W 0 −μ W 1 | &lt; ε c holds 
6 
for every split of W into W = W 0 · W 1 
7 
Outputμ W 
ADWIN 

Example 

W = 101010110111111 
W 0 = 10 W 1 = 1010110111111 

ADWIN: ADAPTIVE WINDOWING ALGORITHM 

1 Initialize Window W 
2 for each t &gt; 0 
3 
do W ← W ∪ {x t } (i.e., add x t to the head of W ) 
4 
repeat Drop elements from the tail of W 
5 
until |μ W 0 −μ W 1 | &lt; ε c holds 
6 
for every split of W into W = W 0 · W 1 
7 
Outputμ W 
ADWIN 

Example 

W = 101010110111111 
W 0 = 101 W 1 = 010110111111 

ADWIN: ADAPTIVE WINDOWING ALGORITHM 

1 Initialize Window W 
2 for each t &gt; 0 
3 
do W ← W ∪ {x t } (i.e., add x t to the head of W ) 
4 
repeat Drop elements from the tail of W 
5 
until |μ W 0 −μ W 1 | &lt; ε c holds 
6 
for every split of W into W = W 0 · W 1 
7 
Outputμ W 
ADWIN 

Example 

W = 101010110111111 
W 0 = 1010 W 1 = 10110111111 

ADWIN: ADAPTIVE WINDOWING ALGORITHM 

1 Initialize Window W 
2 for each t &gt; 0 
3 
do W ← W ∪ {x t } (i.e., add x t to the head of W ) 
4 
repeat Drop elements from the tail of W 
5 
until |μ W 0 −μ W 1 | &lt; ε c holds 
6 
for every split of W into W = W 0 · W 1 
7 
Outputμ W 
ADWIN 

Example 

W = 101010110111111 
W 0 = 10101 W 1 = 0110111111 

ADWIN: ADAPTIVE WINDOWING ALGORITHM 

1 Initialize Window W 
2 for each t &gt; 0 
3 
do W ← W ∪ {x t } (i.e., add x t to the head of W ) 
4 
repeat Drop elements from the tail of W 
5 
until |μ W 0 −μ W 1 | &lt; ε c holds 
6 
for every split of W into W = W 0 · W 1 
7 
Outputμ W 
ADWIN 

Example 

W = 101010110111111 
W 0 = 101010 W 1 = 110111111 

ADWIN: ADAPTIVE WINDOWING ALGORITHM 

1 Initialize Window W 
2 for each t &gt; 0 
3 
do W ← W ∪ {x t } (i.e., add x t to the head of W ) 
4 
repeat Drop elements from the tail of W 
5 
until |μ W 0 −μ W 1 | &lt; ε c holds 
6 
for every split of W into W = W 0 · W 1 
7 
Outputμ W 
ADWIN 

Example 

W = 101010110111111 
W 0 = 1010101 W 1 = 10111111 

ADWIN: ADAPTIVE WINDOWING ALGORITHM 

1 Initialize Window W 
2 for each t &gt; 0 
3 
do W ← W ∪ {x t } (i.e., add x t to the head of W ) 
4 
repeat Drop elements from the tail of W 
5 
until |μ W 0 −μ W 1 | &lt; ε c holds 
6 
for every split of W into W = W 0 · W 1 
7 
Outputμ W 
ADWIN 

Example 

W = 101010110111111 
W 0 = 10101011 W 1 = 0111111 

ADWIN: ADAPTIVE WINDOWING ALGORITHM 

1 Initialize Window W 
2 for each t &gt; 0 
3 
do W ← W ∪ {x t } (i.e., add x t to the head of W ) 
4 
repeat Drop elements from the tail of W 
5 
until |μ W 0 −μ W 1 | &lt; ε c holds 
6 
for every split of W into W = W 0 · W 1 
7 
Outputμ W 
ADWIN 

Example 

W = 101010110111111 |μ W 0 −μ W 1 | ≥ ε c : CHANGE DET.! 
W 0 = 101010110 W 1 = 111111 

ADWIN: ADAPTIVE WINDOWING ALGORITHM 

1 Initialize Window W 
2 for each t &gt; 0 
3 
do W ← W ∪ {x t } (i.e., add x t to the head of W ) 
4 
repeat Drop elements from the tail of W 
5 
until |μ W 0 −μ W 1 | &lt; ε c holds 
6 
for every split of W into W = W 0 · W 1 
7 
Outputμ W 
ADWIN 

Example 

W = 101010110111111 Drop elements from the tail of W 
W 0 = 101010110 W 1 = 111111 

ADWIN: ADAPTIVE WINDOWING ALGORITHM 

1 Initialize Window W 
2 for each t &gt; 0 
3 
do W ← W ∪ {x t } (i.e., add x t to the head of W ) 
4 
repeat Drop elements from the tail of W 
5 
until |μ W 0 −μ W 1 | &lt; ε c holds 
6 
for every split of W into W = W 0 · W 1 
7 
Outputμ W 
ADWIN 

Example 

W = 01010110111111 Drop elements from the tail of W 
W 0 = 101010110 W 1 = 111111 

ADWIN: ADAPTIVE WINDOWING ALGORITHM 

1 Initialize Window W 
2 for each t &gt; 0 
3 
do W ← W ∪ {x t } (i.e., add x t to the head of W ) 
4 
repeat Drop elements from the tail of W 
5 
until |μ W 0 −μ W 1 | &lt; ε c holds 
6 
for every split of W into W = W 0 · W 1 
7 
Outputμ W 
ADWIN 

ADWIN using a Data Stream Sliding Window Model, 

can provide the exact counts of 1's in O(1) time per point. 

tries O(log W ) cutpoints 

uses O( 1 
ε log W ) memory words 
the processing time per example is O(log W ) (amortized 
and worst-case). 

Sliding Window Model 

1010101 101 11 1 1 

Content: 
4 
2 
2 1 1 

Capacity: 
7 
3 
2 1 1 

20 / 26 

ADWIN bagging using Hoeffding Adaptive Trees 

Decision Trees: Hoeffding Adaptive Tree 

CVFDT: Hulten, Spencer and Domingos 

No theoretical guarantees on the error rate of CVFDT 

Parameters needed : size of window, number of 
examples,... 

Hoeffding Adaptive Tree: 

replace frequency statistics counters by estimators 

don't need a window to store examples 

use a change detector with theoretical guarantees to 
substitute trees 

Advantages: 

1 

Theoretical guarantees 

2 

No Parameters 
1 Adaptive-Size Hoeffding Tree bagging 

2 ADWIN Bagging 

3 Empirical evaluation 

22 / 26 
Dataset 
Most Accurate Method 
Hyperplane Drift 0.0001 
Bag10 ASHT W+R 
Hyperplane Drift 0.001 
DDM Bag10 ASHT W 
SEA W = 50 
BagADWIN 10 HAT 
SEA W = 50000 
BagADWIN 10 HAT 
RandomRBF No Drift 50 centers 
Bag 10 HT 
RandomRBF Drift .0001 50 centers BagADWIN 10 HAT 
RandomRBF Drift .001 50 centers 
DDM Bag10 ASHT W 
RandomRBF Drift .001 10 centers 
BagADWIN 10 HAT 
Cover Type 
DDM Bag10 ASHT W 
Poker 
BagADWIN 10 HAT 
Electricity 
DDM Bag10 ASHT W 
CovPokElec 
BagADWIN 10 HAT 
SEA 
W= 50000 
Time 
Acc. 
Mem. 
BagADWIN 10 HAT 
154.91 88.88 ± 0.05 
2.35 
DDM Bag10 ASHT W 
44.02 88.72 ± 0.05 
0.65 
NaiveBayes 
5.52 84.60 ± 0.03 
0.00 
NBADWIN 
12.40 87.83 ± 0.07 
0.02 
HT 
7.20 85.02 ± 0.11 
0.33 
HT DDM 
7.88 88.17 ± 0.18 
0.16 
HAT 
20.96 88.40 ± 0.07 
0.18 
BagADWIN 10 HT 
53.15 88.58 ± 0.10 
0.88 
Bag10 HT 
30.88 85.38 ± 0.06 
3.36 
Bag10 ASHT W+R 
33.56 88.51 ± 0.06 
0.84 
Accuracy 

71,4 

71,9 

72,4 

72,9 

73,4 

73,9 

10.000 140.000 270.000 400.000 530.000 660.000 790.000 920.000 

Instances 

Accuracy (%) 

BagAdwin HAT 
DDM BagHAST 
EDDM BagHast 
DDM HT 
EDDM HT 
BagAdwin HT 
BagHAST 

Summary 

Future Work 

</table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">/ 26</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3">/ 26</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4">/ 26</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="7">/ 26</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="8">/ 26</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="8">/ 26</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="8">/ 26</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="8">/ 26</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="9">/ 26</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="16">/ 26</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>The Moa (another native NZ bird) is not only flightless, like the Weka, but also extinct.</p><p>The Moa (another native NZ bird) is not only flightless, like the Weka, but also extinct.</p><p>The Moa (another native NZ bird) is not only flightless, like the Weka, but also extinct.</p><p>The Moa (another native NZ bird) is not only flightless, like the Weka, but also extinct.</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Theorem</head><p>At every time step we have: 1 (False positive rate bound). If µ t remains constant within W , the probability that ADWIN shrinks the window at this step is at most δ . 2 (False negative rate bound). Suppose that for some partition of W in two parts W 0 W 1 (where W 1 contains the most recent items) we have |µ W 0 − µ W 1 | &gt; 2ε c . Then with probability 1 − δ ADWIN shrinks W to W 1 , or shorter.</p><p>ADWIN tunes itself to the data stream at hand, with no need for the user to hardwire or precompute parameters.</p></div>			</div>
			<div type="references">

				<listBibl/>
			</div>
		</back>
	</text>
</TEI>
