<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /mnt/sda1/Dissertation/grobid/grobid-0.5.6/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">KALAKA-2: a TV Broadcast Speech Database for the Recognition of Iberian Languages in Clean and Noisy Environments</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luis</forename><forename type="middle">J</forename><surname>Rodríguez-Fuentes</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Grupo de Trabajo en Tecnologas Software (GTTS</orgName>
								<orgName type="department" key="dep2">Departamento de Electricidad y Electrnica</orgName>
								<orgName type="institution">University of the Basque Country UPV/EHU</orgName>
								<address>
									<postCode>48940</postCode>
									<settlement>Leioa</settlement>
									<country key="ES">Spain</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mikel</forename><surname>Penagarikano</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Grupo de Trabajo en Tecnologas Software (GTTS</orgName>
								<orgName type="department" key="dep2">Departamento de Electricidad y Electrnica</orgName>
								<orgName type="institution">University of the Basque Country UPV/EHU</orgName>
								<address>
									<postCode>48940</postCode>
									<settlement>Leioa</settlement>
									<country key="ES">Spain</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amparo</forename><surname>Varona</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Grupo de Trabajo en Tecnologas Software (GTTS</orgName>
								<orgName type="department" key="dep2">Departamento de Electricidad y Electrnica</orgName>
								<orgName type="institution">University of the Basque Country UPV/EHU</orgName>
								<address>
									<postCode>48940</postCode>
									<settlement>Leioa</settlement>
									<country key="ES">Spain</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mireia</forename><surname>Diez</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Grupo de Trabajo en Tecnologas Software (GTTS</orgName>
								<orgName type="department" key="dep2">Departamento de Electricidad y Electrnica</orgName>
								<orgName type="institution">University of the Basque Country UPV/EHU</orgName>
								<address>
									<postCode>48940</postCode>
									<settlement>Leioa</settlement>
									<country key="ES">Spain</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Germán</forename><surname>Bordel</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Grupo de Trabajo en Tecnologas Software (GTTS</orgName>
								<orgName type="department" key="dep2">Departamento de Electricidad y Electrnica</orgName>
								<orgName type="institution">University of the Basque Country UPV/EHU</orgName>
								<address>
									<postCode>48940</postCode>
									<settlement>Leioa</settlement>
									<country key="ES">Spain</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Barrio</forename><surname>Sarriena</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Grupo de Trabajo en Tecnologas Software (GTTS</orgName>
								<orgName type="department" key="dep2">Departamento de Electricidad y Electrnica</orgName>
								<orgName type="institution">University of the Basque Country UPV/EHU</orgName>
								<address>
									<postCode>48940</postCode>
									<settlement>Leioa</settlement>
									<country key="ES">Spain</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">KALAKA-2: a TV Broadcast Speech Database for the Recognition of Iberian Languages in Clean and Noisy Environments</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.5.6" ident="GROBID" when="2019-12-23T23:49+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>		<profileDesc>
			<textClass>
				<keywords>
					<term>Spoken Language Recognition</term>
					<term>Broadcast Speech</term>
					<term>Iberian Languages</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This paper presents the main features (design issues, recording setup, etc.) of KALAKA-2, a TV broadcast speech database specifically designed for the development and evaluation of language recognition systems in clean and noisy environments. KALAKA-2 was created to support the Albayzin 2010 Language Recognition Evaluation (LRE), organized by the Spanish Network on Speech Technologies from June to November 2010. The database features 6 target languages: Basque, Catalan, English, Galician, Portuguese and Spanish, and includes segments in other (Out-Of-Set) languages, which allow to perform open-set verification tests. The best performance attained in the Albayzin 2010 LRE is presented and briefly discussed. The performance of a state-of-the-art system in various tasks defined on the database is also presented. In both cases, results highlight the suitability of KALAKA-2 as a benchmark for the development and evaluation of language recognition technology.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>A TV broadcast speech database, named KALAKA-2, was designed, collected and built with the purpose of supporting the Albayzin 2010 Language Recognition Evaluation (LRE) organized by the Spanish Thematic Network on Speech Technologies from May to November 2010 . This was the second of a series of language recognition evaluations, which started with the Albayzin 2008 LRE <ref type="bibr" target="#b7">(Rodriguez-Fuentes et al., 2010a)</ref>. In fact, KALAKA-2 is a major update of KALAKA <ref type="bibr" target="#b8">(Rodriguez-Fuentes et al., 2010b)</ref>, the database created to support the Albayzin 2008 LRE, which consisted of wide-band TV broadcast speech recordings and featured 4 target languages: Basque, Catalan, Galician and Spanish. The update involves the addition of Portuguese and English as target languages, the addition of new Out-Of-Set languages and the use of noisy and/or overlapped speech for a new test condition. Besides recycling all the materials of KALAKA, new TV broadcast shows were recorded, including both planned and spontaneous speech in diverse environment conditions (excluding telephonechannel speech) and multiple speakers. The database consists of three subsets: training, development and test, which allow to build and evaluate language recognition systems for six target languages: Basque, Catalan, English, Galician, Portuguese and Spanish. English has been included for its leading role as a world interchange language (note that English is also the official language in Gibraltar). The remaining languages have jointly evolved in the Iberian peninsula during centuries, most of them sharing a common origin in Latin, so the recognition task could be specially challenging, as performance results in the Albayzin 2008 LRE already revealed. The KALAKA-2 datasets further increase the difficulty of the task by extending the challenge to noisy/overlapped speech.</p><p>The development and test datasets include not only target languages but also Out-Of-Set (OOS) languages, so that open-set evaluations can be carried out. OOS languages (Arabic, French, German and Romanian) have been chosen based on the availability of TV channels, but also taking into account their similarity to target languages. In this regard, both French and Romanian are Romance languages, as four of the target languages; on the other hand, Arabic had a strong influence on Spanish (specially at the lexical level) and a moderate influence on Portuguese, Catalan and Galician; finally, German belongs to the same family of languages as English. The train dataset amounts to more than 82 hours of speech, with more than 10 hours (in some cases, more than 12 hours) of clean speech per target language and more than 2 hours (in some cases, more than 3 hours) of noisy/overlapped speech per target language. The development and test datasets have the same size (around 21 hours of speech) but a different distribution of OOS languages. The whole database amounts to around 125 hours of speech (2.5 times the size of KALAKA) and is distributed in five DVD, after direct request to the authors. In the future, we plan to license the database through a distribution agency such as LDC or ELRA. The rest of the paper is organized as follows. The design of the database and the recording setup are addressed in Sections 2 and 3, respectively. Section 4 describes how the recorded materials were processed and organized, including the recycling of KALAKA, the classification of recordings, the selection of speech materials, the extraction of fixed (nominal) length segments and the encoding of filenames. Section 5 summarizes the results obtained in the Albayzin 2010 LRE and presents a state-of-the-art language recognition system developed and evaluated on KALAKA-2. Finally, conclusions are given in Section 6.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Design Issues</head><p>KALAKA-2 was designed as an extension of KALAKA with the purpose of: (1) adding two new target languages (English and Portuguese), and (2) allowing the evaluation of systems under a new test condition for noisy and/or overlapped speech. The materials produced for KALAKA were fully recycled for KALAKA-2, as follows: the train and development sets of KALAKA were posted to the train set of KALAKA-2, and the test set of KALAKA was posted to the development set of KALAKA-2. New TV broadcasts were recorded, selected and classified, specially for the two newly added target languages (Portuguese and English), for the OOS languages and for the noisy condition in all languages, taking care of including as much diversity as possible regarding speakers, speech modalities, etc. Also, as an attempt to avoid unintentional overfitting to a given speaker or environment and to make the evaluation as robust as possible, disjoint subsets of TV shows were posted to the training, development and test datasets. It must be noted that the test set of KALAKA-2 was entirely built on new recordings, thus being completely independent of KALAKA. No constraints were imposed to training segments regarding duration, whereas development and test segments of three nominal durations (30, 10 and 3 seconds) were produced, to measure language recognition performance as a function of the available amount of speech.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Recording Setup</head><p>To keep consistency, new recordings were done under the same setup (cable TV provider, devices, connectors, audio conversions, etc.) used for KALAKA. CD quality (16 bit / 44.1 kHz / stereo) recordings were done through a home connection to cable TV, by means of a Roland Edirol R-09 ultra-light digital audio recorder (http://www.roland.com/products/en/R-09). Audio signals were downsampled to 16 kHz, left and right channels being averaged into one single channel, by means of SoX (http://sox.sourceforge.net). This way, storage requirements were reduced in a factor of 5.51, while keeping an acceptable (wide-band) quality for speech processing applications. The resulting signals were stored in WAV files. KALAKA-2 recordings were made at three different times: October-November 2008 (Arabic, Romanian and English), April-May 2010 (Arabic, German, French, Romanian, English and Portuguese) and August-September 2010 (Basque, Catalan, Galician and Spanish). <ref type="table" target="#tab_0">Table 1</ref> shows the TV channels and the recorded time for each language. The recorded time for all languages amounts to around 257 hours, which is more than two times the size of KALAKA-2.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Database Construction</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Classification of Recordings</head><p>Side information was gathered for each recording: language, show type (news, documentary, talk show, debate, etc.), duration, environment conditions, rate of speech overlaps, etc. This information was used to distribute TV shows  <ref type="table" target="#tab_2">Table 3</ref> for details).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Selection of Speech Fragments</head><p>This task was performed by listening to and looking at audio signals. The selected fragments may contain speech from two or more speakers, but only a single language. Two types of fragments were discarded for further use: (1) narrow-band (telephone-channel) speech fragments, and (2) fragments with background speech or speech overlaps using a different language than that used in the foreground (the nominal language). The remaining materials were cut into clean and noisy speech fragments. Clean speech fragments were allowed to have any length greater than 30 seconds. Noisy speech fragments were forced to be between 30 and 35 seconds long. Since finding long fragments under a single background condition was not easy, the purity constraint was relaxed. In the case of clean speech, besides some portions of silence, relatively short portions of noisy and/or overlapped speech were also allowed. In the case of noisy speech, relatively short portions of clean speech were allowed. This task was performed from May to June 2010 (training and development datasets) and in September 2010 (evaluation dataset) by members of the research team. After discussing and determining the selection criteria for the resulting sets of segments to be as homogeneous as possible, each member worked in a fully autonomous way and the resulting speech fragments (of indefinite duration) were pooled together.</p><p>No further processing was applied to speech fragments posted to the training dataset, which consists of two separate subsets, the first one containing more than 10 hours (in some cases, more than 12 hours) of clean speech per target language, and the second one containing more than 2 hours (in some cases, more than 3 hours) of noisy/overlapped speech for each target language. No training data are provided for OOS languages. The distribution of training data is shown in <ref type="table" target="#tab_1">Table 2</ref>. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Automatic Extraction of Fixed-Length Segments</head><p>Clean-speech fragments posted to the development and test datasets were taken as source to automatically extract segments of fixed duration (30, 10 and 3 seconds), using a greedy algorithm which aimed to catch natural segments (i.e. speech segments surrounded by low-energy regions) with small (always positive) deviations from nominal durations (see <ref type="bibr" target="#b8">(Rodriguez-Fuentes et al., 2010b)</ref> for details). Noisy-speech fragments posted to the development and test datasets were stored as 30-second segments, since their duration ranged from 30 to 35 seconds. Then, a greedy algorithm similar to that used for clean speech was applied to automatically extract 10-and 3-second noisy-speech segments. The development and evaluation datasets are identical in size and characteristics, except for the distribution of OOS languages and the proportion of clean and noisy speech. Both datasets contain at least 150 speech segments per target language and nominal duration. Each segment contains speech from one or more speakers in one of the 6 target languages or in an OOS language. The development set consists of 4950 speech segments, 3492 containing clean speech and 1458 containing noisy speech, their total duration being 21.24 hours (70% corresponding to clean speech and 30% to noisy speech). The evaluation set consists of 4992 speech segments, 3345 containing clean speech and 1647 containing noisy speech, their total duration being 21.43 hours (67% corresponding to clean speech and 33% to noisy speech). The distribution of segments per language is shown in <ref type="table" target="#tab_2">Table 3</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">Filename Encoding</head><p>The speech files of KALAKA-2 were initially stored with conventional names, according to the same protocol defined for KALAKA: a sequence LLCDDXXX.wav, where LL is the international language code (ca, eu, gl, en, es, pt, ar, de, fr, ro), C is the dataset identifier (t, d, e), DD is the </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>. Task and Conditions</head><p>Following NIST evaluations (see e.g. <ref type="bibr" target="#b1">(Martin and Greenberg, 2010)</ref>), the task defined for the Albayzin 2010 LRE consisted on deciding by computational means whether or not a target language was spoken in a test utterance. Performance was computed by presenting the system a set of trials and comparing system decisions with the right ones (stored in a keyfile). Each trial comprises a segment of audio containing speech in a single language, the identity of the target language and the set of non-target languages (those that may appear in the segment instead of the target language). For each trial, the system is required to output: (1) a hard decision about whether the target language is spoken in the segment; and (2) a score, such that the higher the score the greater the confidence that the segment contains the target language.</p><p>The Albayzin 2010 LRE involved independent language verification trials for a set of 6 target languages: Basque, Catalan, English, Galician, Portuguese and Spanish. Three segment durations (30, 10 and 3 seconds), two evaluation modes (closed-set vs. open-set) and two environment conditions (clean vs. noisy speech) were defined, leading to 12 evaluation tracks. Remind that closed-set evaluation assumes that only target languages can be spoken in test utterances, whereas open-set evaluation relaxes that assumption by allowing any language (i.e. also OOS languages) to be spoken in test utterances.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.2.">Performance Measures</head><p>System performance was primarily measured in terms of the well-known average cost C avg <ref type="bibr" target="#b2">(Martin and Le, 2008)</ref>, which is a combination of the miss and false alarm error rates (P miss and P f a ) obtained by the system at a given operation point (threshold), pooled across target languages.</p><p>The C avg measure depends on language priors (P target , P non−target and P OOS ) and application dependent costs (C miss and C f a ). Details about the applied values can be found in . Detection Error Tradeoff (DET) curves <ref type="bibr" target="#b3">(Martin et al., 1997)</ref> were also computed (using NIST software 1 ) to visualize and compare the global performance of systems, including marks for the actual and minimum C avg operation points.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.3.">Results</head><p>The best C avg performance attained at each track in the Albayzin 2010 LRE is shown in <ref type="table" target="#tab_3">Table 4</ref>. Test conditions (CC, OC, CN and ON) are coded so that the first letter refers to closed-set (C) or open-set (O) evaluation and the second letter refers to clean-speech (C) or noisy speech (N) test segments. DET curves corresponding to the best primary systems in the four test conditions for the subset of 30-second segments are shown in <ref type="figure" target="#fig_0">Figure 1</ref>. For the easiest condition (CC-30s), the best system yielded C avg = 0.0181, which is comparable to the performance attained by state-of-the-art language recognition systems on NIST LRE datasets. Note also that this performance is much better than that attained for a similar task in the Albayzin 2008 LRE (C avg = 0.0552, see (Rodriguez-Fuentes et al., 2010a)). Such an important difference may be due in part to technology improvements from 2008 to 2010, but also to the availability of more training data for target languages, and to the introduction of English and Portuguese as target languages, which makes the task easier on average, since most systems manage to discriminate them from the other target languages, as the low false alarm probabilities of English and Portuguese (compared to those of the other target languages) suggest (see  for details). Regarding the dependence on the nominal duration of test segments, the C avg obtained on 10-second segments is around twice that obtained on 30-second segments, and the same trend is observed for 3-second segments with regard to 10-second segments. This is consistent with previous results for other evaluations.  Performance degraded when moving from the closed-set to the open-set evaluation, due to a higher number of false alarms related to OOS languages. As shown in <ref type="table" target="#tab_3">Table 4</ref>, the best system in the OC-30s condition yielded C avg = 0.0296, meaning around 63.5% increase in cost with regard to the best system in the CC-30s condition. The increase in cost was less noticeable for 10-and 3-second segments. Finally, as illustrated in <ref type="figure" target="#fig_0">Figure 1</ref>, dealing with noisy speech led to the highest (but not catastrophic) performance degradations. The increase in cost when moving from clean to noisy speech ranged from 40% to 80%, being relatively smaller for short segments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.">State-of-the-Art System Development and</head><p>Evaluation Based on KALAKA-2</p><p>The development of KALAKA and KALAKA-2 was motivated by the lack of multilingual wide-band speech databases (specially including Iberian languages) for the development and evaluation of language recognition systems in applications not suitably covered by NIST LRE datasets, such as multilingual spoken document retrieval on wide-band broadcast speech resources. In particular, KALAKA-2 allows for the development and evaluation of language recognition systems for wide-band broadcast speech in both quiet and noisy background environments, which is a relevant step towards more realistic conditions with regard to KALAKA, where only clean speech was considered. A state-of-the-art language recognition system has been developed and evaluated based on the datasets of KALAKA-2. As will be shown below, the relatively low performance attained by this system in some tasks, specially those dealing with noisy speech, highlights the suitability of KALAKA-2 as a benchmark for the development and evaluation of new approaches.</p><p>The system presented in this section resembles almost exactly that submitted by our research group to the 2011 NIST LRE , which fused two acoustic and three phonotactic subsystems and demonstrated very competitive performance (fourth best primary system in the NIST 2011 LRE core condition): C avg = 0.0892 for the 24 worst performing language pairs and C avg = 0.0169 when the average was computed over all the pairs. All the models (except for the phone decoders applied in the phonotactic subsystems) have been trained exclusively on the training dataset of KALAKA-2. Two sets of models have been estimated: the first one, used for the clean-speech tracks, was trained on clean speech; the second one, used for the noisy-speech tracks, was trained on the whole training dataset, including both clean and noisy speech. Backend and fusion parameters have been estimated using the development dataset of KALAKA-2, under two configurations: (1) closed-set, for which only segments containing target languages were used; and <ref type="formula">(2)</ref> open-set, for which all the segments were used. In the following paragraphs, we provide a brief description of the component subsystems and the backend and fusion approaches.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.1.">Acoustic Subsystems</head><p>Acoustic features consist of the concatenation of 7 Mel-Frequency Cepstral Coefficients and the Shifted Delta Cepstrum coefficients <ref type="bibr">(Torres-Carrasquillo et al., 2002)</ref> under a 7-2-3-7 configuration, a gender independent 1024mixture Gaussian Mixture Model (GMM) is used as Universal Background Model (UBM) and zero-order and centered and normalized first-order Baum-Welch statistics were computed for each input utterance. The first acoustic subsystem follows the Linearized Eigenchannel GMM (LE-GMM) approach (also known as Dot-Scoring), which makes use of a linearized, channel compensated and normalized approximation of the likelihood ratio in the GMM-UBM approach to score test segments against target models <ref type="bibr" target="#b12">(Strasheim and</ref><ref type="bibr">Brümmer, 2008) (Brümmer et al., September 2009</ref>). The second acoustic subsystem follows the Total Variability generative iVector approach, as described in <ref type="bibr" target="#b4">(Martínez et al., 2011)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.2.">Phonotactic Subsystems</head><p>Three phonotactic sub-systems were developed under a phone-lattice Support Vector Machine (SVM) approach. Given an input signal, an energy-based voice activity detector was applied in first place, which split and removed long-duration non-speech segments. Then, the Temporal Patterns Neural Network (TRAPs/NN) phone decoders developed by the Brno University of Technology (BUT) for Czech, Hungarian and Russian <ref type="bibr" target="#b10">(Schwarz, 2008)</ref>, were applied to perform phone tokenization. Regarding channel compensation, noise reduction, etc. the three sub-systems relied on the acoustic front-end provided by BUT decoders. BUT decoders were configured to produce phone posteriors that were converted to phone lattices by means of HTK <ref type="bibr" target="#b15">(Young et al., 2006)</ref> along with the BUT recipe, on which expected counts of phone n-grams were computed using the lattice-tool of SRILM <ref type="bibr" target="#b11">(Stolcke, 2002)</ref>. Finally, a SVM classifier was applied, SVM vectors consisting of counts of features representing the phonotactics of an input utterance. In this work, phone n-grams up to n = 3 were used, weighted as in <ref type="bibr" target="#b6">(Richardson and Campbell, 2008)</ref>. L2regularized L1-loss support vector classification was applied, by means of LIBLINEAR <ref type="bibr" target="#b0">(Fan et al., 2008)</ref>, whose source code was slightly modified to get regression values.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.3.">Backend and Fusion</head><p>When processing an input utterance, each subsystem provides a score for each target language. In this work, a generative Gaussian backend is estimated for each subsystem (based on the scores obtained for the development set) and applied to get log-likelihoods for target languages (under the open-set configuration, an additional log-likelihood for OOS languages is also computed). Log-likelihods are then fused according to a discriminative linear model which minimizes the so called C LLR function on the development set, by means of logistic regression under a multiclass paradigm <ref type="bibr">(Brümmer and van Leeuwen, 2006)</ref>. After the fusion model is applied, well-calibrated scores are obtained, for which a minimum expected cost Bayes decision threshold is applied, according to application-dependent language priors and costs. Backend and fusion parameters have been separately estimated for each nominal duration on the development set, and then applied to the corresponding segments in the evaluation set. The FoCal toolkit has been used to estimate and apply the backend and fusion models <ref type="bibr">(FoCal, 2008)</ref>. <ref type="table" target="#tab_5">Table 5</ref> shows the C avg performance attained by the stateof-the-art language recognition system described above in all the tracks of the Albayzin 2010 LRE. <ref type="figure" target="#fig_1">Figure 2</ref> shows the corresponding DET curves for the tracks involving 30second segments. Remind that two different systems have been developed, the first one built and evaluated on clean speech (CC and OC tracks, solid DET curves) and the second one built and evaluated on a mix of clean and noisy speech (CN and ON tracks, dotted DET curves). System performance was consistently better than that attained in the Albayzin 2010 LRE (see <ref type="table" target="#tab_3">Table 4</ref> and <ref type="figure" target="#fig_0">Figure  1</ref>) in all conditions, except for 3-second segments. This could be due to a lack of robustness in the estimation of backend and fusion parameters when using extremely short segments (specially when dealing with noisy speech). The average cost was extremely low for the CC-30s track (C avg = 0.0063), meaning that, quite probably, this database would not support technology improvements for that condition (closed-set evaluation, clean speech, 30second segments), since differences in performance would Operation points corresponding to actual (×) and minimum (•) C avg are marked on the curves. be too small and not significant. In the remaining tracks, the average costs were high enough to allow statistically significant performance improvements. In particular, performance for the noisy-speech condition was far worse than that found for the clean-speech condition. It is worth noting that moving from clean to noisy speech produced higher degradation than moving from closed-set to open-set evaluation. In other words, keeping the test closed to target languages but using noisy speech seems to be more challenging than expanding the test with OOS languages but using clean speech. This conclusion is graphically supported by DET curves in <ref type="figure" target="#fig_1">Figure 2</ref>: performance in the closedset noisy-speech condition (dotted red curve) was consistenly worse than that in the open-set clean-speech condition (solid blue curve). Finally, performance degraded as less speech (i.e. less information) was available to make decisions (short segments), following the same pattern observed above for the Albayzin 2010 LRE.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.4.">Results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Conclusions</head><p>In this paper, we address the design, data collection, construction and evaluation of KALAKA-2, a database containing wide-band (16 kHz) clean and noisy speech signals recorded from TV broadcasts. KALAKA-2 was created and used specifically for the Albayzin 2010 Language Recognition Evaluation, but can be freely requested to authors. It amounts to around 125 hours of speech and consists of three datasets: training, development and evaluation, which allow to build and evaluate language recognition systems for six target languages: Basque, Catalan, English, Galician, Portuguese and Spanish (Iberian languages + English). The database also includes speech signals in other languages, to allow open-set verification trials.</p><p>The best performance attained in all the tracks of the Albayzin 2010 LRE has been presented as a means of evaluating the database. The best performance in the core condition (closed-set, clean-speech, 30-second segments) was much better than the best performance for the same condition in the Albayzin 2008 LRE, due to several reasons, including the availability of more training data and the introduction of two target languages (English and Portuguese) feturing extremely low confusion rates with the other target languages. Significant degradation was observed as less speech was available: roughly, the cost doubled when moving from 30-second to 10-second segments, and doubled again when moving from 10-second to 3-second segments. Moving from closed-set to open-set tests (i.e. allowing for test segments with OOS languages) also led to degraded performance, but the highest degradation was found when dealing with noisy speech. A second evaluation has been carried out, using the datasets of KALAKA-2 to build and evaluate a state-of-the-art language recognition system. Performance using this system was better than that attained in the Albayzin 2010 LRE in all conditions except for 3-second segments (probably due to unreliable estimations of backend and fusion models), but the same trends are observed (e.g. the highest degradation is produced by noisy speech) and the same conclusions can be drawn. In brief, the average costs were high enough to allow statistically significant performance improvements in all the tracks except for the easiest one (closed-set, clean-speech, 30-second segments). The most challenging condition involves extremely short (3-second) noisy speech segments in open-set tests, for which the best performance reported in this paper is C avg = 0.1551. Therefore, KALAKA-2 provides plenty of margin to support further language recognition technology developments for wide-band broadcast speech in quiet and noisy background environments.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>DET curves for the best primary systems submitted to the Albayzin 2010 LRE in all tracks (30-second segments). Operation points corresponding to actual (×) and minimum (•) C avg are marked on the curves.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>DET curves for a state-of-the-art language recognition system developed on KALAKA-2, in the four tracks of the Albayzin 2010 LRE involving 30-second segments.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" validated="true"><head>Table 1 :</head><label>1</label><figDesc>TV channels and recorded time (in minutes) for each language in KALAKA-2.</figDesc><table>Language TV Channels 
Recorded time 
Basque 
ETB1, ETBSat 
1996 
Catalan 
TVCi 
1842 

English 
DWTV, BBCWorld, 
CNN, Bloomberg 
2705 

Galician 
TVG 
2240 
Portuguese RTPi 
2608 

Spanish 

TVE1, La 2, 
La Sexta, Cuatro, 
Tele5, Antena3, ETB2, 
TV Canaria Sat, 
AndalucíaTV, 
TeleMadrid, 
ExtremaduraTV, 
CNNPlus 

2090 

Arabic 
Al Jazeera 
497 
French 
TV5Monde Europe 
499 
German 
DWTV 
431 
Romanian 
PROTV 
552 

into the training, development and test datasets, keeping in 
mind that the three datasets should contain similar propor-
tions of show types, and that all the recordings of a given 
TV show should be posted to the same dataset. To avoid 
tuning systems to reject specific OOS languages, different 
proportions of OOS languages were posted to the develop-
ment and test datasets (see </table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" validated="false"><head>Table 2 :</head><label>2</label><figDesc></figDesc><table>Distribution of training segments per target lan-
guage in KALAKA-2, for clean and noisy speech: number 
of segments (#) and total duration (T , in minutes). 

Clean speech 
Noisy speech 
# 
T (minutes) 
# 
T (minutes) 
Basque 
406 
644 
112 
135 
Catalan 
341 
687 
107 
131 
English 
249 
731 
136 
152 
Galician 
464 
644 
125 
134 
Portuguese 387 
665 
160 
197 
Spanish 
342 
625 
133 
222 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" validated="true"><head>Table 3 :</head><label>3</label><figDesc>Distribution of segments per language (the same for each duration) in the development and evaluation datasets of KALAKA-2.</figDesc><table>Devel 
Eval 
clean noisy clean noisy 
Basque 
146 
29 
130 
74 
Catalan 
120 
47 
149 
55 
Target 
English 
133 
60 
135 
69 
languages Galician 
137 
60 
121 
83 
Portuguese 
164 
77 
146 
58 
Spanish 
136 
83 
125 
79 
Arabic 
100 
25 
115 
22 
OOS 
French 
120 
32 
70 
34 
languages German 
108 
73 
13 
32 
Romanian 
0 
0 
111 
43 

duration code (00: undefined, 03: 3 seconds, 10: 10 sec-
onds, 30: 30 seconds), and XXX is a three-digit number 
which identifies each segment under each category. Then, 
with the purpose of keeping language content undisclosed, 
new filenames consisting of a seemingly random string of 8 
hexadecimal digits (followed by the .wav extension) were 
produced. To that end, the encoding/decoding algorithm 
defined for KALAKA was applied, based on a password, 
SHA-1 hashing of file contents and a XOR scheme (see 
(Rodriguez-Fuentes et al., 2010b) for details). The database 
is distributed with encoded filenames and a keyfile describ-
ing file contents. 

5. Database Evaluation 

5.1. The Albayzin 2010 LRE 

5.1.1</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" validated="true"><head>Table 4 :</head><label>4</label><figDesc>Best performance (C avg ) attained at each track in the Albayzin 2010 LRE.</figDesc><table>30s 
10s 
3s 
CC 0.0181 0.0359 0.0844 
OC 0.0296 0.0445 0.1029 
CN 0.0253 0.0636 0.1217 
ON 0.0475 0.0936 0.1551 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" validated="false"><head></head><label></label><figDesc>1 http://www.itl.nist.gov/iad/mig/tools/DETware v2.1.targz.htm</figDesc><table></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5" validated="false"><head>Table 5 :</head><label>5</label><figDesc></figDesc><table>C avg performance attained at each track of the Al-
bayzin 2010 LRE by a sate-of-the-art language recognition 
system developed on KALAKA-2. 

30s 
10s 
3s 
CC 0.0063 0.0263 0.0888 
OC 0.0171 0.0437 0.1094 
CN 0.0177 0.0599 0.1476 
ON 0.0390 0.0867 0.1740 

</table></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.">Acknowledgements</head><p>This work has been supported by the University of the Basque Country UPV/EHU, under grant GIU10/18 and project US11/06, by the Government of the Basque Country, under program SAIOTEK (project S-PE11UN065), and the Spanish MICINN, under Plan Nacional de I+D+i (project TIN2009-07446, partially financed by FEDER funds). Mireia Diez is supported by the Department of Education, Universities and Research of the Government of the Basque Country, under a 4-year research fellowship. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8.">References</head></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Toolkit for Evaluation, Fusion and Calibration of statistical pattern recognizers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai-Wei</forename><surname>Rong-En Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cho-Jui</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiang-Rui</forename><surname>Hsieh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chih-Jen</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lin</surname></persName>
		</author>
		<ptr target="http://sites.google.com/site/nikobrummer/focal" />
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page" from="1871" to="1874" />
			<date type="published" when="2008" />
		</imprint>
	</monogr>
	<note>LIBLINEAR: A library for large linear classification</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">The 2009 NIST Language Recognition Evaluation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alvin</forename><surname>Martin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Craig</forename><surname>Greenberg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Odyssey 2010 -The Speaker and Language Recognition Workshop</title>
		<meeting><address><addrLine>Brno, Czech Republic</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="165" to="171" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Language Recognition Evaluation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alvin</forename><forename type="middle">F</forename><surname>Martin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Audrey</forename><forename type="middle">N</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Odyssey 2008 -The Speaker and Language Recognition Workshop</title>
		<meeting>Odyssey 2008 -The Speaker and Language Recognition Workshop</meeting>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
	<note>paper 016</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">The DET Curve in Assessment of Detection Task Performance</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Martin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Doddington</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Kamm</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ordowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Przybocki</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Eurospeech</title>
		<meeting>Eurospeech</meeting>
		<imprint>
			<date type="published" when="1997" />
			<biblScope unit="page" from="1985" to="1988" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Language Recognition in iVectors Space</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Martínez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oldrich</forename><surname>Plchot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lukás</forename><surname>Burget</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Interspeech</title>
		<meeting>the Interspeech<address><addrLine>Firenze, Italy</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="861" to="864" />
		</imprint>
	</monogr>
	<note>Ondrej Glembek, and Pavel Matejka</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">University of the Basque Country (EHU) Systems for the 2011 NIST Language Recognition Evaluation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mikel</forename><surname>Penagarikano</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amparo</forename><surname>Varona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luis</forename><forename type="middle">J</forename><surname>Rodriguez-Fuentes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mireia</forename><surname>Diez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">German</forename><surname>Bordel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the NIST 2011 Language Recognition Evaluation (LRE) Workshop</title>
		<meeting>the NIST 2011 Language Recognition Evaluation (LRE) Workshop<address><addrLine>Atlanta (USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Language recognition with discriminative keyword selection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Richardson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Campbell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ICASSP</title>
		<meeting>ICASSP</meeting>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="4145" to="4148" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">The Albayzin 2008 Language Recognition Evaluation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">J</forename><surname>Rodriguez-Fuentes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Penagarikano</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Bordel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Varona</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Odyssey 2010: The Speaker and Language Recognition Workshop</title>
		<meeting>Odyssey 2010: The Speaker and Language Recognition Workshop<address><addrLine>Brno, Czech Republic</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="172" to="179" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">KALAKA: A TV broadcast speech database for the evaluation of language recognition systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">J</forename><surname>Rodriguez-Fuentes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Penagarikano</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Bordel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Varona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Diez</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 7th International Conference on Language Resources and Evaluation (LREC 2010)</title>
		<meeting>the 7th International Conference on Language Resources and Evaluation (LREC 2010)<address><addrLine>Valleta, Malta</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="1678" to="1685" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">The Albayzin 2010 Language Recognition Evaluation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luis</forename><forename type="middle">Javier</forename><surname>Rodriguez-Fuentes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mikel</forename><surname>Penagarikano</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amparo</forename><surname>Varona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mireia</forename><surname>Diez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">German</forename><surname>Bordel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Interspeech</title>
		<meeting>Interspeech<address><addrLine>Firenze, Italia</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="1529" to="1532" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Phoneme recognition based on long temporal context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Petr</forename><surname>Schwarz</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2008" />
			<pubPlace>Brno, Czech Republic</pubPlace>
		</imprint>
		<respStmt>
			<orgName>Faculty of Information Technology, Brno University of Technology</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Ph.D. thesis</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">SRILM -an extensible language modeling toolkit</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreas</forename><surname>Stolcke</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Interspeech</title>
		<meeting>Interspeech</meeting>
		<imprint>
			<date type="published" when="2002" />
			<biblScope unit="page" from="257" to="286" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">SUNSDV system description: NIST SRE</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Albert</forename><surname>Strasheim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niko</forename><surname>Brümmer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIST 2008 Speaker Recognition Evaluation Workshop Booklet</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">A</forename><surname>Torres-Carrasquillo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Singer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">A</forename><surname>Kohler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">J</forename></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Approaches to language identification using Gaussian mixture models and Shifted Delta Cepstral features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">A</forename><surname>Greene</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">R</forename><surname>Reynolds</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Deller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ICSLP</title>
		<meeting>ICSLP</meeting>
		<imprint>
			<date type="published" when="2002" />
			<biblScope unit="page" from="89" to="92" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">The HTK Book (for HTK Versin 3.4). Entropic, Ltd</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steve</forename><surname>Young</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gunnar</forename><surname>Evermann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Gales</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Hain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Kershaw</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xunying</forename><surname>Lui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gareth</forename><surname>Moore</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julian</forename><surname>Odell</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2006" />
			<pubPlace>Dave Ollason, Dan Povey, Valtcho Valtchev, and Phil Woodland; Cambridge, UK</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
