<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /mnt/sda1/Dissertation/grobid/grobid-0.5.6/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">BABEŞ-BOLYAI UNIVERSITY CLUJ-NAPOCA FACULTY OF MATHEMATICS AND INFORMATICS SPECIALIZATION: COMPUTER SCIENCE License Thesis Reinforcement Learning methods applied in Mario</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<title level="a" type="main">BABEŞ-BOLYAI UNIVERSITY CLUJ-NAPOCA FACULTY OF MATHEMATICS AND INFORMATICS SPECIALIZATION: COMPUTER SCIENCE License Thesis Reinforcement Learning methods applied in Mario</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.5.6" ident="GROBID" when="2019-12-23T23:47+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Reinforcement learning is a subsection of machine learning. It's purpose is to teach some software agent to take adequate actions, so that he can maximize a given cumulative reward.</p><p>The aim of this dissertation is to analyze and compare some of the widely used reinforcement learning algorithms. In order to achieve our goal we needed to choose an environment which can be easily modeled to apply these methods. The model presented in this paper is based on the well-known game</p><p>Mario. In addition to the comparison of these algorithms, the dissertation's other objective is to modify these methods in such a way, that we can maximize the results for the game of our choice.</p><p>In conclusion we can state that these algorithms depend heavily on the environment model and they exploit every inaccuracy of it. Therefore, we had to develop and specify our environment model continuously.</p><p>This work is the result of my own activity. I have neither given nor received unauthorized assistance on this work.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.1.">A gépi tanulás</head><p>A gépi tanulás az a tudomány, amely célja, hogy a számítógép folyamatos iteráció során adatok és tapasztalatok általánosítása alapján képes legyen a folyamatos fejlődésre és végeredményként elérje egy konkrét feladat sikeres elvégzését <ref type="bibr" target="#b15">[Domingos, 2012]</ref>. Ahogy egyre több adat áll rendelkezésünkre, annál nehezebb feladatokra vagyunk képesek megtanítani a számítógépünket.</p><p>Főbb kategóriái:</p><p>• Felügyelt tanulás (supervised learning) -megpróbáljuk modellezni a kapcsolatot a bemeneti és kimeneti adataink között. Rendelkezünk tanító-, teszt-és opcionálisan validációs halmazzal, amiket felcímkézünk a feladatnak megfelelően. A tanítóhalmaz alapján fogja a számítógép megtanulni a kapcsolatot a bemenetek és kimenetek között. A teszthalmaz segítségével tudjuk megmondani, hogy mennyire tanulta meg jól a feladatot a számítógép, míg a validációs halmazzal tovább tudjuk finomítani a modellezést. Főbb felügyelt tanulási módszerek: osztályozás (pl. emailek spamnem spam osztályba való sorolása), regresszió (pl. egy ház árának becslése bizonyos paraméterek, illetve egy tanítóhalmaz alapján).</p><p>• Felügyeletlen tanulás -nem rendelkezünk felcímkézett tanítóhalmazzal. Az adathalmazunkban valamilyen struktúrát, szabályt keresünk. Felügyeletlen tanulási módszer például a klaszterezés (pl. ügyfelek csoportosítása vásárlási szokásaik alapján).</p><p>• Félig felügyelt tanítás -a tanítóhalmazunknak csak egy bizonyos része van felcímkézve. Ide tar- </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.2.">Dolgozat áttekintése</head><p>A dolgozatunk célja egy olyan visszacsatolsásos ügynök létrehozása, amely képes átvinni a Mario játékot. E cél megvalósításához több módszerrel is próbálkoztunk.</p><p>A következő fejezetben bemutatásra kerül a visszacsatolásos tanulás elméleti háttere és megoldási módszerei 2. Majd az Alkalmazás című fejezetben (3) láthatjuk hogyan épül fel az alkalmazásunk és hogy miként alkalmaztuk ezeket a módszereket benne. Az eredményeinket a 4-es fejezetben mutatjuk be. Végül, a továbbfejlesztési lehetőségeket az 5-ös fejezetben soroltuk fel.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">fejezet</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Visszacsatolásos tanulás</head><p>Összefoglaló: Visszacsatolásos tanulás alapjait taglaljuk.</p><p>Ebben a fejezetben a visszacsatolásos tanulás lesz részletesen taglalva. Bemutatásra kerülnek ebben a doméniumban használatos alapfogalmak, általános megoldási módszerek és továbbá három konkrét módszer.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">A visszacsatolásos tanulás alapjai</head><p>A visszacsatolásos tanulás az a gépi tanulási módszer, amely úgy próbál bizonyos szituációkhoz cselekvéseket társítani, hogy közben egy jutalmat maximalizál. A többi gépi tanulási módszerrel ellentétben, a tanulónak, akire a továbbiakban ügynökként fogunk hivatkozni, nincs megmondva, hogy milyen cselekedeteket hajtson végre. Fel kell fedezze a környezetét és meg kell tapasztalja, hogy melyek számára azok a lépések, amelyek a legnagyobb jutalmat reprezentálják <ref type="bibr" target="#b29">[Sutton és Barto, 1998</ref>].</p><p>A felügyelt tanulással szemben, ahol a gép tesztadatok alapján próbál adott bemenetekre kimenetet származtatni <ref type="bibr" target="#b27">[Russell et al., 2003</ref>], a visszacsatolásos tanulás központjában egy jutalom-orintált ügynök áll <ref type="bibr" target="#b24">[Quah és Quek, 2006</ref>].</p><p>Minden ügynöknek van egy célja, amit a lehető leghatékonyabban szeretne elérni. Tudja érzékelni a környezét, tud cselekedni, viszont a cselekedetei megváltoztatják a körülötte lévő teret. Az ügynök nem tudja, hogy mennyire hatékony lépést választ magának hosszú távon, csupán egy pillanatnyi jutalmat kap cselekedetei után és érzékeli az új állapotot amibe került. Annak érdekében, hogy maximalizálni tudja a jutalmait, az ügynök minél több tapasztalatot kell szerezzen a környezetéről, cselekedetei következményeiről és a jutalmakról <ref type="bibr" target="#b25">[Ribeiro, 2002]</ref>. Ezt a jelenséget megfigyelhetjük részletesebben a 2.1 illusztráción is, ahol az ügynök bizonyos ismeretekkel rendelkezve meglépi az általa válaszott lépést és ennek következtében jutalommal és további információval/tudással gazdagodik.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">FEJEZET: VISSZACSATOLÁSOS TANULÁS</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Ügynök</head><p>Környezet Megfigyelés Cselekedet Jutalom 2.1. ábra. A képen megfigyelhető, hogy az ügynök a cselekedetei által kommunikál a környezetével. Rendelkezik a környezete állapotáról és ennek függvényében választja a lépését. Ennek következtében kap egy bizonyos számbeli jutalmat és érzékeli környezete változásait.</p><p>Az egyik legnagyobb kihívást az jelenti, hogy az ügynök cselekedetei a pillanatnyi jutalmon kívűl befolyásolják még a következő állapotokat és ennek következtében az epizódban szereplő összes, ez után következő jutalmat is. Így az ügynöknek nem csak a pillanatnyi, hanem a késleltett jutalmakból is tanulnia kell <ref type="bibr" target="#b20">[Kaelbling et al., 1996]</ref>. Egy epizód olyan állapot-cselekedet-jutalom sorozatot jelképez, amely egy terminális állapotban végződik. Terminális állapot azt jelenti, hogy az ügynöknek nem áll rendelkezésére érvényes cselekedet az adott állapotban.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">Felfedezés és kizsákmányolás</head><p>Az egyik kihívás ami csak a visszacsatolásos tanulásban van jelen, az a felfedezések és kizsákmányolások közötti megfelelő egyensúly megtalálása <ref type="bibr" target="#b19">[Ishii et al., 2002]</ref>. Felfedezés alatt azt értjük, hogy az ügynök egy véletlenszerű cselekedet hajt végre egy bizonyos állapotban, míg a kizsákmányolás azt jelenti, hogy az ügynök az eddigi ismeretei alapján választja a számára legnagyobb jutalommal járó cselekedetet.</p><p>Annak érdekében, hogy az ügynök maximalizálni tudja a jutalmát, olyan cselekedeteket kell válasszon, amit a múltban már kipróbált és tudja róluk, hogy sok jutalmat generáltak számára. Ahhoz viszont, hogy felfedezze a legjobb cselekedeteket, ki kell próbáljon olyanokat is, amiket a múltban még nem próbált. Tehát az ügynök ki kell zsákmányolja az eddigi tudását annak érdekeben, hogy jutalomhoz jusson, viszont fel kell fedezzen új cselekedeteket is, hogy jobb döntéseket tudjon hozni a jövőben.</p><p>A dillemát az jelenti, hogy nem követhetjük csak a felfedezést, vagy csak a kizsákmányolás elvét, mivel biztos kudarchoz jutunk <ref type="bibr" target="#b34">[Yogeswaran és Ponnambalam, 2012]</ref>. A legelterjedtebb megoldás e problémára az " mohó" ügynök (2.2), aki úgy próbálja megtalálni a felfedezések és kizsákmányolások közötti egyensúlyt, hogy az esetek 1-valószínűséggel követi az eddigi tudását és megpróbálja maximalizálni a jutalmát, míg a maradék valószínűséggel arra törekszik, hogy eddigi tudását bővítse, úgy hogy az adott állapotban egy véletlenszerű lépést visz véghez.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">FEJEZET: VISSZACSATOLÁSOS TANULÁS</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Ügynök</head><p>Biztos jutalom Ismeretlen ε=?</p><p>2.2. ábra. A kép az úgynevezett " mohó" ügynököt ábrázolja, aki az esetek többségében, pontosabban 1-valószínűséggel a biztos jutalmat választja, míg a maradék valószínűséggel, egy eddig számára ismeretlen, véletlen lépést hajt végre.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3.">A visszacsatolásos tanulás elemei</head><p>Az ügynökön és környezeten túl, a visszacsatolásos tanulásnak további négy fő alkomponense van: irányelv, jutalomfüggvény, értékfüggvény és opcionálisan egy modell a környezetről.</p><p>Az irányelv meghatározza az ügynök viselkedését adott időben. Lényegében megmondja, hogy adott időben egy állapotban milyen valószínűséggel fogunk egy bizonyos cselekedetet végrehajtani <ref type="bibr" target="#b29">[Sutton és Barto, 1998</ref>].</p><p>A jutalomfüggvény hátorzza meg az ügynök célját. Valójában minden állapot-cselekedet párhoz meghatároz egy értéket, egy úgynevezett jutalmat, ami a az állapot jóságát határozza meg. Az ügynök mindig megpróbálja maximalizálni a jutalmát és a jutalomfüggvény pedig megmondja, hogy adott pillanatban mely események gazdaságosak számára <ref type="bibr" target="#b12">[Boyan és Moore, 1995]</ref>.</p><p>A jutalomfüggvénnyel ellentétben az értékfüggvény meghatározza, hogy mi a célszerű hosszútávon.</p><p>Tehát megmondja, hogy adott állapotból kiindulva, az ügynök mennyi jutalomhoz férhet hozzá. Függetlenül attól, hogy egy állapot azonnali jutalma kicsi, hosszútávon lehet hogy célszerűbb ezt választani, mivel értékes állapotok következnek utána <ref type="bibr" target="#b30">[Sutton et al., 2000]</ref>.</p><p>A környezet modellje utánozni próbálja a környezetet, vagyis egy adott állapotra és cselekedetre meg tudja jósolni a következő állapotot és a következő jutalmat. Tervezésre használják, ami alatt azt értjük, hogy úgy választunk cselekedetet, hogy számításba vesszük a lehetséges jövőbeli állapotokat, mielőtt még megtapasztalnánk azokat <ref type="bibr" target="#b14">[Dayan és Niv, 2008</ref>]. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4.">Megoldási módszerek</head><formula xml:id="formula_0">P a (s, s ) = P r {s t+1 = s | s t = s, a t = a}</formula><p>Hasonlóan az R függvény, adott állapotra (s), cselekedetre (a) és bármilyen következő állapotra (s'), megadja a várható jutalmat: </p><formula xml:id="formula_1">R a (s, s ) = E {r t+1 | s t = s, a t = a, s t+1 = s } γ ∈ [</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4.2.">Monte Carlo módszerek</head><p>A Monte Carlo módszereknek tapasztalatra alapulnak, ami nem más, mint állapot, cselekedet és jutalom sorozat, amit vagy valós, vagy szimulált interakcióból kapunk. A valós tapasztalatból való tanulás azért fontos, mivel nem igényel modellt a környezetről, sem előzetes ismereteket a környezetről. Szimulált tapasztalaból való tanulás is hatásos lehet, viszont szükségünk van a környezet egy modelljére és időköltséges lehet szimulálni az átmeneteket <ref type="bibr" target="#b16">[Doucet et al., 2001</ref>].</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4.3.">Időbeli-differencia tanulás (Temporal-Difference learning)</head><p>A visszacsatolásos tanulás legelterjedtebb gondolata az időbeli-differencia (TD) tanulás <ref type="bibr" target="#b22">[Papadimitriou és Tsitsiklis, 1987]</ref>. Ez a módszer megpróbálja egyesíteni A Monte Carlo és dinamikus programozás módszerek elveit. Képes becsléseit valós időben firssíteni (nem szükséges megvárnia egy epizód végét) és tapasztalatból tanulni.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.5.">Időbeli-differencia tanulási módszerek</head><p>A legelterjetebb TD módszerek a Q-tanulás, Sarsa, R-tanulás, Ügynök-kritikus, TD(0) és TD(λ).</p><p>A legegyszerűbb, a TD(0), egy állapotának a t. időpillanatbeli (s t ) jóságának (V (s t )) a frissítését a 2. FEJEZET: VISSZACSATOLÁSOS TANULÁS következő képlet alapján végzi:</p><formula xml:id="formula_2">V (s t ) ← V (s t ) + α[r t + γV (s t+1 ) − V (s t )]</formula><p>ahol γ a Markov döntési folyamatban levő gamma, α egy úgynevezett tanulási ráta, ami megmondja hogy az új ismeretek milyen mennyiségben befolyásolják az eddigi tudást és r t a cselekedetért kapott jutalom <ref type="bibr" target="#b29">[Sutton és Barto, 1998</ref>]. Minimális eltérésekkel, az összes TD módszer az előbbi összefüggés alapján módosítja a feltételezéseit egy állapotról.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.5.1.">Q-tanulás</head><p>Chripstopher J. C. H. Watkins doktori disszertációjában vezette be a visszacsatolásos tanulás világába a Q-tanulást, amely környezeti modell nélkül képes optimális irányelvet találni <ref type="bibr" target="#b33">[Watkins, 1989]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>(Képes megtalálni a Q-függvényt)</head><p>A módszer arra alapszik, hogy egy adott állapot-cselekedet átmenet jóságát egyből firssíti, miután a cselekedetet végrehajtottuk <ref type="bibr" target="#b32">[Watkins és Dayan, 1992]</ref>. Legegyszerűbb formájában a Q-tanulás a következő képpen írható fel <ref type="bibr" target="#b26">[Rummery és Niranjan, 1994]</ref>:</p><formula xml:id="formula_3">Q(s t , a t ) ← Q(s t , a t ) + α r t+1 + γ max a Q(s t+1 , a) − Q(s t , a t )</formula><p>Ahol:</p><p>• Q(s t , a t ) egy mátrix, amely az állapot-cselekedet átmenetek Q értékét tartalmazza;</p><p>• r t+1 -jutalom amit a cselekedetért kaptunk;</p><p>• α -a tanulási ráta (0 &lt; α ≤ 1), ami azt mutatja, hogy az új tapasztalat mennyire fontos az ügynök számára. Amikor a tanulási ráta 0, akkor az ügynök nem tanul semmit, ellenben, amikor 1, akkor mindig csak a legújabb tapasztalatot tartja számon;</p><p>• γ -az engedmény faktor (0 &lt; γ ≤ 1), megegyezik a Marköv döntési folyamatban lévő γ -val;</p><p>• max a Q(s t+1 , a) -a becslésünk a következő állapot jutalmára; Válassz egy a cselekedetet a Q-mátrixból</p><formula xml:id="formula_4">• r t+1 + max a Q(s t+1 , a) -</formula><p>Lépd meg a-t és figyeld meg a jutalmat r és az új állapotot s'</p><formula xml:id="formula_5">Q(s t , a t ) ← Q(s t , a t ) + α r t+1 + γ max a Q(s t+1 , a) − Q(s t , a t )</formula><p>s ← s' Ameddig az s állapot nem terminális 2.3. ábra. Q-tanulás pszeudokódja <ref type="bibr" target="#b29">[Sutton és Barto, 1998</ref>] könyve alapján. A kódrészletben használt Q(s t , a t ) egy mátrix, amely az állapot-cselekedet átmenetek Q értékeit tartalmazza, r t+1 a jutalom, amit egy cselekedet elvégzéséért kapunk, α a tanulási ráta, ami az új tapasztalat fontosságát jelképezi és γ az engedmény faktor, amely meghatározza a pillanatnyi és jövőbeli jutalmak közötti fontosságot.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.5.2.">SARSA tanulás</head><p>A Sarsa algoritmust, 1994-ben Rummery és Niranjan <ref type="bibr" target="#b26">[Rummery és Niranjan, 1994]</ref> fejlesztette ki a Q-tanulás algoritmusából kiindulva.</p><p>Maga a "Sarsa" név egy rövidítés, amit R. Sutton ajánlott Rummeryéknek. A név a State-Action-Reward-State-Action sorozatból (Állapot-Cselekvés-Jutalom-Állapot-Cselekvés) származik. Már a módszer neve jelzi, hogy az állapot-cselekedet átmenetek Q értékeit milyen elgondolás alapján frissítjük.</p><p>Ahogy az alábbi összefüggésben is látszik, egy állapot-cselekedet átmenet függ az eddigi Q értéktől, a jutalomtól, amit a cselekedetért kaptunk és a következő állapot Q értékétől.</p><formula xml:id="formula_6">Q(s t , a t ) ← Q(s t , a t ) + α [ r t+1 + γ Q(s t+1 , a t+1 ) − Q(s t , a t ) ]</formula><p>A Q-tanulással ellentétben, ahol feltételezve van, hogy egy állapot után a legmagasabb Q értékű állpotba megyünk tovább ( max a Q(s t+1 , a) ), a Sarsa tanulásban követjük az eddigi irányelvünket, vagyis előrevetítjük, hogy milyen állapotba mennénk tovább ( Q(s t+1 , a t+1 ) ) <ref type="bibr" target="#b23">[Precup et al., 2001]</ref> Válassz egy a cselekedetet a Q-mátrixból Lépd meg a-t és figyeld meg a jutalmat r és az új állapotot s'</p><formula xml:id="formula_7">Válassz egy a' cselekedetet a Q-mátrixból s' állapotból kiindulva Q(s t , a t ) ← Q(s t , a t ) + α [ r t+1 + γ Q(s t+1 , a t+1 ) − Q(s t , a t ) ] s ← s' a ← a'</formula><p>Ameddig az s állapot nem terminális 2.4. ábra. SARSA-tanulás pszeudokódja <ref type="bibr" target="#b29">[Sutton és Barto, 1998</ref>] könyve alapján. A kódrészletben használt Q(s t , a t ) egy mátrix, amely az állapot-cselekedet átmenetek Q értékeit tartalmazza, r t+1 a jutalom, amit egy cselekedet elvégzéséért kapunk, α a tanulási ráta, ami az új tapasztalat fontosságát jelképezi és γ az engedmény faktor, amely meghatározza a pillanatnyi és jövőbeli jutalmak közötti fontosságot.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.5.3.">Deep Q-tanulás</head><p>Ahogy már említettük a 2.5.1 részben, ha az eseményterünk számossága meghaladja a memóriánk kapacitását, akkor le kell mondanunk a tömbös reprezentációról. Ilyen esetben kényelmes és hatékonyt megoldását nyújtanak a mély neurális hálók (angolul: deep neural networks).</p><p>A standard neurális hálók egymáshoz kapcsolódó nódusokon, mesterséges neuronokon alapulnak <ref type="bibr" target="#b28">[Schmidhuber, 2015]</ref>. (2.5) Ezek a neuronok három fő komponensből épülnek fel: bemenetből, súlyfüggvényből és aktiváló függvényből. Kapnak egy bemenetet, amit a súlyfüggvény felerősít vagy legyengít.</p><p>Ezután az aktiváló függvény meghátorozza, hogy a neuron milyen mértékben aktiválódik. Mivel egy neurális hálóban több ezer neuron található, ezért a súlyfüggvényeinek megfelelő beállítása kulcsfontosságú szerepet tölt be a kívánt eredmény eléréséhez. Azt a folyamatot, amikor ezeket a súlyfüggvényeket szabályozzuk, tanításnak (trainingnek) nevezzük <ref type="bibr" target="#b17">[Gershenson, 2003]</ref>.</p><p>Ha a bemeneti és kimeneti réteg között több, mint egy rejtett réteg is található (2.6-os ábra), akkor deep neurális hálóról beszélünk <ref type="bibr" target="#b28">[Schmidhuber, 2015]</ref>. Ezeknek a rejtett rétegeknek az a célja, hogy optimalizálja a tanulást. Segítségükkel képesek vagyunk komplexebb adatokat hatékonyabban modellezni <ref type="bibr" target="#b11">[Bengio et al., 2009</ref>].</p><p>Azért érdemes a visszacsatolásos tanulás világában ezeket a modelleket használni, mivel a problémák sémáját könnyen rá tudjuk illeszteni a hálók szerkezetére. <ref type="bibr" target="#b18">[Gu et al., 2016]</ref> A háló bemeneti rétegét a feladat állapottere, míg a kimeneti rétegét az ügynök mozgástere (cselekedet halmaza) fogja alkotni. Mivel jól tudnak függvényket közelíteni, ezért a Deep Q-tanulásban a Q függvényt így próbáljak előállítani.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">FEJEZET: VISSZACSATOLÁSOS TANULÁS</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Kimenet</head><p>x 3</p><p>x 1 x 2 2.5. ábra. Az ábrán egy egyszerű mesterséges neuron (perceptron) található, amely jelen esetben három bemenettel rendelkezik: x 1 , x 2 és x 3 , amelyekből majd egy kimenetet származtat.</p><p>Kimenet Bemenetek 2.6. ábra. Az ábrán egy deep neurális háló található, amelyen az utolsó két rejtett réteg látható, viszont a levegőből jövő nyilak sugallják, hogy még számos köztes réteg lehet a ezek és a bemenet között.</p><p>A Deep Q-tanulásban a legnagyobb kihívást az jelenti, hogy modellünk a Q függvényhez konvergáljon. Annak érdekében, hogy felgyorsítsuk a tanulási sebességet, fontos, hogy múltbéli átmenetekből, tapasztalatból is tanítsuk. Ezeket a tapasztalatokat túl költséges lenne eldobni első felhasználás után, ezért a rendszerüket folyamatosan ezekkel a régebbi emlékekkel tanítjuk <ref type="bibr" target="#b21">[Lin, 1993]</ref>. Egy módszer ennek megvalósítására az lenne, hogy ezeket az információkat egy memória csomagban tároljuk és minden epizódus végén véletleszerű mintákkal tanítanánk a neurális hálónkat.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">fejezet Alkalmazás</head><p>Összefoglaló: Az alkalmazás lesz bemutatva. Socketnek nevezzük azt a kétoldalú kommunikációt két program között, amelyek egy halózaton belül futnak és ami segítségével ezek üzeneteket tudnak küldeni és fogadni egymás között. Ez a végpont az IP cím és portszám kombinációjából tevődik össze. Így a programok tudják, hogy melyik hálózaton és melyik másik programmal kommunikálnak. <ref type="bibr">[SKT]</ref> Lua Szerver RF Ügynök Python Socket 3.1. ábra. Az ábrán megtekinthetjük az alkalmazásunk architekturális felépítést. Két fő komponenssel rendelkezik: egy Lua szerverrel, amely környezetén belül fut az ügynökünk kö0rnyezete és egy RF ügynökből, ahol az RF a Reinforcement Learninget, vagyis a visszacsatolásos tanulást jelenti. Ez a két komponens Python Socketeken keresztül kommunikál egymással.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Alkalmazás áttekintése</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Üzenetek</head><p>Ahogy említettük, a két komponensünk JSON üzeneteken keresztül kommunikál egymással. Maga a JSON szó a JavaScript Object Notation kifejezés akronímája, amely egy szintaxist határoz meg arra vonatkozóan, hogy hogyan tároljuk és dolgozzunk az adatainkkal. Egy JSON objektum a következő képen néz ki: {"kulcs" : "érték"}, ahol az érték újabb ilyen párosokat foglalhat magába. A mi applikációnkba három fajta üzenetcsoportot különböztethetünk meg: key, game és config.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.1.">Key típusú üzenetek</head><p>Miután az ügynökünk meghatározta a számára legoptimálisabb lépést, a key típusú üzenetek segítségével tudja ezt elküldeni a szervernek (környezetnek). Miután elküldtük a szervernek a kívánt lépésünket, a szerver ezt meglépi és visszaküldi nekünk a játék állását a cselekedet utáni állapotban. Egy ilyen típusú kérésnek {"key" : {"value" : "X"}} alakja van, ahol X a játékunk által felismert cselekedetek nevét veheti fel, vagyis a Fel, Le, Balra, Jobbra, Ugrás(A), Ok(B) és Start értékeket.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.2.">Config típusú üzenetek</head><p>A Config típusú üzenetcsoport a következő üzeneteket tartalmazza:</p><p>• {"conf ig" : {"divisor" : "X"}} -képesek vagyunk módosítani az elhagyni kívánt képkockáknak a számát; (frame divisor)</p><p>• {"conf ig" : {"image" : "X"}} -az emulátoron látott képnek a minőségét szabályozza;</p><p>• {"conf ig" : {"f rame" : "X"}} -a szerver hánytól kezdje el számolni a képkockákat;</p><p>• {"conf ig" : {"speed" : "X"}} -milyen gyors legyen a játékmenet.</p><p>A legnagyobb fontossága az előbbiek közül a frame divisor-nak van, mivel ez fogja meghatározni, hogy a szerver hány képkockánként várja majd az ügynökünk cselekedeteit. Ha ezt túl nagyra állítjuk be, akkor az ügynök nem tudja érzékelni időben környezete változásait és így gyengén teljesítene. Mivel a szerver, miután megkapott egy cselekedetet, addig fogja "nyomva tartani a gombot" (ismétli az utoljára kapott cselekedetet) az emulátoron, ameddig egy újabbat nem kap. Ezért túl kicsi frame divisor esetén, bizonyos cselekedetek esetén, mint például az ugrásnál az ügynök nem tudja kihasználni a teljes mozgástartományát és így nem képes minden akadályon túljutni. Számos próbálkozás után arra jutottunk, hogy a legideálisabb az, mikor két képkockánként várja a szerver az üzeneteket. Így az ügynök időben észleli a közelgő veszélyeket és egyben van elég ideje ahhoz, hogy a cselekedeteket helyesen tudja végrehatjani.</p><p>A speed típusú config üzenet valójában három értékkel párosítható. Ezek a normal, maximum és turbo. Az utolsó kettő ugyanazt a hatást eredményezi: 10-szer gyorsabbra állítja a játék menetét, mint az alapértelmezett normál mód. Mivel az ügynök gyorsabban ki tudja számolni a számára optimális lépést, mint ahogy az emulátoron a játék előrehalad, ezért nagyon fontos és hasznos opció számunkra, hogy fel tudjuk gyorsítani a játékmenetet.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.3.">Game típusú üzenetek</head><p>A game típusú üzenetekkel az ügynök információt szerezhet környezetéről. Ebbe a kategóriába az üzenetek formátuma {"game" : {"value" : "X"}} , ahol X felveheti a következő opciók egyikét: Tiles, Image, Info és Reset.</p><p>Egy Tiles üzenettel a szerver visszaküldi az ügynöknek a környezete állapotát mátrix formában.</p><p>Ahogy a 3.2-es ábrán is látjuk, az ügynök egy 13x13 as méretű tömböt kap, ahol minden tömbbeli elem értelmezési tartománya az {0, 1, 2, 3}.</p><p>Ezek jelentései:</p><p>• 0 -azt jelöli, hogy az adott pozíción nem található semmi;</p><p>• 1 -ábrázolja az összes semleges blokk és érmék helyzetét;</p><p>• 2 -jelöli az ellenségek helyzetét;</p><p>• 3 -jelképezi az ügynökünk helyzetét.</p><p>Az info típusú üzenettel az előbb leírt állapotleíráson kívül tudomást szerzünk arról, hogy ügynökünk hány élettel rendelkezik és hogy hány érmét vett fel a játék során.</p><p>Az image üzenet visszaküld egy képet a játék pillanatnyi állapotáról, míg a a reset típusú kérés törli a játék eddigi állását és elindítja újból a játékot.  <ref type="figure">1 1 1 1 1 1 1 1 1 1 1   1 1 1 1 1 1 1 1 1 1 1 1 1   0</ref>  3.2. ábra. A jobb oldali ábrán láthatjuk, hogy az ügynökünk milyen formában "érzékeli" a környezetét, ami a bal oldali ábrán található. A 3-as mutatja az ügynökünk helyét, a 2-esek jelölik az ellenséges lényeket, az 1-esek reprezentálják a semleges blokkokat és érmékét, míg a 0-ok az üres helyet jelölik.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Lua szerver</head><p>Mivel a játékunkhoz (Mario), csak egy emulator segítségével tudunk könnyen kódot írni és platformfüggetlenné tenni, ezért szükségünk volt keresni egy ilyen környezetet. Választásunk az FCEUX <ref type="bibr">[FCE]</ref> emulátorra esett, mivel ez egyben támogatja a Lua programozási nyelvet és platformfüggetlen is. Továbbá, megnyit socketen keresztül egy kommunikációs csatornát, ami egy kliens befogadására képes és bizonyos számú képkockánként üzeneteket vár tőle. Ezek az üzenetek be kell tartsák a 3.2 fejezetben leírt specifikációkat, ahhoz hogy a szerver képes legyen ezeket feldolgozni és válaszolni rájuk.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.">Python kliens</head><p>A Python kliens a lelke az alkalmazásunknak, mivel itt található az ügynökünk "agya". Ugyancsak itt vannak implementálva a visszacsatolásos tanulás különböző algoritmusai és ami a legfontosabb, hogy ugyancsak itt található a környezetünk ábrázolása is. Így, a kliensünket felbonthatjuk egy kommunikációs, egy visszacsatolásos tanulásos és egy környezet ábrázolásos részre. Ez megtekinthető a 3.3-as ábrán is. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">FEJEZET: ALKALMAZÁS</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.1.">A Lua szerver és az RF ügynök közötti kommunikációt megvalósító komponens</head><p>A kommunikációs modul tartalmazza azokat az osztályokat és metódusokat, amelyek segítségével kapcsolatot tudunk teremteni a Lua szerverrel.</p><p>A Client osztály implementálja a socket kommunikációt a Python socket csomag segítségével. Addot host és port szám megléte mellett, képes rákapcsolódni az adott socketre. Ha sikeresen megtörtént a kapcsolatteremtés, akkor tudunk unikód üzeneteket küldeni és fogadni.</p><p>A Message osztály példánya foglalja magába az üzenetünket, a szervernek szeretnénk küldeni. A 3.1 részben leírt üzeneteket képes előállítani és egy végleges üzenetbe összefűzni ezeket. A legfontosabb metódusok ebben az osztályban a clear, amely törli az aktuális üzenetünket és a finalize_message, amely kiegészíti az elküldeni kívánt üzenetet a megfelelő zárójelekkel, annak érdekében, hogy megmaradjon a helyes JSON formátum.</p><p>A Utils osztály tartalmazza azokat a metódusokat amelyek értelmezik a fentebb említett üzeneteket.</p><p>A fontosabb függvények ezek közül a tiles_matrix_from_json, amely felépíti a játék állását reprezentáló kétdimenziós tömböt, a position_from_json, amely megadja az ügynökünk aktuális pozicióját és a player_state_from_json, amely visszatéríti, hogy az ügynökünk végállapotba került-e vagy sem.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">FEJEZET: ALKALMAZÁS</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.2.">A környezet ábrázolását szolgáló modul</head><p>Ez a rész alkotja a környezeti modellünket, amely a 3.2-es alfejezetben leírt modul segítségével kommunikál az emulátorral és az attól kapott információkat feldolgozva továbbítja az ügynökünknek egy számára érthető formában. Két fontos dolgot valósít meg a környezeti modell: meghatározza a jutalom mátrixot és adott állapottérre (3.2-es ábrán látható két dimenziós tömb) meghatározza, hogy milyen állapotban vagyunk.</p><p>Egy állapot meghatározza, hogy az ügynök adott időpillanatban milyen körülmények között található. A mi esetünkben az ügynök egy 13*13-as dimenziójú tömbként érzékeli a környezetét, ahol minden tömbbeli elem négy értéket vehet fel (lásd 3.2-es fejezet) ezért összesen 676 lehetséges állapotunk van.</p><p>Mivel ez túl sok állapot ahhoz, hogy kézileg meg tudjuk adni minden állapot-cselekedet átmenetre a megfelelő jutalom értéket és ezeknek az összehangolása szinte lehetetlen feladat, ezért szükségünk volt általánosítani az állapotterünket. Olyan helyzetekben, ahol úgy éreztük, hogy az ügynökünk gyengén teljesít, megpróbáltunk új állapotokat megfogalmazni. Így, jelenleg 13 álapottal rendelkezik a környezeti modellünk. Ez a szám természetesen növelhető, annak érdekében, hogy jobb eredményeket érjünk el, viszont egy-egy új állapot behozatala a modellbe nagyon sok időbe és próbálkozásba telik.</p><p>A jelenleg definiált állapotok:</p><p>• nincs ellenség és blokk a közelünkben;</p><p>• blokk fölöttünk és nincs ellenség a közelben;</p><p>• ellenség az előttünk lévő mezőn;</p><p>• kis akadály előttünk;</p><p>• nagy akadály előttünk;</p><p>• blokk fölöttünk és ellenség több mint 2 blokk távolságban;</p><p>• levegőben vagyunk;</p><p>• nincs föld előttünk;</p><p>• beragadtunk nagy akadály előtt;</p><p>• beragadtunk kis akadály előtt;</p><p>• ellenség a hátunk mögötti mezőn;</p><p>• ellenség előttünk miközben levegőben vagyunk;</p><p>• beragadtunk akadály előtt miközben levegőben vagyunk. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">FEJEZET: ALKALMAZÁS</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Az ügynökök rangsorolása</head><p>Célunk az volt, hogy egy olyan programot alkossunk, amelyik a lehető legtöbbször viszi át a játékunkat.</p><p>Ezért, ügynökeinket, ezen aspektus alapján rangosoroltuk. (4.2-es ábra) Ahogy a 4.1-es fejezet elején is említettük, ügynökeinket 15 ezer hosszúságú epizódokra futtattuk, viszont a lentebb említett statisztikákat csak az utolsó 1000 epizód teljesítményeiből származtattuk.</p><p>A legjobb eredményt a standard Q ügynökkel értük el, amely 56.8 % -os sikerességi rátával rendelkezett. Ezt azután értük el, hogy a környezeti modellünkben módosítottuk a jutalomfüggvény értékeit.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">fejezeẗ</head><p>Osszefoglaló</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.">Következtetések</head><p>Célunk az volt, hogy visszacsatolásos tanulási módszerekkel egy olyan ügynököt hozzunk létre, amely képes átvinni a Mario játékot. Ahol jelenleg tartunk, azt valamilyen szinten sikernek tekinthetjük, megpróbáltuk modellezni a Mario játékot és eredményekre jutottunk. Bár nem sikerült maximalizálni a Mario ügynökünk sikerességét, viszont számítási képességek hiányában úgy érezzük, hogy kihoztuk a legtöbbet az adott erőforrásokból.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.">Továbbfejlesztési lehetőségek</head><p>A későbbiekben szeretnénk jobb erőforrásokat szerezni. Továbbá, jó lenne megvalósítani, hogy az ügynökünk ne csak az első pályát legyen képes átvinni, hanem az egész játékot. E cél eléréséhez új állapotokat kell definiálnunk (például vegye fel az érméket, ha van a közelében; próbáljon meg lemenni a csövekben, ha letséges) és finomítanunk kell a környezeti modellünket. A meglévő ügynökeinket ki szeretnénk bővíteni Monte Carlo 2.4.2 módszerekkel, hogy tapasztalatból is tanulhassanak. Szívesen megpróbálkoznánk a dinamikus programozás módszereivel 2.4.1 is, hogy az ügynökünk "szimulálja" a játék kimenetét minden cselekedet végrehajtása előtt és ezáltal eldönthesse, melyik lenne a következő legoptimálisabb művelet számára. Ehhez viszont, ahogy már említettük, sok memóriára és számítási képességre van szükségünk, mivel nagyon sok köztes lépés lehet, míg végállapothoz jutunk. Érdemes lenne az eddig felhasznált módszerek elveit egyesíteni, amely által saját módszert is létrehozhatnánk a feladat jobb eredményeinek elérése érdekében.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>Az alkalmazásunk célja különböző visszacsatolásos tanulási módszerek kipróbálása és ezek összehasonlítása. Ezért, nem lett volna célszerű túlbonyolítani az architektúránkat, mivel csak lassította volna a kommunikációt a komponenseink között. Ahogy a 3.1-es ábrán is látszik, projektünk két fontos részből áll: egy Lua szerverből (3.3) és egy visszacsatolásos tanulás kliensből (3.4). Ez a két modul socketen keresztül, JSON objektumokkal kommunikál egymással.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>A</head><label></label><figDesc>Lua programozási nyelv a leggyorsabb interpretált szkriptnyelv a világon. Erőssége, hogy kis méretű, (1.1 MBájt forrásfájllal és dokumentációval együtt) minden olyan rendszeren használható amelyik rendelkezik egy standard C kompilátorral és könnyen beágyazható más nyelvben írt programokba. [Lua] A LuaRocks package manager segítségével létrehozhatunk és telepíthetünk Lua modulokat. A mi esetünkben, szükségünk van a lua-cjson és lua-socket modulokra, amelyek segítségével képesek vagyunk JSON üzeneteket feldolgozni és socketekre kapcsolódni. Azért volt szükségünk a fentebb leírt modulokra és egyben a Lua környezetre, hogy képesek legyünk kommunikálni a játékkal, amit az emulátoron elindítottunk. Mivel az FCEUX támogatja a Lua szkripteket, keresnünk kellett egy modult, ami úgymond, szerverré alakítja az emulátoron futó játékot. Miután betöltünk egy szkriptet elvégezhetjük a feladatunkat miközben fut a játék, viszont a szkript felelőségge lesz biztosítani a képkockák folyamatos betöltését és a memóracímek karbantartását. Dolgozatunk célja nem az volt, hogy egy emulátoron futó játékból szervert csináljunk, ezért kerestünk egy modult, ami ezt elvégezte. Szerencsére, Marcus Edel biztosított egy ilyen csomagot azok számára, akik nem szerettek volna feltétlenül a kutatáson kívűl ezzel a problémával is foglalkozni és ezt nyílvánossá is tette az egyik github repozitoriján. [NES] Csomagja tartalmazza a szkirpteket, amelyek játékunkat szerverré alakítja. Kiolvassa a megfelelő memóriacímekről [RAM] a játék állását, ellenségek pozicióját és az elért pontszámot.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" validated="false"><head></head><label></label><figDesc>BABEŞ-BOLYAI TUDOMÁNYEGYETEM KOLOZSVÁR MATEMATIKA ÉS INFORMATIKA KAR INFORMATIKA SZAḰ</figDesc><table>Allamvizsga-dolgozat 

Visszacsatolásos tanulási 
módszerek a Mario játékra 
alkalmazva 

TÉMAVEZETŐ: 

SZENKOVITS ANNAMÁRIA, 

EGYETEMI TANÁRSEGÉD 

SZERZŐ: 

SCHNEBLI ZOLTÁN 
Tartalomjegyzék 
1. fejezet 

Bevezető 

Összefoglaló: A gépi tanulás alapjai lesznek bemutatva, a visszacsatolásos tanulással a 
középpontban. 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" validated="false"><head></head><label></label><figDesc>A dinamikus programozás olyan algoritmusokat fed, amelyek adott környezeti modell alapján hatékony döntéseket tudnak hozni. Ezek az algoritmusok nagyon precíz modellt és nagyon sok számítást ígényelnek, mivel új döntéshozatal előtt az összes eddigi cselekedet esetén megvizsgálják, hogy mennyire volt hatékony. Mivel a környezeti modellt általában nem lehetséges előállítani, ezért a többi visszacsatolásos módszerek a dinamikus programozás ezen aspektusát próbálják kijavítani<ref type="bibr" target="#b13">[Busoniu et al., 2010]</ref>.</figDesc><table>0, 1] az engedmény faktor, amely meghatározza a pillanatyni és jövőbeli jutalmak között a fontossá-

got. Ha γ értéke a 0-hoz közeledik, akkor az ügynök hajlamos lesz arra, hogy csak az azonnali jutalmakat 

vegye figyelembe, míg, ha a faktor az 1-hez áll közelebb, akkor a jövőbeli jutalmakat próbálja maxima-

lizálni [Watkins és Dayan, 1992]. 

2.4.1. Dinamikus programozás 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" validated="false"><head></head><label></label><figDesc>a tanult érték. memóriában, egy mátrixban eltárolni az átmeneteket, akkor ezeket neurális hálókkal szokták modellezni.Az utóbbi módszerről a 2.5.3 fejezetben lesz részletesebben szó.A 2.3-es illusztráción látható pszeudokód leírja egy ügynök tanulását, aki a Q-tanulás elvét követi.Minden epizód elején kell válasszon egy kezdőállapotot. Környezeti modelltől függően, ez lehet vagy 2. FEJEZET: VISSZACSATOLÁSOS TANULÁS kötött, vagy lehet véletlenszűren választani. Ezután kiválasztja és végrehajtja azt a cselekedetet, amely a legnagyobb jutalommal jár, tehát legnagyobb a Q értéke a mátrixban. Megfigyeli a jutalmat és az állapotot amibe került és frissíti az eddigi tudását a tapasztalt változások alapján a képlet segítségével.Átlép a következő állapotba és ismétli ezen lépéseket, míg el nem ért a kívánt epizódszám végéig. Egy epizód addig tart, amíg végállapotba nem kerül.</figDesc><table>Annak függvényében, hogy egy feladatban hány állapot és cselekedet áll rendelkezésre, a Q-tanulást 

kétféle képpen lehet implementálni. Ha relatív kevés állapot-cselekedet átmenetünk van, akkor érdemes 

a táblázatos módszert alkalmazni, ahol ezeket egy mátrixban reprezentáljuk. Amikor túl költséges a 

Inicializáljuk a Q mátrixot 
Ismételd Minden epizódra 

Inicializáljuk az s állapotot 
Ismételd 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5" validated="false"><head></head><label></label><figDesc>. (2.4-es ábra) 2. FEJEZET: VISSZACSATOLÁSOS TANULÁS Inicializáljuk a Q mátrixot Ismételd Minden epizódra Inicializáljuk az s állapotot Ismételd</figDesc><table></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7" validated="false"><head></head><label></label><figDesc>juk jelölni, hogy pillanatanyilag milyen állapotban vagyunk. Ennek az eredményét továbbadjuk a rejtett rétegeknek. Ezek számával és milyenségével kapcsolatosan több fajta konfigurációval próbálkoztunk, aminek az eredményeit a 4.1-es fejezetben mutatjuk be részletesebben. Az utolsó rétegünk fogja visszaadni, hogy milyen cselekedetet érdemes adott körülményekben végrehatjatni.Modellunk tanítására az ügynökünk eddigi tapasztalait használtuk fel. Egy listába eltároltuk mindig az utolsó 10000 (állapot, cselekedet, jutalom, következő állapot, végállapot-e) ötöst. Minden epizód végén választunk véletlenszerűen, rögzített számú minta ötöst az előbb említett listából. Majd a mintákból vett állapotokra megvizsgáljuk, hogy milyen cselekedeteket jósolt a hálónk. Végül súlyozzuk ezen állapotok jutalmait a Q függvény alapján 2.5.1 és frissítjük az élek súlyait a hálónkban az állapotok és jutalmak segítségével.Az ügynökeinket, megfelelő erőforrás hiányában, 15 ezer epizód erejéig futtattuk, ami körülbelül 15 órának felelt meg. Ebből az első 14 ezerben az ügynökeink tanultak, míg a maradék ezerben teszteltük az eredményeket. Ahhoz, hogy ügynökeink bármiféle eredményeket tudjanak elérni, szükséges volt ezek paramétereinek megfelelő beállítására. Ahogy a 2.4.3-as fejezetben is láttuk, a Q, Deep Q és SARSA ügynöknek szüksége van egy tanulási rátára (α) és egy lecsengési faktorra (γ).A tanulási ráta mondja meg, hogy az újonnan szerzett tapasztalat mennyire fontos az ügynök számára.Vagyis meghatározza annak a "lépésnek" a méretét, amivel az optimális stratégia felé haladunk. Ha az úgynök túl nagyokat "lép" a tanulásban, akkor megtörténhet, hogy túllépi és sohasem találná meg az optimális lépéssorozatot. Tehát divergálna.Kísérleteink kezdete során a tanulási rátát α = 0.1 -re állítottuk, mivel úgy véltük, hogy ez elég kicsi lesz. Viszont ilyen rátával a SARSA ügynökünknek csupán 2.19 százalékos sikere volt, vagyis tanítás után 1000 játékból csak 22-őt sikerült neki végigvinnie. Miután az α értékét lecsökkentettük 10 −6 -ra, ügynökünk sikerességi rátája megnőtt 50.7 százalékra.Ugyanez a jelenség megfigyelhető volt a Deep Q ügynöknél is, viszont a standard Q ügynök pont fordítottan reagált. Az α = 0.1 paraméterrel sokkal gyorsabban megtanulta az optimális lépéssorozatot és jobban teljesített (56.8%-os sikerességi ráta) a 10 −6 -os beállításhoz képest, (0%-os sikerességi ráta) ahol az ügynök fejlődése ugyan látszott, viszont még sok ezer epizódra lett volna szüksége, hogy eredményeket tudjon felmutatni. Ezenkívül megfigyelhetjük még a 4.1-es ábrán, hogy bár nagyobb α értékre az ügynök sokkal hamarabb végig tudja vinni a játékot, viszont, ahogy fentebb említettük, "túllépi" az optimalitást és többször hibáz.A lecsengési faktor megmondja, hogy az ügynökünk mennyire tervezi hosszú távon maximalizálni a jutalmait, ezért nem volt célszerű ezzel a paraméterrel kísérleteznünk. Mivel a célunk az volt, hogy az ügynök a lehető legjobban teljesítsen, ezért szükség volt rá, hogy a faktort egy 1-hez közeli értékre állítsuk, mivel így megpróbálja majd a jutalmait miaximalizálni. Az előbbi meggondolás alapján a γ-t 0.95-re állítottuk.4. FEJEZET: EREDMÉNYEK BEMUTATÁSA ÉS ÉRTÉKELÉSE4.1. ábra. A fenti képeken a standard Q ügynök teljesítményei látszanak különböző tanulási ráta értékekre. Ezek a teljesítmények 15 ezer epizód eredményei, ahol az ügynök által elért maximális távolságot rajzoltuk ki. Ahhoz, hogy láthatóbb legyen, 200-anként átlagolva vannak az értékek. A bal oldali képen az α = 0.1, míg a jobb oldalt az α = 10 −6 -os konfiguráció látszik.A Deep Q ügynök esetén, a fenti két paraméteren kívűl még meg kellett határozzuk, hogy a neurális hálójában hány rejtett rétege legyen, milyen nagy legyen az emlékezete és hogy egyszerre mennyi adattal legyünk képesek tanítani a rendszert.A hálónk összes rétege teljesen összekapcsoltak, amelyek 128 neuronnal rendelkeznek. A bemeneti és kimeneti rétegen kívűl egy rejtett rétegünk van. Próbálkoztunk még 2 rejtett réteggel is, viszont, így megduplázódtak a neuronjaink száma, amiket be kell állítson magának. Az utóbbi konfiguráció mellett 15 ezer epizód elteltével sem volt érzékelhető az ügynök tanulása. Az emlékezet nagyságát kezdetben 5000 egységre hagytuk, viszont úgy vettük észre, hogy nem igazán hajt végre új cselekedeteket bizonyos idő elteltével tanulás közben. Mivel a memóriánk megengedte, megnöveltük ennek a méretét 15000 egységre. A hálónk azon tulajdonságát, hogy egyszerre mennyi adattal tudjuk tanítani, másnéven, hogy mekkora legyen a batch mérete. Kísérleteink kezdete során a batch méretét 32-re állítottuk, viszont így nem láttunk fejlődést az ügynök fejlődésében. Ezért addig dupláztuk ezt a számot, amíg nem láttunk fejlődést. Ezt az elgondolást követve jutottunk el az 1024-es batch mérethez.</figDesc><table>A jutalommátrix kulcsfontosságú az ügynök tanításában. Rosszul konfigurált jutalommátrixszal az 

ügynökünk nem lenne képes megtanulni az optimális irányelvet. Ez a tömb meghatározza, hogy az ügy-

nök egy adott állapotban bármilyen cselekedetre mennyi jutalmat kap. Ez a jutalom a [-1,1] intervallum-

ból veszi fel az értékeit. Mivel az ügynöknek minden állapotban, minden cselekedetre kell ismernie a 

lehetséges jutalmakat, ezért egy 13*5-ös mátrixot kellett meghatároznunk. Bár az ügynökünk 7 csele-

kedetet képes elvégezni (3.2.1), mi mégis figyelmen kívűl hagytuk a Le, Fel, Start, illetve B akciókat 

és definiáltuk az átlós ugrásokat (bal-és jobb ugrás). Ezt azért tettük meg, mivel a célunk az volt, hogy 

minél gyorsabban és optimálisabban teljesítse a pályát, tehát nincs szüksége felvenni semmi segítséget. 

Segítség alatt gombákat, extra életet, pénzérméket és virágokat értünk. Ezeknek az a célja, hogy könnyít-

se a játékot a játékosok számára. (például minden 100. felvett érme után újabb életet kap a játékos) Így, 

ezeknek az akcióknak nincs jelentőségük amíg az ügynök normál formában van. 

3.4.3. A visszacsatolásos tanulási modul 

Ez a rész tartalmazza a különböző ügynökeink (Q, SARSA és Deep Q) megvalósításait, amelyek elméleti 

hátterét és működését a 2.4.3-as fejezetben mutattuk be. 

Mindhárom ügynökünk másképp közelíti meg az optimális irányelv felfedezését, viszont vannak 

hasonlóságok is közöttük. Például mindhárom ugyanazzal a konfigurációs paraméterrel fut, de erről egy 

későbbi fejezetben fogunk részletesebben beszélni. Továbbá mindegyik ügynök az mohó stratégiával 

(2.2) határozza meg következő lépését. 

A Q és SARSA ügynökkel ellentétben, ahol a Q függvényt egy tömbbel reprezentáljuk, a Deep Q 

ügynöknél ugyanezt egy neurális hálóval (2.5.3) próbáljuk elérni. A háló ábrázolásához egy Pythonban 

írt API-t, a Keras-t [Ker] használtuk, amely segítségével könnyen tudunk neurális hálókat modellezni. 

Különböző modulokat használhatunk amin a Keras futhat, mint például a CNTK [CNT], Theano [The] és 

TensorFlow [Ten]. Mi az utóbbit használtuk, mivel alapból ezt használja a Keras és minden tulajdonság-

gal rendelkezett, amire szükségünk volt. Egy szekvenciális modellt használtunk a háló reprezentálásához, 

ami annyit jelent, hogy a különböző rétegek egymás után következnek. Három réteget különítettünk el. 

Az első, ami az állapotokat fogja tőlünk megkapni lesz egyben a bemeneti rétegünk. Mivel tudjuk, hogy 

13 állapotunk van összesen, ezért a bemenetünk dimenziója egy 13x1-es tömb lesz, ahol "1"-essel fog-

4. fejezet 

Eredmények bemutatásaésértékelése 

4.1. Az ügynökök beállításai 

</table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3">. FEJEZET: ALKALMAZÁS</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3">. FEJEZET: ALKALMAZÁS</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">SARSA és Deep Q ügynök teljesítményei (epizódonként elért maximális távolságai) látszanak. Ezek a teljesítmények 15 ezer epizód eredményei, ahol az ügynök által elért maximális távolságokat rajzoltuk ki. Ahhoz, hogy a változási tendecia világosabb legyen, 200-anként átlagolva vannak az értékek. Megfigyelhető, hogy a legjobban a standard Q és SARSA ügynök teljesített</title>
	</analytic>
	<monogr>
		<title level="m">FEJEZET: EREDMÉNYEK BEMUTATÁSA ÉS ÉRTÉKELÉSE 4.2. ábra. A fenti képeken a standard Q</title>
		<imprint/>
	</monogr>
	<note>míg a Deep Q ügynök még csak tanulási fázisban van</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Ő akkor érte el ezt a teljesítményt, amikor a tanulási ráta értékét lecsökkentettük 10 −6 -ra és kivettük az algoritmusból azt az optimalizációt, amivel az ügynök felfedezési rátáját szerettük volna növelni. Ez annyiban állt, hogy amikor az ügynök kiválasztja a legnagyobb Q értékkel rendelkező cselekdetet és ennek negatív az értéke, akkor e helyett egy véletlenszerű cselekdetet hajt végre, mivel mindegy kéne legyen, hogy egy nem célszerű lépést hajt végre vagy felfedez egy új átmenetet. A leggyengébben a Deep Q ügynök teljesített, aki csupán 3.59 %-ban tudta befejezni a játékot</title>
	</analytic>
	<monogr>
		<title level="m">Ezután következik a SARSA ügynökünk, aki 50.7 %-ban teljesítette sikeresen az 1000 epizódot</title>
		<imprint/>
	</monogr>
	<note>Ahogy a 4.2 ábrán is látszik az ügynök sokkal lassabban tanul, mint a vetélytársai</note>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Egy másik rangsorolási elv lehetett volna még az elért maximális jutalom alapján, viszont arra kellett rájöjjünk, hogy a szerzett jutalom nagysága nem mindig arányos az ügynök által maximálisan elért távolsággal. (4.3-as ábra) Miután észrevettük, hogy az ügynök sokkal több jutalmat összegyűjtött, mint amilyen távolra elért</title>
		<imprint/>
	</monogr>
	<note>kiszámoltuk a jutalom és távolság statisztikák Pearson korrelációs együtthatóját</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Azt vettük észre, hogy a standard Q és SARSA ügynökök statisztikái között a korrelációs együttható +/-0.65 volt, míg a Deep Q ügyknél 0.35. Az utóbbinál azért volt kevesebb, mivel olyan lépéssorozatot tanult meg az ügynök, ahol maximalizálni tudta a jutalmát, anélkül, hogy végig kellett volna vigye a játékot. Ez a környezet modelljének a pontatlanságának róható fel</title>
	</analytic>
	<monogr>
		<title level="m">Ez a szám megmondja, hogy két változó mennyire korrelált, vagyis hogy mennyire vannak összeköttetésben</title>
		<imprint/>
	</monogr>
	<note>hogy a körenyezeti modell megtervezése a legkritikusabb lépés egy visszacsatolásos ügynök létrehozásában</note>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Fejezet: Eredmények</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bemutatása</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Értékelése</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Ezek a teljesítmények 15 ezer epizód eredményei, ahol az ügynök által elért maximális távolságot és jutalmat rajzoltuk ki. Ahhoz, hogy könnyebben leolvasható legyen a változási tendencia, 200-anként átlagoltuk az értékeket. A bal oldali képen látható, hogy az ügynök nagyon gyengén szerepelt. A pálya távolságának csupán egyharmadát teljesítette, miközben a jutalma, ami a jobb oldalon látható, azt jelöli, hogy nagyon sok jutalmat gyűjtött össze. Ez azt jelenti, hogy a jutalom mátrixban rosszul vannak definiálva az értékek</title>
		<ptr target="https://www.microsoft.com/en-us/cognitive-toolkit/" />
	</analytic>
	<monogr>
		<title level="m">A fenti képeken a Deep Q ügynök távolság és jutalom teljesítményei látszanak</title>
		<imprint>
			<biblScope unit="page" from="2017" to="2021" />
		</imprint>
	</monogr>
	<note>Irodalomjegyzék Microsoft cognitive toolkit home page</note>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Fceux</surname></persName>
		</author>
		<ptr target="http://www.fceux.com/web/home.html" />
		<imprint>
			<biblScope unit="page" from="2017" to="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Page</forename><surname>Fanuc Home</surname></persName>
		</author>
		<ptr target="https://www.fanuc.eu/" />
		<imprint>
			<biblScope unit="page" from="2017" to="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Nes</surname></persName>
		</author>
		<ptr target="https://github.com/zoq/nes" />
		<imprint>
			<biblScope unit="page" from="2017" to="2029" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Super mario bros ram map</title>
		<ptr target="https://datacrystal.romhacking.net/wiki/Super_Mario_Bros" />
	</analytic>
	<monogr>
		<title level="m">RAM_map. Accessed</title>
		<imprint>
			<biblScope unit="page" from="2018" to="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Page</forename><surname>Tensorflow Home</surname></persName>
		</author>
		<ptr target="https://www.tensorflow.org/" />
		<imprint>
			<biblScope unit="page" from="2017" to="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Learning deep architectures for ai. Foundations and trends R in Machine Learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Others</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="1" to="127" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Generalization in reinforcement learning: Safely approximating the value function</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">A</forename><surname>Boyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">W</forename><surname>Moore</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="1995" />
			<biblScope unit="page" from="369" to="376" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Reinforcement learning and dynamic programming using function approximators</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Busoniu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Babuska</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>De Schutter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ernst</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2010" />
			<publisher>CRC press</publisher>
			<biblScope unit="volume">39</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Reinforcement learning: the good, the bad and the ugly. Current opinion in neurobiology</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dayan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Niv</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="page" from="185" to="196" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">A few useful things to know about machine learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Domingos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Communications of the ACM</title>
		<imprint>
			<biblScope unit="volume">55</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="78" to="87" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">An introduction to sequential monte carlo methods</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Doucet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>De Freitas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>És Gordon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Sequential Monte Carlo methods in practice</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2001" />
			<biblScope unit="page" from="3" to="14" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Artificial neural networks for beginners</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Gershenson</surname></persName>
		</author>
		<idno>cs/0308031</idno>
		<imprint>
			<date type="published" when="2003" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Continuous deep q-learning with model-based acceleration</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Lillicrap</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>És Levine</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2829" to="2838" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Control of exploitation-exploration meta-parameter in reinforcement learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ishii</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Yoshida</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>És Yoshimoto</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural networks</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">4-6</biblScope>
			<biblScope unit="page" from="665" to="687" />
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Reinforcement learning: A survey</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">P</forename><surname>Kaelbling</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">L</forename><surname>Littman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">W</forename><surname>És Moore</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of artificial intelligence research</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page" from="237" to="285" />
			<date type="published" when="1996" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Reinforcement learning for robots using neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-J</forename><surname>Lin</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1993" />
		</imprint>
		<respStmt>
			<orgName>Carnegie-Mellon Univ Pittsburgh PA School of Computer Science</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Technical report</note>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">The complexity of markov decision processes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">H</forename><surname>Papadimitriou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">N</forename><surname>És Tsitsiklis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Mathematics of operations research</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="441" to="450" />
			<date type="published" when="1987" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Off-policy temporal-difference learning with function approximation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Precup</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">S</forename><surname>Sutton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>És Dasgupta</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2001" />
			<biblScope unit="page" from="417" to="424" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Maximum reward reinforcement learning: A non-cumulative reward criterion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">H</forename><surname>Quah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Quek</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Expert Systems with Applications</title>
		<imprint>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="351" to="359" />
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Reinforcement learning agents</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Ribeiro</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Artificial intelligence review</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="223" to="250" />
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">On-line Q-learning using connectionist systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">A</forename><surname>Rummery</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>És Niranjan</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1994" />
			<biblScope unit="volume">37</biblScope>
		</imprint>
		<respStmt>
			<orgName>University of Cambridge, Department of Engineering</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Artificial intelligence: a modern approach</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">J</forename><surname>Russell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Norvig</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">F</forename><surname>Canny</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">M</forename><surname>Malik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">D</forename><surname>Edwards</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2003" />
			<publisher>Prentice hall Upper Saddle River</publisher>
			<biblScope unit="volume">2</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Deep learning in neural networks: An overview</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural networks</title>
		<imprint>
			<biblScope unit="volume">61</biblScope>
			<biblScope unit="page" from="85" to="117" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">Reinforcement learning: An introduction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">S</forename><surname>Sutton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">G</forename><surname>Barto</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1998" />
			<publisher>MIT press Cambridge</publisher>
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Policy gradient methods for reinforcement learning with function approximation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">S</forename><surname>Sutton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">A</forename><surname>Mcallester</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">P</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>És Mansour</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2000" />
			<biblScope unit="page" from="1057" to="1063" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Reinforcement learning and markov decision processes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Otterlo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Van És Wiering</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Reinforcement Learning</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2012" />
			<biblScope unit="page" from="3" to="42" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">J</forename><surname>Watkins</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>És Dayan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Machine learning</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="issue">3-4</biblScope>
			<biblScope unit="page" from="279" to="292" />
			<date type="published" when="1992" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">Learning from delayed rewards</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">J C</forename><surname>Watkins</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1989" />
			<pubPlace>King&apos;s College, Cambridge</pubPlace>
		</imprint>
	</monogr>
	<note type="report_type">PhD thesis</note>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Reinforcement learning: exploration-exploitation dilemma in multi-agent foraging task</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Yogeswaran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ponnambalam</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Opsearch</title>
		<imprint>
			<biblScope unit="volume">49</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="223" to="236" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
