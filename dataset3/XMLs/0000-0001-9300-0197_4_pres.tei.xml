<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /mnt/sda1/Dissertation/grobid/grobid-0.5.6/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">RECURRENT NEURAL NETWORK IN NATURAL LANGUAGE PROCESSING</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sütő</forename><surname>Evelyne</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Babeş-Bolyai University</orgName>
								<address>
									<addrLine>Computer Science</addrLine>
									<settlement>Cluj-Napoca</settlement>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">RECURRENT NEURAL NETWORK IN NATURAL LANGUAGE PROCESSING</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.5.6" ident="GROBID" when="2019-12-23T23:47+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>		<profileDesc>
			<abstract/>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>NEURAL NETWORKS</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>VARIANTS OF LSTM</head><p>• The first variation of LSTM introduced in 1997 included only input and output gate withan internal state.</p><p>• LSTM was not able to reset its own internal state. To solve this the forget gate has been introduced.</p><p>• Later this model was expanded with peephole connections, connections from the cells to the gates, which can control the gates in order to make precise timings easier to learn.</p><p>• LSTM networks were improved further by replacing the mixture of</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Real Time Recurrent Learning and Backpropagation Through</head><p>Time with full Backpropagation Trough Time training.</p><p>• Generating new LSTM cells with evolutionary algorithms.</p><p>• Hybrid networks e.g. LSTM with Convolutional networks. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>PARAPHRASE GENERATION</head><p>• Paraphrasing, the act to express the same meaning in different possible ways.</p><p>• Paraphrase generation has been researched in <ref type="bibr">[Prakash et al., 2016]</ref> in 2016.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>• Conclusions</head><p>• Encoder-Decoder model with deep LSTM networks.</p><p>• They have reached state of the art results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>CONCLUSION</head><p>• Recurrent networks are ideal for sequence modeling.</p><p>• LSTM is an appropriate approach when long term dependencies have to be modeled.</p><p>• LSTMs are still popular research topics.</p><p>• LSTMs sometimes can be too complicated to train.</p><p>• The literature is still looking for simpler architectures to replace them e.g. 2D Convolutional networks. <ref type="bibr">Graves, A. and Jaitly, N. (2014)</ref>.</p><p>Towards end-to-end speech recognition with recurrent neural networks.</p><p>In International Conference on Machine Learning, pages 1764-1772.</p><p>Nallapati, R., Zhou, B., Gulcehre, C., <ref type="bibr">Xiang, B., et al. (2016)</ref>.</p><p>Abstractive text summarization using sequence-to-sequence rnns and beyond. arXiv preprint arXiv:1602.06023. Sequence to sequence learning with neural networks.</p><p>In Advances in neural information processing systems, pages 3104-3112.</p><p>Wu, Y., Schuster, M., Chen, Z., Le, Q. V., Norouzi, M., Macherey, W., <ref type="bibr">Krikun, M., Cao, Y., Gao, Q., Macherey, K., et al. (2016)</ref>.</p><p>Google's neural machine translation system: Bridging the gap between human and machine translation. arXiv preprint arXiv:1609.08144.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :Figure 2 :Figure 4 : 6 LSTMFigure 5 :Figure 6 : 8 LSTMFigure 7 :</head><label>12465687</label><figDesc>The representation of a simple Artificial Neural Network with one hidden layer. Image source: https://en.wikipedia.org/wiki/Artificial_neural_networkoutput = σ( N ∑ n=1 (w i * x i + b i )) Recurrent Neural Network . Image source: https://magenta.tensorflow.org h t = σ(W xh x t + W hh h t−1 + b h )(2)y t = W hy h t + b y Image source: http: //colah.github.io/posts/2015-08-Understanding-LSTMs/ Image source: http: //colah.github.io/posts/2015-08-Understanding-LSTMs/ LSTM Image source:http: //colah.github.io/posts/2015-08-Understanding-LSTMs/ Image source: http: //colah.github.io/posts/2015-08-Understanding-LSTMs/</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 8 :Figure 9 :Figure 10 : 5 •</head><label>89105</label><figDesc>Image source: https://medium.com/botsupply/generative-model-Image source: https://www.packtpub.com/mapt/book/big_data_and_ business_intelligence/ Image source: https://geekyisawesome.blogspot.com/2016/10/ NEURAL MACHINE TRANSLATION • Researchers have applied this model to English-French translation [Sutskever et al., 2014] in 2014 • Conclusions • LSTM sequence to sequence models are able to map even very long sentences to the translation language • Deep LSTMs can significantly outperform shallow LSTMs • Unoptimized model was able to produce state of the art results with relatively short training • State of the art result: 37 BLEU score • This model's result: 36.Researchers improved this unoptimized version [Wu et al., 2016] in 2016 to use for Google's translation system SPEECH RECOGNITION • Another application is shown in [Graves and Jaitly, 2014] done in 2014 for speech recognition system. • Conclusions • Bi-Directional LSTMs are needed to exploit future context as well. • Connectionist Temporal Classification • Model that does not need data preprocessing. • Their best score on 81 hour dataset (Wall street journal) is 8.2 while the baseline was 7.8 (word error rate/character error rate)TEXT SUMMARIZATION• Sequence to sequence model was used for text summarization in[Nallapati et al., 2016]  in 2016• Conclusions• Encoder-Decoder model with a bi-directional GRU-RNN as the encoder and a uni-directional GRU-RNN.• Address the problem of rarewords • They have reached state of the art results.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>Prakash, A., Hasan, S. A., Lee, K., Datla, V., Qadir, A., Liu, J., and Farri, O. (2016). Neural paraphrase generation with stacked residual lstm networks. arXiv preprint arXiv:1610.03098. Sutskever, I., Vinyals, O., and Le, Q. V. (2014).</figDesc></figure>
		</body>
		<back>
			<div type="references">

				<listBibl/>
			</div>
		</back>
	</text>
</TEI>
