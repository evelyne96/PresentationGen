<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /mnt/sda1/Dissertation/grobid/grobid-0.5.6/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">From CATS to SAT: Modeling Empirical Hardness to Understand and Solve Hard Computational Problems</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Leyton-Brown</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of British Columbia</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">From CATS to SAT: Modeling Empirical Hardness to Understand and Solve Hard Computational Problems</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.5.6" ident="GROBID" when="2019-12-24T15:31+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>From combinatorial auctions to supply chains and beyond, researchers in multiagent resource allocation frequently find themselves confronted with hard computational problems. This tutorial will focus on empirical hardness models, a machine learning methodology that can be used to predict how long an algorithm will take to solve a problem before it is run.</p><p>My coauthors and I first developed this line of research in our work on the Combinatorial Auction Test Suite (CATS), when investigating whether "realistic" combinatorial auction problems were always computationally easier than the hardest artificial distributions.</p><p>We did eventually figure out how to tune our instance generators to create harder instances. Along the way, however, we also developed a host of other methods that I will survey in this tutorial. These included ways of accurately predicting an algorithm's runtime on an unseen instance, determining which instance properties most affect an algorithm's performance, and building algorithm portfolios that can dramatically outperform their constituent algorithms.</p><p>After satisfying ourselves that empirical hardness models are a useful way of tackling combinatorial auction problems, we sought to demonstrate their effectiveness on a more widelystudied NP-complete problem, and hence turned to SAT. I will also describe some of the techniques we developed for this second problem domain, including the direct prediction of satisfiability status, the construction of hierarchical models, and the inclusion of incomplete local search algorithms. I will conclude by describing SATzilla, an algorithm portfolio constructed from 19 state-of-theart complete and incomplete SAT solvers, which won 5 medals at the 2007 SAT competition.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body/>
		<back>
			<div type="references">

				<listBibl/>
			</div>
		</back>
	</text>
</TEI>
