<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /mnt/sda1/Dissertation/grobid/grobid-0.5.6/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Towards Feasible PAC-Learning of Probabilistic Deterministic Finite Automata</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jorge</forename><surname>Castro</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Departament de Llenguatges i Sistemes Informàtics</orgName>
								<orgName type="laboratory">LARCA Research Group</orgName>
								<orgName type="institution">Universitat Politècnica de Catalunya</orgName>
								<address>
									<settlement>Barcelona</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ricard</forename><surname>Gavaldà</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Departament de Llenguatges i Sistemes Informàtics</orgName>
								<orgName type="laboratory">LARCA Research Group</orgName>
								<orgName type="institution">Universitat Politècnica de Catalunya</orgName>
								<address>
									<settlement>Barcelona</settlement>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Towards Feasible PAC-Learning of Probabilistic Deterministic Finite Automata</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.5.6" ident="GROBID" when="2019-12-23T23:49+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We present an improvement of an algorithm due to Clark and  Thollard (Journal of Machine Learning Research, 2004)  for PAC-learning distributions generated by Probabilistic Deterministic Finite Automata (PDFA). Our algorithm is an attempt to keep the rigorous guarantees of the original one but use sample sizes that are not as astronomical as predicted by the theory. We prove that indeed our algorithm PAClearns in a stronger sense than the Clark-Thollard. We also perform very preliminary experiments: We show that on a few small targets (8-10 states) it requires only hundreds of examples to identify the target. We also test the algorithm on a web logfile recording about a hundred thousand sessions from an ecommerce site, from which it is able to extract some nontrivial structure in the form of a PDFA with 30-50 states. An additional feature, in fact partly explaining the reduction in sample size, is that our algorithm does not need as input any information about the distinguishability of the target.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.1">Context</head><p>Probabilistic Finite-State Automata (PFA) are thoroughly studied objects, both because of its inherent theoretical interest and their applications. Probabilistic Deterministic Finite-State Automata (PDFA) are a robust and natural subclass of PFA: See <ref type="bibr" target="#b5">[6]</ref> for a study of the relations among these models, as well as HMM and POMDP.</p><p>These devices generate distributions on strings, and learning to approximate them from a sample is one of the central associated problems. A good number of algorithms have been proposed to infer PDFA. Some of them are only empirically evaluated while, for others, convergence in the limit to the target PDFA can be proven; see among others <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b1">2,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b10">11]</ref>.</p><p>In the more demanding PAC model, some evidence that learning PDFA is hard was provided by Kearns et al. <ref type="bibr" target="#b9">[10]</ref>. More precisely, it is shown in <ref type="bibr" target="#b9">[10]</ref> that assuming that noisy parities are hard to PAC-learn, distributions generated by 2-letter PDFA cannot be PAC-learned in time polynomial in n, 1/ , and 1/δ, where from now on n denotes an upper bound on the number of states in the target machine, and and δ are the usual accuracy and confidence parameters in the PAC framework.</p><p>On the other hand, Ron et al. <ref type="bibr" target="#b12">[13]</ref> gave an algorithm to PAC learn acyclic PDFA if polynomiality is measured in an additional parameter, the distinguishability of the target states -which we will denote as µ from now on. This formalized the observation, present already e.g. in <ref type="bibr" target="#b1">[2]</ref>, that one of the reasons that made some PDFA hard to learn was the presence of states with very similar suffix distributions. Clark and Thollard <ref type="bibr" target="#b2">[3]</ref> showed how to extend this result to cyclic PDFA if still another parameter, the expected length of the generated strings L is taken into account. Their work is the culmination of a line of research, in the sense of identifying a set of parameters that make polynomial-time learning possible. We will state their result precisely in Section 2.</p><p>A number of papers have since presented variations or extensions of Clark and Thollard's algorithm (for brevity, called the C-T algorithm from now on). The related paper <ref type="bibr" target="#b13">[14]</ref> by the same authors presents a more algorithmic view of the same ideas, with emphasis on the structure identification part. Palmer and Goldberg <ref type="bibr" target="#b11">[12]</ref> showed the analogous result for learning with respect to the variation (L 1 ) distance rather than the KL-distance as C-T. Guttman et al <ref type="bibr" target="#b8">[9]</ref> showed that the class of µ 2distinguishable PDFA is also learnable with respect to the KL-distance; C-T uses the easier µ ∞ -distinguishability measure. See also the related results in Guttman's thesis <ref type="bibr" target="#b7">[8]</ref>. Denis et al <ref type="bibr" target="#b4">[5]</ref> gave a quite deep PACstyle result for the full class of PFA, although the parameters in which the algorithm is polynomial are not completely identified there. Gavaldà et al. <ref type="bibr" target="#b6">[7]</ref> give another variation of C-T that adapts to the complexity of the target, in the sense that it may stop earlier than the worst-case bound if convergence is achieved.</p><p>While the Clark-Thollard result proves polynomial-time learnability of PDFA, the actual polynomial is huge for interesting parameter values. For example, it is in the order of 10 24 for |Σ| = 2, n = L = 6, and = δ = µ = 0.1. For values similar to these ones, the algorithm in <ref type="bibr" target="#b6">[7]</ref> uses sample sizes in the order of 10 5 in their experiments. On the other hand, these algorithms do not look that different from other state-merging algorithms in the literature, which often do pretty well in practice with much smaller samples.</p><p>We believe that it is an interesting question whether these huge numbers are unavoidable if PAC-like guarantees are required or whether one can design algorithms with guaranteed polynomiality in well-defined parameters that have use realistic sample sizes; or, let us say, about as realistic as those of the algorithms which have been validated empirically only. It is important to note that we discuss algorithms having no prior knowledge about the structure of the state space; otherwise, the problem is much simpler.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.2">Our Results</head><p>Our initial intention in this work was to produce an algorithm that does not ask for a bound on the distinguishability µ of the target PDFA. This value is in practice very hard to guess, and basically only trial-and-error can be used. As the work progressed, we incorporated other optimizations, all of which can still be rigorously justified. Yet, our algorithm is as easy to describe, if not more, than the original C-T algorithm.</p><p>We show that our algorithm PAC-learns in the same sense as that the C-T algorithm. In fact it learns with respect to a more demanding notion of distinguishability than the L ∞ -distance as C-T, which we call prefL ∞ -distance. This proof is the core of the paper.</p><p>While all our improvements are technical rather than conceptual, their combination could lead to dramatic experimental speedups. We describe a few experiments with an implementation of our algorithm that, we admit, are still far from being "an experimental evaluation". We first use the example PFAs in <ref type="bibr" target="#b6">[7]</ref>, having 10 states each, for which the algorithm in <ref type="bibr" target="#b6">[7]</ref> required about 4·10 5 examples to converge. Our algorithm identifies the structure of the PDFAs and achieves low error with about 200-500 examples, i.e., a reduction by a factor of 1000 w.r.t. <ref type="bibr" target="#b6">[7]</ref>. An additional example taken from <ref type="bibr" target="#b1">[2]</ref> produces similar results.</p><p>We perform an additional experiment on a large dataset: a weblog of a high-traffic Spanish online travel agent, recording about 120,000 user sessions. Each session can be modelled as a string over an alphabet of size about 90, and average length about 12. On this dataset, our algorithm is able to identify some nontrivial structure: it extracts PDFA with 30-50 states that are certainly quite different from trees. We are currently in touch with the company to assess whether the patterns embodied in the PDFA make sense to local experts.</p><p>To finish this section, let us remark an important difference of our algorithm with C-T, of a high-level rather than purely technical nature: The C-T algorithm receives a number of parameters of the target as input, computes the worst-case number of examples required for those parameters, and asks for the full sample of that size upfront. Rather, we place ourselves in the more common situation where we have a given, fixed sample and we have to extract as much information as possible from it. Our main theorem then says that the algorithm PAC-learns provided this sample is large enough with respect to the target's parameters (some of which, such as µ, are unknown; we are currently working on removing the need to have the other parameters as inputs).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Preliminaries</head><p>We essentially follow notation in <ref type="bibr" target="#b2">[3]</ref>. A PDFA A is a tuple Q, Σ, τ, γ, ξ, q 0 where Q is a finite set of states, Σ is the alphabet, τ : <ref type="bibr" target="#b0">1]</ref> defines the probability of emitting each symbol from each state (γ(q, σ) = 0 when σ ∈ Σ and τ (q, σ) is not defined), ξ is a special symbol not in Σ reserved to mark the end of a string, and q 0 ∈ Q is the initial state. Transition function τ is extended to Q × Σ in the usual way.</p><formula xml:id="formula_0">Q × Σ −→ Q is the transition function, γ : Q × (Σ ∪ {ξ}) −→ [0,</formula><p>Given an observation string xξ = σ 0 . . . σ k ξ emitted by a known PDFA A, the state at each step can be tracked by starting from the initial state q 0 and following the labelled transitions according to x until reaching last symbol ξ. Also, the probability of generating a given string xξ from state q can be calculated recursively as follows: if x is the empty word λ the probability is γ(q, ξ), otherwise x is a string σ 0 σ 1 . . . σ k with k ≥ 0 and γ(q, σ 0 σ 1 . . . σ k ξ) = γ(q, σ 0 )γ(τ (q, σ 0 ), σ 1 . . . σ k ξ).</p><p>The probability of state q in PDFA A is defined as the sum of values γ(q 0 , xξ), where x ranges over the set of strings in Σ that traverse q.</p><p>Assuming every state of A has non-zero probability of generating some string, one can define for each state q a probability distribution D A q on Σ : For each x, probability D A q (x) is γ(q, xξ). The one corresponding to the initial state D A q 0 is called the distribution defined by A, written D A in short. When there is no ambiguity, we will omit superindex A.</p><p>Given a multiset S of strings from Σ we denote by S(x) the multiplicity of x in S, write |S| = x∈Σ S(x) and for every σ ∈ Σ define S(σ) = x∈Σ S(σx). To resolve the ambiguity of this notation on strings of length 1, we will always use greek letters to mean elements of Σ, and latin letters for strings. We also denote by S(ξ) the multiplicity of the empty word, S(λ). To each multiset S corresponds an empirical distributionŜ defined in the usual way,Ŝ(x) = S(x)/|S|. Finally, prefixes(S) denotes the multiset of prefixes of strings in S.</p><p>We consider several measures of divergence between distributions. Let D 1 and D 2 be probability distributions on Σ . The Kullback-Leibler divergence, KL for short, is defined as</p><formula xml:id="formula_1">KL(D 1 , D 2 ) = x D 1 (x) log D 1 (x) D 2 (x) .</formula><p>The L ∞ supremum distance is</p><formula xml:id="formula_2">L ∞ (D 1 , D 2 ) = max x∈Σ |D 1 (x) − D 2 (x)|.</formula><p>Finally, we also use the supremum distance on prefixes (introduced here, as far as we know):</p><formula xml:id="formula_3">prefL ∞ (D 1 , D 2 ) = max x∈Σ |D 1 (xΣ ) − D 2 (xΣ )|. Definition 1. We say distributions D 1 and D 2 are µ-distinguishable when µ ≤ max{L ∞ (D 1 , D 2 ), prefL ∞ (D 1 , D 2 )}. A PDFA A is µ-distinguishable</formula><p>when for each pair of states q 1 and q 2 their corresponding distributions D q 1 and D q 2 are µ-distinguishable.</p><p>Observe that prefL ∞ (D q 1 , D q 2 ) ≥ µ iff there is any x ∈ Σ such that |γ(q 1 , x)−γ(q 2 , x)| ≥ µ. By definition, our measure of distinguishability is never smaller than the usual L ∞ -distinguishability in the literature <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b11">12]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Description of the Algorithm</head><p>We show below a learning algorithm for PDFAs that has as input parameters the alphabet size |Σ|, an upper bound L on the expected length of strings emitted from any state of the target (alternatively, a bound on the expected length of strings from the initial state and a bound on the variance), an upper bound n on the number of states of the target, and the confidence (δ) and precision ( ) parameters. In contrast with the C-T algorithm, it does not need as input parameter the distinguishability µ of the target. According to <ref type="bibr" target="#b2">[3]</ref>, a PAC learner for the class of PDFA can be easily obtained from a polynomial-time algorithm, so-called Learner from now on, satisfying the requirements listed below; we follow their notation.</p><p>1. Learner returns (with high probability) a graph G isomorphic to a subgraph of the target PDFA A. This means that there is a bijection Φ from a subset of states of A to all nodes of G such that 1) Φ(q 0 ) = v 0 (where q 0 , v 0 are the initial states of A and G, respectively) and 2) if</p><formula xml:id="formula_4">τ G (v, σ) = w then τ (Φ −1 (v), σ) = Φ −1 (w). 2.</formula><p>The states in A whose probability is greater than 2 /(L + 1), which we call frequent states, have a representative in G. That is Φ is defined on frequent states. 3. If q is a frequent state in A and σ ∈ Σ is such that γ(q, σ) &gt; 5 (we say (q, σ) is a frequent transition) then τ G (Φ(q), σ) exists and it equals Φ(τ (q, σ)).</p><formula xml:id="formula_5">4. A multiset S v is attached to every node v in the graph. If v represents a frequent target state q (i.e., Φ(q) = v where q is frequent), then for every σ ∈ Σ ∪ {ξ}, it holds |S v (σ)/|S v | − γ(q, σ)| &lt; 1 .</formula><p>A multiset holding this property is said to be 1 -good.</p><p>Numbers 1 , 2 and 5 above and auxiliar quantities 0 and δ 0 that we use later are defined as follows. Note that they do not depend on µ. n|Σ|(L + 1) δ 0 = δ n 2 |Σ| + 3n|Σ| + n From a such graph G a PDFA hypothesis H can be easily built having a small KL divergence with respect to A. This is described in the paragraphs "Completing the Graph" and "Estimating Probabilities" in <ref type="bibr" target="#b2">[3]</ref>, page 480. Basically, it is enough to complete the graph when necessary by introducing a new node, the ground node, representing all the low frequency states and new transitions to the ground node. Finally, a smoothing scheme is performed in order to estimate the transition probabilities.</p><p>The proof that an algorithm Learner with these properties, plus this additional graph completion and probability estimation step, is a PAClearner is essentially the contents of Sections 4.3, 4.4 and 5 in <ref type="bibr" target="#b2">[3]</ref>. It does not involve distinguishability at all, so we can apply it in our setting even if we have changed our measure of distinguishability.</p><p>Our learning algorithm takes as inputs the parameters listed above and a sample from the target machine containing N examples. Learner performs at most n|Σ| + 1 learning stages, each one making a pass over all training examples and guaranteed to add one transition to the graph G it is building.</p><p>At the beginning of each stage, Learner has a graph G that summarizes our current knowledge of the target A. Nodes and edges in G represent, respectively, states and transitions of the target A. We call safe nodes the nodes of G, as they are inductively guaranteed (with probability at least 1 − δ 0 ) as stand for distinct states of A, with transitions among them as in A. Safe nodes are denoted by strings in Σ .</p><p>Attached to each safe node v there is a multiset S v that keeps information about the distribution on the target state represented by v. The algorithm starts with a trivial graph G consisting of a single node v 0 = λ representing the initial state q 0 of the target, whose attached multiset is formed by all the available examples.</p><p>When a new stage starts, the learner adds a candidate node u = v u σ for each (safe) node v u of G and each σ ∈ Σ such that τ G (v u , σ) is undefined. Candidate nodes gather information about transitions of A leading from states that have already a safe representative in G but not having yet an edge counterpart in G. Attached to each candidate node u there is also a multiset S u , initially empty. The learner also keeps, for each candidate node u, a list L u of safe nodes that have not been yet distinguished (proved different) from u. Initially L u contains all nodes in G.</p><p>For each training example xξ = σ 0 . . . σ i−1 σ i σ i+1 . . . σ k ξ in the dataset, Learner traverses the graph matching each observation σ i to a state until either (1) all observations in x have been exhausted or (2) a transition to a candidate node is reached. This occurs when all transitions up to σ i−1 are defined and lead to a safe node w, but there is no edge out of w labeled by σ i . In this case, we add the suffix σ i+1 . . . σ k to the multiset S u of candidate node u = wσ i and, before processing a new example, we consider all pairs (u, v) where safe node v belongs to L u . For each such pair, we call function Test Distinct(u, v) described in <ref type="figure" target="#fig_1">Figure 1</ref>. If the test returns "distinct", we assume that u and v reach distinct states in the target and we remove v from L u .</p><p>A candidate node becomes important when |S u | exceeds 0 N/2. Every time that there is an important candidate node u = v u σ whose associated set L u is empty, u is promoted to a new safe node (G gets a new node labelled with u), u is not anymore a candidate node, and an edge from v u to u labeled by σ is added to G. The multiset S u is attached to the new safe node, u is included in the list L u of all remaining candidate nodes u , and the phase continues.</p><p>If all training examples are processed without the condition above occurring, the algorithm checks whether there are any important candidate nodes left. If none remains, Learner returns G and stops. Otherwise, it closes the phase as follows: It chooses the important candidate u = v u σ and the safe node v ∈ L u having smallest distinguishability on the empirical distributions (samples), and identifies them, by adding to G an edge from v u to v labeled by σ.</p><p>Finally, the phase ends by erasing all the candidate nodes and another phase starts.</p><formula xml:id="formula_6">function Test Distinct(u, v) //u is a candidate node; v is safe m u ← |S u |; s u ← | prefixes(S u )|; m v ← |S v |; s u ← | prefixes(S v )|; t u,v ← 2 min(mu,mv) ln 4m 2 u (su+sv)π 2 3δ 0 1/2 d ← max L ∞ (Ŝ u ,Ŝ v ), prefL ∞ (Ŝ u ,Ŝ v )</formula><p>if d &gt; t u,v then return "distinct" else return "not clear" </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Analysis</head><p>In this section we show that algorithm Learner satisfies conditions (1)-(4) before, and therefore can be turned into a PAC-learner for PDFA. The following two lemmas describe the behavior of Test Distinct. Here D u (respectively, D v ) denotes the target distribution on state q = τ (q 0 , u) (q = τ (q 0 , v)).</p><formula xml:id="formula_7">Lemma 2. If D u = D v function Test Distinct(u, v) returns "not clear" with probability 1 − 6δ 0 /(π 2 m 2 u ). Proof. Let D = D u (= D v ). Function Test Distinct returns "different" when there exists a string x such that |Ŝ u (xΣ ) −Ŝ v (xΣ )| &gt; t u,v or |Ŝ u (x) −Ŝ v (x)| &gt; t u,v . First, we bound the probability of the event prefL ∞ (Ŝ u ,Ŝ v ) &gt; t u,v .</formula><p>To avoid summing over infinitely many x, consider S u ∪S v ordered, say lexicographically. Then the event above is equivalently to saying "there is an index i in this ordering such that some prefix of the ith string in S u ∪ S v in this ordering, call it x, satisfies the condition above". (This is another way of saying that only x's appearing in prefixes(S u ∪ S v ) can make the inequality true, since all others havê S u (xΣ ) =Ŝ v (xΣ ) = 0.) Therefore, its probability is bounded above by the maximum of (s u + s v ) Pr[ |Ŝ u (xΣ ) −Ŝ v (xΣ )| &gt; t u,v ] over all strings x. By the triangle inequality, this is at most</p><formula xml:id="formula_8">(s u +s v ) Pr[ |Ŝ u (xΣ ) − D(xΣ )| &gt; t u,v /2] + Pr[|Ŝ v (x) − D(xΣ )| &gt; t u,v /2] . Since E[Ŝ u (xΣ )] = E[Ŝ v (xΣ )] = D(xΣ )</formula><p>, by Hoeffding's inequality this is at most</p><formula xml:id="formula_9">(s u + s v )(2 exp(−2(t 2 u,v /4) m u ) + 2 exp(−2(t 2 u,v /4) m u )) ≤ 4(s u + s v ) exp(−(t 2 u,v /2) min(m u , m v )),</formula><p>which is 3δ 0 /(π 2 m 2 u ) by definition of t u,v . A similar reasoning also shows that the probability of the event L ∞ (Ŝ u ,Ŝ v ) &gt; t u,v is at most 3δ 0 /(π 2 m 2 u ) and we are done. Proof. We first bound the size of prefixes(S u ∪ S v ). Clearly, its expected size is at most L|S u ∪S v |. Then, by Markov's inequality,</p><formula xml:id="formula_10">Pr[| prefixes(S u ∪ S v )| ≥ 2 δ 0 ·L|S u ∪S v |]</formula><p>is less than δ 0 /2. Therefore, we have with probability at</p><formula xml:id="formula_11">least 1 − δ 0 /2 that s u + s v ≤ 2(m u + m v )L/δ 0 .</formula><p>Now assume there is a string x witnessing that prefL ∞ (D u , D v ) &gt; µ (otherwise some x is a witness for L ∞ (D u , D v ) &gt; µ and we argue in a similar way), i.e. a string such that</p><formula xml:id="formula_12">|D u (xΣ ) − D v (xΣ )| &gt; µ. If min(m u , m v ) ≥ 8 µ 2 ln 8(mu+mv)m 2 u Lπ 2 3δ 2 0</formula><p>, by the argument above with high probability we have t u,v ≤ µ/2 and the probability of returning "different" is at least the probability of the event |Ŝ u (xΣ ) −Ŝ v (xΣ )| &gt; µ/2. The hypothesis on x and the triangle inequality shows that probability of the complementary event |Ŝ u (xΣ ) −Ŝ v (xΣ )| ≤ µ/2 is at most</p><formula xml:id="formula_13">Pr[ |Ŝ u (xΣ ) − D u (xΣ )| &gt; µ/4] + Pr[ |Ŝ v (xΣ ) − D v (xΣ )| &gt; µ/4].</formula><p>By the Hoeffding bound, this sum is less than δ 0 /2, and we are done.</p><p>Lemmas 4-8 below share the hypothesis the current G is isomorphic to a subgraph of A and deal with one fixed stage of the learning algorithm. Probabilities are taken over samples.</p><p>Lemma 4. Let u be a candidate node. If u is promoted to safe (in this stage) then, with probability 1 − δ 0 , node u corresponds to a new target state, i.e., one not represented in the current graph G.</p><p>Proof. We show that a candidate node u representing the same target state than a safe state v has very small probability of being promoted. By Lemma 2 at any specific call to Test Distinct(u, v), the function returns the wrong value "different" with probability at most 6δ 0 /(π 2 m 2 u ). The test is called once for every example included in S u , so the value of m u increases by 1 at each consecutive call within the stage. Therefore, the probability that safe node v is ruled out from L u is at most mu≥1 6δ 0 /(π 2 m 2 u ) = δ 0 Lemma 5. Let u be a candidate node, and let µ be the distinguishability of the target. If N is greater than 16 0 µ 2 (3e ln 48 0 µ 2 +ln 16Lπ 2 3δ 2 0 ) with probability 1 − nδ 0 , if candidate node u is merged with a safe node v then, strings u and v end in the same state in the target.</p><p>Proof. Assume candidate u is merged with safe node v. Necessarily u is important and N ≥ m u &gt; 0 N/2. As v is safe it also holds m v &gt; 0 N/2. It is also clear that m u + m v ≤ 2N . From these values of m u , m v and the hypothesis on N , it can be checked that min(m u , m v ) satisfies requirement in Lemma 3. So, if they were representatives of different target states, safe v would remain in L u with probability at most δ 0 . Lemma 6. Assume that, after processing all examples, graph G has no safe node v representing a frequent state q of A. If N &gt; 8(L+1) 2 ln 1 δ 0 then, with probability 1 − δ 0 , the learner will not finish yet.</p><p>Lemma 7. Assume that, after processing all examples, graph G has no edge representing a frequent transition (q, σ) in A. If N &gt; 8(L+1) <ref type="bibr">2 5</ref> ln 1 δ 0 then with probability 1 − δ 0 , some candidate is important and the learner will not finish yet. and u is promoted to safe then, with probability 1 − δ 0 , multiset S u is 1 -good.</p><formula xml:id="formula_14">Let N 0 be max 16 0 µ 2 (3e ln 48 0 µ 2 + ln 16Lπ 2 3δ 2 0 ), 8 0 2 1 ln 2(|Σ|+1) δ 0 .</formula><p>A straightforward induction shows the main theorem: Theorem 9. If N &gt; N 0 , with probability 1−δ Learner returns a graph G satisfying requirements (1)-(4) listed above.</p><p>As explained already, the second phase of the learning algorithm takes the graph G, completes it if necessary, and sets the transition probabilities according to their empirical distribution. An additional smoothing is performed, as described in <ref type="bibr" target="#b2">[3]</ref>. We do not describe it here, but state that the resulting PDFA will approximate the target in the KL distance, as can be deduced from the proof in <ref type="bibr" target="#b2">[3]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Small Targets</head><p>Our first experiments used the two automata tested by Gavaldà et al. <ref type="bibr" target="#b6">[7]</ref>, shown in <ref type="figure" target="#fig_3">Figure 2</ref>. The one on the left is a (nondeterministic) HMM repeatedly generating strings in {abb, aaa, bba} with different probabilities. The one on the right is the "cheese maze": at each state (or square), an observation (a letter in {1, 2, 3}) indicates the number of walls around that state, with the exception of s10 where the automaton terminates. They have thus 10 states each. We have additionally used the Reber grammar automaton, with 8 states, discussed in <ref type="bibr" target="#b1">[2]</ref> and shown in <ref type="figure" target="#fig_2">Figure 3</ref>.</p><p>For each of these automata, and different values of N , we generated 10 examples of size N from the target, and run our algorithm on these examples. In all experiments we used δ = 0.05 and the (known) number of target states for n. For the Reber grammar, the full structure of the automaton was identified about half the times with N = 100, but systematically identified when N = 200, at which point transition probabilities were correct within (absolute) 5%. For N = 1000, transition probabilities were correct within 1%. For the cheese maze automaton, at N = 300 the structure was found 9 out of 10 times, with transition probabilities correct up to 2%. For N = 1000, the structure was correctly found in all trials. Interestingly, when the program was changed to use only L ∞ -, rather than prefL ∞ -distinguishability, the point at which the structure is identified more than 50% of the times was around N = 1300. That is, using prefL ∞ did help in this case. Results were similar for the HMM on the left of <ref type="figure" target="#fig_3">Figure 2</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">An Experiment with a Real, Large Dataset</head><p>As a larger test, we used a web logfile recording sessions from a high-traffic Spanish online travel agency, selling flights, hotel stays, car rentals, and theater tickets. Each entry in the logfile records a user request to the company's web to initiate some action. The local experts distinguish 91 types of requests or tags; some examples could be "search flight", "search hotel", "book flight", "credit card info", "home", "register", "help", etc. We preprocessed the logfile transforming each request into a tag identifier and grouping clicks from the same user into sessions. Therefore each session can be viewed as a string over a 91letter alphabet. The median and average of session length are 4 and 11.9, excluding 1-click sessions, and we had 120,000 sessions to work with.</p><p>We ran our algorithm on subsets of several sizes N of this dataset. Since human web users cannot be perfectly modelled by PDFA, one should not expect high accuracy. Still, there are certainly patterns that users tend to follow, so it was worth checking whether any structure is found.</p><p>We tried N = 40, 000 to N = 100, 000 in multiples of 10, 000. <ref type="figure">Figure 4</ref> presents, for each N , the size of the resulting PDFA and the L 1 -distance from the dataset to a randomly generated sample of size 100, 000 from the output machine. Note that the L 1 distance can have value up to 2. (In fact, we tried generating several independent samples of size 100k from the PDFA obtained with the 100k sample and computing their L 1 mutual distance. The results were around 0.39 even though they came from the same machine, so this value is really the baseline once sample size has been fixed to 100k.) The table shows that convergence to a fixed machine did not occur, which is no surprise. On the other hand, the resulting machines were not at all tree-like PDFAs that occur when no states can be merged: most safe states did absorb candidate states at some point. Given the alphabet size (91) and number of states (≥ 30), depicting and understanding them is not immediate.</p><p>Note that we have not discussed the values of and L used in the experiments. In fact, our implementation does not use them: they are used only to determine when a state is important. In particular, observe that and L are not used in the state distinctness test. The implementation keeps merging candidate states as long as there is any left at the end of the stage, hence every candidate state is eventually merged. We believe that it is possible to prove that this variant is still a PAC learner, since non-important states, after smoothing, by definition do not contribute much to the KL distance. We believe it is also possible to remove the need for an upper bound on n without significantly increasing sample size in practice; this would give a PAC-learning whose only parameter is the confidence δ.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusions</head><p>We believe that these first experiments, as preliminary as they are, show that maybe one cannot rule out the existence of a provably-PAC learner that has reasonable sample sizes in practice. More systematic experimentation, as well as improving of our slow, quick-and-dirty implementation, is work in progress.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 1 .</head><label>1</label><figDesc>The state-distinctness test</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Lemma 3 .</head><label>3</label><figDesc>If D u and D v are µ-distinguishable and min(m u , m v ) Distinct(u, v) returns "different" with probability 1 − δ 0 .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 2 .</head><label>2</label><figDesc>Example PFAs from<ref type="bibr" target="#b6">[7]</ref> Lemma 8. Let u be a candidate node. If the number N of training examples is greater than 1</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 3 .</head><label>3</label><figDesc>The Reber grammar; plot taken from<ref type="bibr" target="#b1">[2]</ref> </figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" validated="false"><head></head><label></label><figDesc>Fig. 4. Results on the online travel agency dataset</figDesc><table>Sample # states L1 distance 
40k 
35 
.582 
50k 
36 
.546 
60k 
39 
.518 
70k 
42 
.516 
80k 
45 
.480 
100k 
54 
.439 

</table></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Learning stochastic regular grammars by means of a state merging method</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">C</forename><surname>Carrasco</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Oncina</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICGI</title>
		<imprint>
			<date type="published" when="1994" />
			<biblScope unit="page" from="139" to="152" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Learning deterministic regular grammars from stochastic samples in polynomial time</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">C</forename><surname>Carrasco</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Oncina</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ITA</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="20" />
			<date type="published" when="1999" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">PAC-learnability of probabilistic deterministic finite state automata</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Thollard</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page" from="473" to="497" />
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Identification of DFA: data-dependent vs data-independent algorithms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>De La Higuera</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Oncina</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Vidal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICGI</title>
		<imprint>
			<date type="published" when="1996" />
			<biblScope unit="page" from="313" to="325" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Learning rational stochastic languages</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Denis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Esposito</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Habrard</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">COLT</title>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="page" from="274" to="288" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Links between probabilistic automata and hidden Markov models: probability distributions, learning models and induction algorithms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dupont</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Denis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Esposito</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition</title>
		<imprint>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="page" from="1349" to="1371" />
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">PAC-learning of Markov models with hidden state</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Gavaldà</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">W</forename><surname>Keller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pineau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Precup</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECML</title>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="page" from="150" to="161" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Probabilistic Automata Distributions over Sequences</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Guttman</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2006-09" />
		</imprint>
		<respStmt>
			<orgName>The Australian National University</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Ph.D. thesis</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Learnability of probabilistic automata via oracles</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Guttman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">V N</forename><surname>Vishwanathan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">C</forename><surname>Williamson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ALT</title>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="page" from="171" to="182" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">On the learnability of discrete distributions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">J</forename><surname>Kearns</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Mansour</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Rubinfeld</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">E</forename><surname>Schapire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Sellie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">STOC</title>
		<imprint>
			<date type="published" when="1994" />
			<biblScope unit="page" from="273" to="282" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Stochastic grammatical inference with multinomial tests</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Kermorvant</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dupont</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICGI</title>
		<imprint>
			<date type="published" when="2002" />
			<biblScope unit="page" from="149" to="160" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">PAC-learnability of probabilistic deterministic finite state automata in terms of variation distance</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Palmer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">W</forename><surname>Goldberg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ALT</title>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="page" from="157" to="170" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">On the learnability and usage of acyclic probabilistic finite automata</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Singer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Tishby</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Comput. Syst. Sci</title>
		<imprint>
			<biblScope unit="volume">56</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="133" to="152" />
			<date type="published" when="1998" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Learning stochastic deterministic regular languages</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Thollard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Clark</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICGI</title>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="page" from="248" to="259" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Probabilistic DFA inference using Kullback-Leibler divergence and minimality</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Thollard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dupont</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>De La Higuera</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2000" />
			<biblScope unit="page" from="975" to="982" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
