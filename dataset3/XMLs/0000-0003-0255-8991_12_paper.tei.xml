<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /mnt/sda1/Dissertation/grobid/grobid-0.5.6/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Frame-Level Features Conveying Phonetic Information for Language and Speaker Recognition Frame-Level Features Conveying Phonetic Information for Language and Speaker Recognition</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2015-06">June 2015</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mireia</forename><surname>Diez Sánchez</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Department of Electricity and Electronics</orgName>
								<orgName type="department" key="dep2">Faculty of Science and Technology Department of Electricity and Electronics</orgName>
								<orgName type="laboratory" key="lab1">Software Technologies Working Group</orgName>
								<orgName type="laboratory" key="lab2">UNIVERSITY OF THE BASQUE COUNTRY Abstract</orgName>
								<orgName type="institution">University of The Basque Country</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><roleName>Dr</roleName><forename type="first">Dra</forename><forename type="middle">Amparo</forename><surname>Varona</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Department of Electricity and Electronics</orgName>
								<orgName type="department" key="dep2">Faculty of Science and Technology Department of Electricity and Electronics</orgName>
								<orgName type="laboratory" key="lab1">Software Technologies Working Group</orgName>
								<orgName type="laboratory" key="lab2">UNIVERSITY OF THE BASQUE COUNTRY Abstract</orgName>
								<orgName type="institution">University of The Basque Country</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">German</forename><surname>Bordel</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Department of Electricity and Electronics</orgName>
								<orgName type="department" key="dep2">Faculty of Science and Technology Department of Electricity and Electronics</orgName>
								<orgName type="laboratory" key="lab1">Software Technologies Working Group</orgName>
								<orgName type="laboratory" key="lab2">UNIVERSITY OF THE BASQUE COUNTRY Abstract</orgName>
								<orgName type="institution">University of The Basque Country</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mireia</forename><surname>Diez Sánchez</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Department of Electricity and Electronics</orgName>
								<orgName type="department" key="dep2">Faculty of Science and Technology Department of Electricity and Electronics</orgName>
								<orgName type="laboratory" key="lab1">Software Technologies Working Group</orgName>
								<orgName type="laboratory" key="lab2">UNIVERSITY OF THE BASQUE COUNTRY Abstract</orgName>
								<orgName type="institution">University of The Basque Country</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Frame-Level Features Conveying Phonetic Information for Language and Speaker Recognition Frame-Level Features Conveying Phonetic Information for Language and Speaker Recognition</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2015-06">June 2015</date>
						</imprint>
					</monogr>
					<note>Author: Supervisors:</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.5.6" ident="GROBID" when="2019-12-23T23:49+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This Thesis, developed in the Software Technologies Working Group of the Department of Electricity and Electronics of the University of the Basque Country, focuses on the research field of spoken language and speaker recognition technologies.</p><p>More specifically, the research carried out studies the design of a set of features conveying spectral acoustic and phonotactic information, searches for the optimal feature extraction parameters, and analyses the integration and usage of the features in language recognition systems, and the complementarity of these approaches with regard to state-of-the-art systems. The study reveals that systems trained on the proposed set of features, denoted as Phone Log-Likelihood Ratios (PLLRs), are highly competitive, outperforming in several benchmarks other state-of-the-art systems. Moreover, PLLR-based systems also provide complementary information with regard to other phonotactic and acoustic approaches, which makes them suitable in fusions to improve the overall performance of spoken language recognition systems.</p><p>The usage of this features is also studied in speaker recognition tasks. In this context, the results attained by the approaches based on PLLR features are not as remarkable as the ones of systems based on standard acoustic features, but they still provide complementary information that can be used to enhance the overall performance of the speaker recognition systems.</p><p>It's been more than five years since I started this journey, five not always easy but still incredible years, in which I've grown, I've changed and most of all I've learned.</p><p>I owe that to a lot of people, I've been lucky enough to find a lot of people to learn from along the way. First of all thanks to the Department of Education, Universities and Research of the Basque Government for the pre-doctoral fellowship (BFI09.263/AE) that allowed me start my research career. Thanks to the Basque Government too for the economic support conceded by the SAIOTEK projects S-PE11UN065, S-PE12UN055 and S-PE13UN105. To the University of the Basque Country for the support provided by the group grants GIU10/18 and GIU13/28. And to the Spanish Ministry of Science and Innovation (MICINN) for the project TIN2009-07446.</p><p>Thanks to all GTTS members. Thanks for putting trust in me and introducing me to the speech technologies world. Thanks for making me feel part of the group since day one, for all the ideas, meetings and debates that have made sense of all these years of work. Germán, for your refreshing new points of view when I was in need of them, and for shaping this manuscript. Luisja, for that first e-mail that sparked my interest for this field and for the re-revisions, I don't think I would have learned to write anything decent if it wasn't for you. Mikel, zure patzientziagatik, kodigoa eta artikuloak destripatzen egon garen hainbeste orduengatik, zure dedikazio eta entusiasmoa transmititzeagatik, dakidan gehiena zuri esker da. Amparo, for being much more than a thesis advisor, for the orientation, the organization, for all the chatting hours, and the big support you've been through all these years. Guztioi, benetan, mila esker.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Esta tesis, desarrollada en el Grupo de Trabajo en Tecnologías del Software del Departamento de Electricidad y Electrónica de la universidad del País Vasco, se centra en el campo de investigación del reconocimiento de la lengua y del locutor.</p><p>Más específicamente, la investigación llevada a cabo estudia el diseño de un conjunto de características que aportan información espectro-acústica y fonotáctica, busca la configuraciónóptima para la extracción de las mismas, y analiza la integración y uso de las características en sistemas de reconocimiento de la lengua, así como la complementariedad de estas aproximaciones con respecto a otros sistemas acústicos y fonotácticos actualmente punteros. El estudio revela que los sistemas entrenados con estas características, que podríamos denominar cocientes de log-probabilidades de fonemas (PLLRs, de sus siglas en inglés), son altamente competitivos, superando en rendimiento a otros sistemas. Además, los sistemas basados en estas características también proporcionan información complementaria con respecto a otras aproximaciones fonotácticas y acústicas, lo que los hace adecuados en fusiones, para mejorar el rendimiento general de los sistemas de reconocimiento.</p><p>También se estudia el uso de las características en tareas de verificación de locutor. En este contexto, los resultados obtenidos por las aproximaciones basadas en PLLRs no son tan notables como los obtenidos con características acústicas, pero proporcionan información complementaria que puede ser utilizada para mejorar el rendimiento general de los sistemas de reconocimiento de locutor.    Los PLLRs se calculan para cada fonema i y para cada ventana en el instante de tiempo t, según la siguiente expresión: Spoken language verification and speaker verification are pattern recognition tasks that consist of recognizing, by computational means, the language spoken and the speaker speaking in an utterance, respectively.</p><formula xml:id="formula_0">P LLR(i|t) = log p(i|t) 1 (N −1) (1 − p(i|t)) i = 1, ..., N<label>(1)</label></formula><p>It should be noted that recognition, identification and verification are different tasks. In identification, it is assumed that the utterance corresponds to one of a certain set of N models, and the task consists on selecting one of the N models as the true identity. A verification system, instead, must decide whether a certain utterance contains speech of a target model or not. That is, each utterance is tested against each model independently. Both tasks are recognition tasks. Even if this work focuses on verification tasks, the term recognition will be used in the manuscript, as its use is a common practice in the field.</p><p>A distinction should be made also between closed and open verification tasks. In the case of a closed task, all the target models considered in the set are known, whereas in open set tasks, "none of the target models", that is, a model considering unknown languages or unknown speakers is also considered in the set of models.</p><p>The main complexity of language and speaker recognition tasks comes from dealing with undesired variabilities present in the utterances due to several factors: the recording device, the transmission channel, environmental noises, mood or physical conditions, differences among speakers in the case of language verification, different languages or aging effects in the case of speaker verification, etc. All these, commonly summarized as channel variabilities, pose a strong difficulty for all the tasks related to speech recognition, like Spoken Language Recognition (SLR) and Speaker Recognition (SR). Extracting informative features robust against those variabilities or designing modeling techniques capable of pattern after the desired variabilities, while discarding the noisy ones, are the main focus of research in both tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.1.1">Structure of Recognition Systems</head><p>Recognition systems have a common basic structure, that can be outlined as follows.</p><p>First of all, it is necessary to record and organize a set of speech signals (or to reuse an existing one) on top of which the system is going to be built. Data should be divided into several sets, one for each of the main stages of SR/SLR system building (see <ref type="figure">Figure 1</ref>.1): the training of the different components present in the recognition system, the development and tuning of the parameters of the different modules, and the evaluation, where system performance is measured on a test benchmark (an independent set of speech signals). Therefore, in general terms, training, development and evaluation data are necessary. The assembling of the sets is a crucial step. Unbalanced sets could pose several problems, like the over-training of the system for certain types of variabilities (under-training for others) causing biased performances and non-robust systems. In optimal circumstances, all the subsets would consist of balanced data representative of the overall variability present in the application scenario. Once the data is available, verification systems can be built.</p><p>The structure of the SLR/SR systems comprises four main modules (see <ref type="figure">Figure 1</ref>.1):</p><p>• Feature/token extractor. The first module of verification systems takes as input the utterance (speech) and aims to concentrate in few and, as far as possible, independent (that is, uncorrelated) parameters/features the information relevant to the classification task.</p><p>• Classifier. Built during the training stage, takes the features as input and scores feature/token sequences with regard to the target models.</p><p>• Backend transformation. This module is assembled on the development stage. It performs a transformation on the raw scores to normalize/calibrate them, that allows us to use a single threshold for all the targets and makes the system work at the desired application point.</p><p>•  In real systems, the distinction between the different modules is not always easy to define, e.g. some classifiers applied on top of features can be considered as higher level feature extractors, which are then used to fed a final classifier. Also, some classifiers provide normalized scores, suppressing the need of an extra backend parameter estimation stage.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.1.2">Evolution</head><p>All the technologies in the area of language and speaker verification have evolved significantly in the last years. These progresses are due to different factors, that are briefly described in the following paragraphs.</p><p>On the one hand, the great effort in database creation/enlargement has promoted a richer benchmark for research, studies and system development. In this regard, it is remarkable the contribution of the National Institute of Standards Technology (NIST) by organizing a series of Language and Speaker Recognition Evaluations (LRE and SRE, respectively). These evaluations, starting in 1996 in the case of LRE and in 1997 for SRE, and held on a regular basis, have been -and still areoutstanding benchmarks for the research community. The contribution of NIST is not limited to providing the databases, as the evaluations also evolve, posing challenging tasks that encourage groups from the research community to continuously improve their systems. The LRE datasets consist of spontaneous conversations collected through telephone (narrow-band) channels involving two speakers. These benchmarks have consistently grown from evaluation to evaluation in terms of amount of data, and have evolved regarding the target languages, ranging from nine up to twenty four. Evaluation tracks involved signals of 3, 10 and 30 second nominal durations. Regarding SRE, NIST databases started providing telephone conversational excerpts recorded over telephone channels, and increasingly added more kinds of recordings, including (in the last SREs) telephone data recorded over microphone channels and conversational speech data from interviews recorded over far-field microphones. The amount of trials has also been constantly increasing on each new release, reaching volumes that comprise recordings of thousands of speakers and millions of trials. As for language recognition, test segments are also divided into three subsets, according to their nominal duration.</p><p>Other resources have also promoted language and speaker recognition research. On the other hand, the increase in computational power has enabled the usage of more complex algorithms for feature extraction and system modeling e.g. the use of supervectors as features or the training of variability matrices in high dimensional spaces. Also, progress has been made from using a single phone decoder to obtain 1-best decoding based approaches, to using lattices in n-grams of different orders extracted from multiple phone recognizers in parallel, to be used as features in further complex modeling techniques.</p><p>Nowadays, Deep Neural Networks (DNNs) are gaining strength in all the stages of SLR and SR systems, from feature extraction to variability compensation, modeling, calibration or even as multiple stages at once. The high computational power attained by current number crunching technology and the big amount of available resources seem to have reached the level needed to train complex and powerful DNNs, which makes them likely to be the path to follow in future research.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.1.3">Applications</head><p>Progress in the field has allowed the development of a wide range of systems and applications for both spoken language and speaker recognition, which are becoming more and more common in the technology surrounding us. The possible applications include, among others:</p><p>• Phone service automation </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.2">Motivation and Objectives of the Work</head><p>This thesis started like any other journey, with a destination and motivation to get there, a vision of the beginning of the path that should be taken, but only a slight idea of the forthcoming route and the things that would be found along the way. That destination, our former objective, was to build a competitive language recognition system.</p><p>The first thing to do before traveling, before even being able to write a plan, is to document about the things you can do, and the experiences others have had while going there. After adequate documentation, the "must sees" clearly showed up. I first toke part in developing a state-of-the-art language recognition system using NIST LRE benchmarks <ref type="bibr" target="#b44">[44]</ref>. This first work, that took me into the speech technologies world, was framed in the Final Year Project and consisted on building a GMM-MAP language verification system, using NIST 2007 LRE as benchmark.</p><p>The closeness of SLR and SR tasks pushed me into the speaker recognition field, leading also to the construction of speaker recognition systems using NIST 2008 SRE <ref type="bibr" target="#b45">[45]</ref>, developed on the Master Thesis, which gave me some knowledge of the sometimes subtle, other times significant differences between both tasks.</p><p>In the meantime, speaker diarization showed up. The application, complexity, combination of several steps to build a system and the bunch of techniques caught our attention. We spent almost a year struggling with tons of papers, software and different ideas, that led to the construction of a dot-scoring based system, that was presented to the Albayzin 2010 speaker diarization evaluation <ref type="bibr" target="#b47">[47,</ref><ref type="bibr" target="#b48">48]</ref>. Even though I remember that time as a truly exciting period, you cannot spread yourself too thin, so the field was finally left aside to focus on language recognition again.</p><p>Albayzin evaluations provided an interesting benchmark to follow the research in the language recognition field <ref type="bibr" target="#b128">[128,</ref><ref type="bibr" target="#b133">[133]</ref><ref type="bibr" target="#b134">[134]</ref><ref type="bibr" target="#b135">[135]</ref><ref type="bibr" target="#b152">152]</ref>. Related to Albayzin evaluations, we also shared the experience of building databases. KALAKA2 and KALAKA3 <ref type="bibr" target="#b129">[129,</ref><ref type="bibr" target="#b130">130]</ref> were created for Albayzin 2010 and 2012 language recognition evaluations, respectively <ref type="bibr" target="#b123">[123,</ref><ref type="bibr" target="#b127">127]</ref>. I could describe the database building episode as some place every researcher in this field should visit once, just to realize that there is no need (nor intention) to go back again.</p><p>The NIST 2011 LRE posed new challenges, focusing on new tasks like pairwise language recognition, and promoted further research on the task, that led us to individual system development, and collaborations with other research groups <ref type="bibr" target="#b109">[109,</ref><ref type="bibr" target="#b113">113,</ref><ref type="bibr" target="#b114">114,</ref><ref type="bibr" target="#b131">131,</ref><ref type="bibr" target="#b132">132]</ref>.</p><p>The usual switching of NIST evaluations between language and speaker, kept us working on the SR field, aiming to improve our systems, adapting them to the latest techniques <ref type="bibr" target="#b46">[46,</ref><ref type="bibr" target="#b106">[106]</ref><ref type="bibr" target="#b107">[107]</ref><ref type="bibr" target="#b108">[108]</ref>.</p><p>Finally "The Idea" arrived: Most SLR approaches are based on either phonotactic or acoustic features. The former aims to get information from the phoneme combinations of each language, that is, tries to model the possible phone sequences allowed or present in each language and uses that information to discriminate between utterances. The latter, instead, divides the signal into short segments or frames (by using different analysis windows) and performs a frequency domain analysis, with the aim of modeling the spectral content of the signal.</p><p>SLR systems rely on combinations of these phonotactic and spectral feature based systems, as it is widely accepted that both kinds of features provide complementary information. However, it is not common practice to combine them into a single feature set, mainly because spectral features are computed on a frame-by-frame basis, whereas phonotactic features provide segmental-level information, and thus there is no clear way to mix them. Most authors build separate acoustic and phonotactic systems and fuse them at the score level to get best SLR performance <ref type="bibr" target="#b31">[31,</ref><ref type="bibr" target="#b133">133,</ref><ref type="bibr" target="#b139">139,</ref><ref type="bibr" target="#b148">148]</ref>.</p><p>Our aim was to try to find a way to integrate phonotactic information in a frame-byframe feature base so that the features could be modeled in state-of-the-art spectralfeature based systems.</p><p>Group meetings and brainstormings led to the Phone Log-Likelihood Ratios <ref type="bibr" target="#b50">[50]</ref>. These features are the core of this thesis, as most of the research, experimentation and developments are performed around this set of features.</p><p>Therefore, the main objectives of the work were (or ended up being):</p><p>• Studying state-of-the-art technology for SLR and SR fields.</p><p>• Obtaining a competitive language recognition system, by development and optimization of the different modules it comprises.</p><p>• Study of the integration of spectro-acoustic and phonetic information into a new set of features.</p><p>• Introducing the new features in state-of-the-art SLR systems.</p><p>• Analyzing their performance in the most relevant benchmarks.</p><p>• Development of a competitive Speaker Recognition System.</p><p>• Integration and optimization of the features as a possible way of introducing phonotactic information in a SR system.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.3">Structure of the Manuscript</head><p>Even though the research work with this thesis has covered both, language and speaker recognition fields, and extensive experimentation has been performed for both tasks, this thesis will focus on Spoken Language Recognition (SLR), that is, state-of-the-art technology (feature extraction techniques, modeling approaches, scoring procedures...) and all the studies will be first presented for SLR tasks. Then, the most important differences between SLR and SR tasks, and specific aspects of the latter will be outlined. The rest of this report is organized as follows:</p><p>Chapter 2 introduces state-of-the-art techniques. First, the main structure and modules of a standard recognition system are illustrated. Then, the evolution of the most relevant datasets for SLR is outlined, specifically, NIST, Albayzin and RATS benchmarks are described. Next, different kinds of features are listed, and feature extraction methodologies are analyzed. Then, the main modeling techniques and channel compensation methodologies are reported. Scoring, system backends and fusion procedures are also addressed. Finally the evaluation measures used to compare SLR system performance are defined.</p><p>The PLLR features are formally defined in Chapter 3. First, the origin of the features and the computation process is presented, as first done in <ref type="bibr" target="#b50">[50]</ref>. An extensive study is then carried out, using the NIST 2007 LRE database, where different feature extraction parameters are optimized. Next, the performance of the proposed approach, based on PLLR features and i-vector modeling, is compared to that of several baseline systems: an acoustic approach, using the same modeling approach (i-vector) based on MFCC-SDC features, and a phone-lattice-SVM phonotactic approach, which utilizes as source of information the same phone decoders used to compute the PLLR features. This section presents results for three independent systems, based on three different phone decoders, and the analysis of the statistical significance of the results. After that, performance is studied also in other relevant datasets: NIST 2009 and 2011 LREs and Albayzin 2010 LRE, to corroborate results and explore the behavior of the features in other benchmarks and types of data <ref type="bibr" target="#b52">[52]</ref>.</p><p>The good performance attained by the system using PLLRs suggested that this set of features could be a promising characterization to use for language recognition. Still, the high feature vector dimensionality (compared to that of other frame-by-frame spectral feature vectors), could pose a problem for some modeling approaches. Chapter 4 addresses this issue by presenting a study on the reduction of dimensionality of PLLR features using supervised and unsupervised techniques <ref type="bibr" target="#b51">[51]</ref>, pursuing a lower feature dimensionality, while maintaining system performance. Then, making use of the compact representation of PLLRs, the integration of larger spectro-temporal information (also common in other spectral features) is analyzed <ref type="bibr" target="#b57">[57]</ref>.</p><p>The dimensionality reduction of the features, and the results attained with the compact representations, led to a multidimensional analysis of the feature space. Chapter 5 introduces this study, developed on NIST LRE benchmarks, revealing that PLLR features show a bounded distribution. An approach to project the features into a different space is presented, which enhances the information retrieved by the system <ref type="bibr" target="#b54">[54,</ref><ref type="bibr" target="#b56">56]</ref>. Thanks to a collaboration carried out during an internship done in the Brno University of Technology, the usage of this set of projected features is then extended to the RATS database, revealing the high performance of the features for short signals recorded in noisy environments <ref type="bibr" target="#b116">[116]</ref>.</p><p>The study is extended to speaker recognition in Chapter 6. This chapter is organized with a structure that resembles that followed in previous chapters for SLR. First, the differences between SLR and SR tasks are outlined, and state-of-the-art techniques for SR are described making emphasis on those not shared with SLR. Then, the application and usage of PLLR features on SR tasks is explained in detail. The work presented covers the optimization of PLLR features for SR and analyzes results on NIST 2010 and 2012 SRE datasets <ref type="bibr" target="#b53">[53,</ref><ref type="bibr" target="#b55">55]</ref>. In Appendix B, the outcome of the participation in international challenges is outlined, including brief system descriptions and the results attained in NIST 2010 SRE <ref type="bibr" target="#b107">[107]</ref>, NIST 2011 LRE <ref type="bibr" target="#b113">[113]</ref>, NIST 2011 SRE analysis workshop <ref type="bibr" target="#b46">[46]</ref>, NIST 2012 SRE, <ref type="bibr" target="#b49">[49]</ref>, and MOBIO 2013 <ref type="bibr" target="#b84">[84]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>State of the Art</head><p>This chapter first introduces the main datasets used in the SLR research area. Then the techniques employed in state-of-the-art language recognition systems are covered. The most useful and popular methods are described covering feature extraction, modeling, channel compensation, scoring, backend and fusion estimation. Finally, the main performance metrics used for system evaluation are formally defined.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Datasets</head><p>As noted before, the quantity, quality and variability of the available data for system training is of the utmost importance in the area of speech processing and language/ speaker recognition. There are various resources available for research in the spoken language recognition field. In this section, we provide details of the most relevant benchmarks for SLR.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.1">NIST LRE Benchmarks</head><p>The National Institute of Standards Technology (NIST) LRE datasets <ref type="bibr" target="#b2">[3]</ref>  • In 2007, the dataset was significantly extended, increasing the number of target languages to 14, and included also several dialects, amounting to 26 different categories. Moreover, non-target language evaluation data was also provided <ref type="bibr" target="#b96">[96]</ref> for a new open-set verification track. In this evaluation, the number of participating sites raised from the previous 12 up to <ref type="bibr" target="#b21">21</ref>, revealing an increasing interest on the field. All the data provided for previous evaluations was used for training or development purposes. Additionally, 20 conversations (40 conversation sides) were provided for target languages not present in previous datasets. Evaluation data amounted to 7575 test segments.</p><p>• The 2009 NIST LRE datasets increased the number of target languages up to 23. For this new benchmark, data coming from VOA radio broadcasts was also included, in addition to conversational telephone speech <ref type="bibr" target="#b93">[93]</ref>. NIST 2009 LRE data comprised data from previous evaluations, plus VOA data, which provided non-audited training data, plus 80 30-second audited segments for each target language. Evaluation data amounted to 14059 test segments.</p><p>• The NIST 2011 LRE differed from previous evaluations in that it focused on language-pair verification conditions. That is, in previous NIST LREs, given a speech signal, the system should give a hard decision on whether the target language was spoken in the segment considering also a set of (multiple) nontarget languages. In this new task, instead, the system would solely focus on two target languages for each trial. The NIST 2011 LRE trials considered language pairs from a set of 24 target languages <ref type="bibr" target="#b69">[69]</ref>. Training and development data for NIST 2011 LRE reused data from previous evaluations, plus a set of 100 30-second audited segments per language for languages not present in preceding datasets. The evaluation dataset amounted to 29511 test segments. • KALAKA was designed to provide data to build systems for four target languages. It also provided data from other four out-of-set languages. As for NIST datasets, development and evaluation signals featured fixed durations of 30, 10 and 3 seconds. The dataset amounted to around 9 hours of training data per target language plus around 1800 speech segments for development and another 1800 for evaluation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.2">Albayzin LRE datasets</head><p>• KALAKA-2 <ref type="bibr" target="#b129">[129]</ref>, built for the next Albayzin 2010 LRE <ref type="bibr" target="#b128">[128]</ref>, is an extension of the previous KALAKA dataset, including two new target languages and two different training subsets of clean and noisy speech segments. The whole database amounts to 125 hours of speech, with more than 10 hours of clean speech and more than 2 hours of noisy/overlapped speech (recorded in noisy background environments) per target language. Two sets of around 1600 speech segments each were provided for development and evaluation.</p><p>• For the last Albayzin language recognition evaluation held on 2012 <ref type="bibr" target="#b123">[123]</ref>, a new application domain was selected, moving from TV broadcast speech into any kind of speech found on the Internet. In KALAKA-3 <ref type="bibr" target="#b130">[130]</ref>, data from KALAKA-2 was used for training purposes, amounting to around 108 hours of speech. Two new conditions were included, Plenty with six target languages for which training data was provided and Empty with four target languages for which no training was provided. Development and evaluation data consisted of similar sets of between 100 and 200 audio segments (30-120 second long) per target language (plus segments for the eleven OOS languages) extracted from YouTube videos.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.3">RATS</head><p>As it can be seen on the descriptions of previous datasets, while more competitive technology emerged, datasets also evolved into more challenging scenarios, involving highly confusable languages, or noisy environments in order to pose new challenges. The RATS dataset <ref type="bibr" target="#b99">[99]</ref> was designed to perform LRE in challenging scenarios, focusing on noisy environments with evaluation tracks for speech segments of 120, 30, 10 and 3 seconds. RATS provides data for 5 target and 10 non-target languages, retransmitted through 8 different communication channels. The Linguistic Data Consortium (LDC) provided data for the RATS program, which consisted of selected signals from Callfriend and Fisher collections, previous NIST datasets and new conversational telephone speech. The data provided for training and development purposes included only signals of 120 seconds nominal duration.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Feature Extraction</head><p>In the feature extraction stage, the parameters that could better characterize the target languages are selected and extracted from the speech utterances. Feature selection and extraction is a broad and fruitful field of research in SLR. As a result, there is a wide range of features to parameterize speech.</p><p>Features can be categorized depending on several factors. A possible way of grouping them could be based on the type of information they convey, as shown in <ref type="figure" target="#fig_36">Figure 2</ref>.1 <ref type="bibr" target="#b37">[37,</ref><ref type="bibr" target="#b160">160]</ref>:</p><p>• Phonetic, spectral and acoustic features obtain information of the audio signals in form of speech units, that is, frame by frame based information, extracted by means of short-time (interval) windows. Languages have their own set of speech units, or phonemic inventory, which defines or summarizes the sounds covered by the language. It is, therefore, straightforward to see why the identification of the phonemes contained in an audio signal could help identifying the language spoken in an utterance. Spectral/acoustic features aim to extract information based on the principles of human sound perception. • Prosody features gather information about tone, rhythm, pitch, stress, etc. These characteristics can be useful not only for affective computing (emotion recognition), but also to discriminate between languages. For instance, Isochrony can be a useful discriminative feature, as languages can divide time in equal portions at different levels, so that, some make the duration of every syllable equal, others divide time so that every mora -a unit that weights the duration of segments that conform a syllable-is equal, and others control the time between two stressed syllables. Some languages have also lexical stress rules, like the ones with fixed stress, which emphasize always the syllable that is placed in the same position in every word (first, penultimate...); other languages stress different syllables according to the structure of the word (regular stress). Tone can be not only a emotional clue, but can also be a distinctive feature to differentiate between words in tonal languages.</p><p>• Phonotactic features compute statistics on top of phoneme-level features, making counts of phonemes, or counts of sequences of phones (n-grams), building this way characteristics covering a larger temporal context. The usefulness of these features can be easily guessed, given that even languages which could share a phonetic inventory would have different allowed phoneme combinations to build words.</p><p>• Morphology features refer to the analysis of morphemes and lexemes and the way these are used to build words in languages. Building a system which is able to identify the words contained in an audio signal would make easy the language recognition task, but at a high cost, given that large language dependent dictionaries would be needed for these approaches to be applied.</p><p>• Syntax features study the way phrases are built out of words. Some tasks require the use of syntax-based features, such as large vocabulary continuous speech recognition.</p><p>All these features provide benefits for language recognition, yet, the most popular approaches combine spectral and phonotactic features, given that these features are not only easy to extract compared to prosodic, morphological or syntax-based features, but also attain good performance. Furthermore, commonly referred to as low-level and high-level features, they have shown to carry complementary information <ref type="bibr" target="#b22">[22]</ref>  <ref type="bibr" target="#b133">[133]</ref>.</p><p>Several works have also explored the use of prosody and other features <ref type="bibr" target="#b76">[76]</ref> [103] <ref type="bibr" target="#b97">[97]</ref>. These systems, though not being that competitive by themselves, provide complementary information to the systems based on acoustic or phonotactic features.</p><p>In this section, we will give an overview of the feature extraction stage, starting with signal processing basics, giving some details about the voice activity detection stage and providing details about the extraction processes of the most common low and high level features for spoken language recognition.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.1">Signal processing</head><p>Signal processing is a field of research that studies the techniques used to process, transform and analyze all kind of physical signals (acoustic, electromagnetic, etc.). The first stage of any language, speaker or speech recognition system is the conversion of the input utterance into a sequence of parameters with relevant information for the recognition task, which is based on different speech audio signal processing techniques.</p><p>Speech processing states that some speech properties are short-time varying, or quasi-stationary in short time intervals (of about tens of miliseconds) <ref type="bibr" target="#b11">[13]</ref>. Besides, it is also known that the human auditory system analyses signals in the frequency domain. Many feature extraction methods, therefore, rely on short time analysis to study signals, making use of frames or windows through the signal to extract information. This process is known as signal sampling or windowing. After that, the signal is transferred into the frequency domain by means of the Discrete Fourier Transform (DFT).</p><p>The choice of the window length (typically from 20 to 30 ms. for speech applications) is a compromise between the stationarity assumption and the frequency resolution attained afterwards. The window shift is usually chosen to obtain overlapped windows (with frame rate of around 10 ms.) to avoid the information loss between frames. Some other issues arise as a consequence of using short analysis windows: The DFT assumes that the analyzed signal is periodic in the considered frame. Computing the DFT of signals that are non-periodic in the analysis window causes an effect known as leakage, which distorts the frequency components of the signal. Different types of windows have been studied in the literature with the aim of optimizing the spectral analysis of signals, reducing leakage while maintaining a low variance. Hamming is the window most commonly applied, though many others have been explored: Dirichlet, triangle, Hanning, Blackman, etc. <ref type="bibr" target="#b71">[71,</ref><ref type="bibr" target="#b115">115]</ref>.</p><p>Another important concept in speech processing is the so called Cepstrum, which is the spectrum of the logarithm of the power spectrum of a signal <ref type="bibr" target="#b11">[13]</ref>. The application of the logarithm in the frequency domain provides a smoothed representation of the signal spectrum, making it possible to characterize the articulation (vocal tract configuration) of the produced sound. DFT components are typically averaged in a bank of filters scaled according to a perceptual scale, so that a set of perceptualfrequency energies are obtained. Finally, a Discrete Cosine Transform (DCT) is applied in order to suppress correlations among filter energies. Cepstrum is broadly used to characterize speech, as it provides a way to separate in the frequency domain the components of the speech signal corresponding to (voiced or unvoiced) excitation and vocal tract filter response (which is the relevant information).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.2">Voice Activity Detection</head><p>Voice Activity Detection (VAD) is an important and challenging process at the feature extraction stage, that deals with the problem of discriminating between speech and non-speech (silent or noisy) regions of the audio signals. An ideal VAD should be robust for a wide range of Signal to Noise Ratio (SNR) conditions and for different types of noises: white, stationary, impulses, etc.</p><p>VAD can be performed in different ways. The simplest techniques are based on the energy content (at the frame level) of speech signals. More sophisticated VAD techniques use other frame-level characteristics, like zero crossing rate, cepstral features, formant shape, or even phonetic features as in <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b77">77]</ref>, where a phonetic recognizer was used to identify silence or noisy frames in order to discard them. Speech and noise statistical models <ref type="bibr" target="#b35">[35,</ref><ref type="bibr" target="#b140">140]</ref>, or long-term signal analysis methods <ref type="bibr" target="#b67">[67]</ref> have also been widely explored.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.3">Spectral and Acoustic low-level Features</head><p>Low-level features model languages with information taken from quasi-stationary spectral characteristics of the audio signal, and their evolution over time. These features are easy to extract and provide a good characterization of the language, making them the most used features in the literature and in applications.</p><p>There is a considerable amount of spectral and acoustic features that have been used for speech and/or speaker recognition, but that have not been widely explored for language recognition tasks: gammatone frequency cepstral coefficients, linear frequency cepstral coefficients, frequency domain linear prediction, etc. In <ref type="bibr" target="#b158">[158]</ref> some of these features are collected and tested in a SLR system, and some further innovative features or variations of the previous are also presented. In this Section, we will describe the most used features in SLR literature.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Mel Frequency Cepstral Coefficients</head><p>Among acoustic features, the Mel-Frequency Cepstral Coefficients (MFCC) are one of the most common representations for language, speaker and speech recognition <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b139">139]</ref>. The MFCC feature computation was first proposed in <ref type="bibr" target="#b38">[38]</ref>. This feature extraction procedure averages the spectral energies in frequency bands defined according to the human perception of differences in frequency, the so called Mel scale. The Mel scale defines an approximately linear scale for frequencies below 1000 Hz, and a logarithmic scale for frequencies above it. This way, a higher resolution is provided for low frequencies.</p><p>The full extraction process can be described as follows: First, audio signals are sampled using typically 20-30 ms Hamming windows at a 10 ms rate. Next, a DFT (in fact, a Fast Fourier Transform, FFT) is applied to get the representation in the frequency domain. Right after, Mel filtering <ref type="bibr" target="#b78">[78]</ref> is applied to get the spectral energies around 20 Mel-scaled frequency bands and logarithms of the output of the filters are computed. Finally a DCT is applied, which provides the representation known as MFCC.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Dynamic coefficients</head><p>Several works have shown empirically that the use of information relative to the evolution of acoustic features is effective in language recognition. Therefore, dynamic coefficients are usually computed on top of MFCC features.</p><p>First-order dynamic coefficients are computed as defined in <ref type="bibr" target="#b156">[156]</ref>:</p><formula xml:id="formula_1">∆f (t) = D d=1 d [f (t + d) − f (t − d)] 2 D d=1 d 2 (2.1)</formula><p>where f (t) is a feature at time t, and 2D + 1 is the size of the regression window. Second-order dynamic coefficients (∆∆) are computed using Eq. 2.1 on first-order dynamic coefficients.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Shifted Delta Cepstrum</head><p>SDCs provide another way of making use of larger temporal context information in the features. Unlike MFCC+∆+∆∆ feature vectors, which carry information of static, first and second order dynamic characteristics, SDCs carry only static and first order dynamic information, computed over a certain number of surrounding windows, centered on the analysis window. That is, SDCs characterize the language by the evolution of local variations of the spectrum around the analysis window. Shifted Delta Cepstrum (SDC) computed on top of MFCCs, is also state-of-the-art in language recognition <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b139">139,</ref><ref type="bibr" target="#b149">149]</ref>. SDCs are specified by four parameters N-d-Pk <ref type="bibr" target="#b149">[149]</ref>: N is the number of coefficients from which derivatives are computed at each frame, d determines the size of analysis windows (consisting of 2 · d + 1 frames) to compute the derivatives, P is the shift (number of frames) between two consecutive analysis windows and k is the number of analysis windows whose delta coefficients are concatenated to form the final feature vector. <ref type="figure" target="#fig_36">Figure 2</ref>.2 shows a diagram of the parameters for SDC feature computation. After the computation of dynamic coefficients, frames marked by the VAD module as silence or non-speech are removed.</p><formula xml:id="formula_2">t-P t t+P t+P*k/2 t-P*k/2 t-d t+d t+P-d t+P+d t-P-d t-P+d t+P*k/2-d t+P*k/2+d t-P*k/2-d t-P*k/2+d</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Perceptual Linear Prediction Features</head><p>Another common characterization (specially in speech recognition tasks) is the one consisting of Perceptual Linear Prediction features (PLP) <ref type="bibr" target="#b73">[73]</ref>, which are based on several concepts related to the critical band masking property of the human auditory model in order to estimate the signal spectrum. Given an audio signal, the PLP estimation algorithm first computes the power spectrum, then warps the frequency axis using the Bark scale. Next, it performs a convolution with a criticalband masking curve and down-samples the signal, preemphasizes the result and applies an intensity-loudness warping. Finally, auto-correlation method of all-pole spectral modeling is applied, which gives autoregressive coefficients, that can be further transformed to obtain parameters like cepstral coefficients.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.4">Phonetic and Phonotactic high-level Features</head><p>Unlike spectral features, high level features are more robust against channel and noise variabilities. However, they are not easy to compute, and large amounts of data are necessary to estimate them. Among high level features, the most common representations are based on the information provided by phone decoders <ref type="bibr" target="#b66">[66]</ref> [26]</p><p>[146] <ref type="bibr" target="#b112">[112]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Phone decoders</head><p>Phone decoders need a large amount of labeled data (determining the phonemes that are present in the speech) to train the models for each of the phones of their phonetic inventory <ref type="bibr" target="#b26">[26]</ref>. Once they are trained, phone decoders are able to take an input sequence of acoustic observations X, and provide an acoustic posterior probability of each state s (1 ≤ s ≤ S) of each phone model i (1 ≤ i ≤ N ) at each frame t, p(i|s, t). This phone-state posterior probabilities are later used to obtain 1-best phone decodings, or lattices (see below).</p><p>Phone decoders can rely on different features, like the MFCCs or PLPs introduced above, usually augmented with Dynamic coefficients or other techniques to obtain information for larger temporal contexts <ref type="bibr" target="#b66">[66,</ref><ref type="bibr" target="#b137">137]</ref>. Once the features are obtained, the input frames are scored with regard to the phonetic units of the phonetic inventory. This can be done using different modelings: GMMs, HMMs, Neural Networks or hybrid models are commonly used in this step.</p><p>Phonemes or speech units, usually span several frames. The scoring at the beginning and end of each speech unit is usually worse than the one attained in the middle frames. With the aim of improving the acoustic modeling of the phonemes, phone decoders are usually trained to provide phone-state posteriors, which represent shorter units and allow a better acoustic modeling of the speech.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>BUT TRAPs/NN phone decoders</head><p>Some of the phone decoders more used in the literature are the open software Temporal Patterns Neural Network (TRAPs/NN) phone decoders, developed by the Brno University of Technology (BUT) <ref type="bibr" target="#b137">[137]</ref>. These phone decoders rely on multilayer perceptron NN, and have the following structure:</p><p>First audio signals are sampled using 25ms Hamming windows at a 10 ms rate. Mel filter bank energies are then estimated, and TRAP feature vectors are computed for each critical band, which module the evolution of the energy values. Mean and variance normalization is applied to the vector, which is then Hamming-windowed, and normalized again with regard to the training set. Each of these vectors is the input to a NN classifier, which estimates phone posterior probabilities for each phonestate model and critical band. The outputs of the classifiers feed another NN, known as merger, which combines the outputs into a single posterior probability vector per frame.</p><p>The BUT TRaPs/NN phone decoders were developed for Czech (CZ), Hungarian (HU) and Russian (RU), feature 45, 61 and 52 phonetic units respectively, and provide three posterior probabilities per phone unit and frame, that is, they are implicitly using three-state phonetic models.</p><p>The main training features of these decoders are:</p><p>• Czech Decoder (CZ) -8 kHz, trained on the Czech SpeechDat(E) Database, containing 12 hours of speech from 1052 Czech speakers (526 males, 526 females), recorded over the Czech fixed telephone network.</p><p>• Hungarian Decoder (HU) -8 kHz, trained on the Hungarian SpeechDat(E) Database, containing 10 hours of speech from 1000 Hungarian speakers (511 males, 489 females), recorded over the Hungarian fixed telephone network.</p><p>• Russian Decoder (RU) -8 kHz, trained on the Russian SpeechDat(E) Database, containing 18 hours of speech from 2500 Russian speakers (1242 males, 1258 females), recorded over the Russian fixed telephone network.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>1-best Decoding</head><p>In the early times of phone decoder based approaches, given an utterance, a phone decoder (trained on one or more languages) would provide the most likely phone sequence according to its phonetic inventory, that is, the most likely sequence of phones.</p><p>These sequences are then normally used to build n-grams, that is, sequences of N phones and counts of n-grams are used as features:</p><formula xml:id="formula_3">count(ŵ i , w i |Θ * ) (2.2) whereŵ i = w i−(n−1) , .</formula><p>.., w i−1 , and w i is the phone at position i of the optimal sequence of phones Θ * .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Phone Lattices</head><p>In more recent phone decoder based approaches, phone lattices were introduced. Lattices model speech utterances as graphs, where nodes represent points in time and each arc corresponds the a phone hypothesis and its corresponding acoustic score. With lattices, instead of just taking the most likely phone sequence, a summation is performed over the different phone sequences <ref type="bibr" target="#b66">[66]</ref>. <ref type="figure" target="#fig_36">Figure 2</ref>.3 shows the representation of a phone lattice and the 1-best decoding output for a single utterance.  It has been experimentally shown that lattices provide a better way of estimating the n-gram probabilities than the 1-best decoding, that is, the information retrieved by summing the likelihoods over all the paths in the phone lattice is more robust than that obtained from the best path.</p><p>Aiming to maximize the information carried out by the features, works in the literature have widely explored different n-gram orders. The increment in the n-gram order enhances system performance, as the extracted higher order n-grams carry more language-specific information, but entails a higher computational cost (as the number of features grows exponentially with n) which may make the modeling of the features intractable. Techniques dealing with this fact normally use n-gram selection methods, discarding low-frequency n-grams (based on counts made on training sets) <ref type="bibr" target="#b121">[121]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Modeling</head><p>Modeling techniques can be classified with regard to different criteria. Based on the training methodology, models can be classified as either generative or discriminative. The former estimates intra-class variability whereas the latter searches for the boundaries between classes. Besides, models can be also seen as parametric or non-parametric. In parametric models, each class is assumed to be modeled by a specific probability density function and model parameters are estimated so as to best fit the probability distribution. Non-parametric models, instead, compare feature vectors directly and the degree of similarity between them is used as a metric to decide whether they belong to the same class or not.</p><p>In this section, we will present the modeling techniques for language recognition that stand out as more popular in the literature.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>GMM-UBM</head><p>In low-level acoustic systems, the target language is modeled with information taken from the spectral characteristics of the audio signal. In the early times of acoustic-based approaches for language recognition, though these systems provided advantages in terms of computational complexity, performance was not as good as that attained by phonotactic systems <ref type="bibr" target="#b159">[159]</ref>. The approach known as Gaussian Mixture Model / Universal Background Model (GMM-UBM) previously used in speaker recognition systems <ref type="bibr" target="#b120">[120]</ref>, was first introduced for language recognition in <ref type="bibr" target="#b155">[155]</ref> using Mel-Frequency Cepstral Coefficients as features.</p><p>GMM are widely used in speech, speaker and language verification systems. A GMM is a generative model that represents the acoustic characteristics of the training set by means of a probability density function defined as a weighted sum of Gaussians, that are defined in the vector space of the acoustic parameters. A GMM consists of a mixture of K D-dimensional Gaussians. A vector w defines the weight of each Gaussian component in the mixture, and each Gaussian is modeled by a vector of means µ µ µ and a D × D covariance matrix Σ. The probability of the input feature vector f , for a GMM model M, is given by:</p><formula xml:id="formula_4">p (f |M) = K k=1 w k 1 (2π) D/2 |Σ k | exp − 1 2 (f − µ k ) T Σ −1 k (f − µ k ) (2.3)</formula><p>There are different approaches and algorithms to estimate the parameters of these statistical models. Among them, maximum likelihood based Expectation Maximization (EM) <ref type="bibr" target="#b42">[42]</ref> has been widely used in the literature.</p><p>The EM algorithm aims to maximize the likelihood of the observed data set with regard to the different parameters (means, covariances and weights of the components) of the model M. For that purpose, the parameters are first initialized (either randomly, or using the K-means algorithm or another reasonable approach). In the E step, the algorithm estimates the posterior probabilities of the responsibilities given the parameters, that is, it estimates the responsibility that each component k takes for generating the data. Then, in the maximization step, the model parameters are reestimated with regard to those responsibilities. Finally the log-likelihood is evaluated to check if it has reached the convergence criterion; if not, the E-M steps are repeated until convergence or a certain number of iterations are reached.</p><p>In the GMM-UBM approach, a universal GMM is trained with data involving a relatively high number of spoken languages (which may or may not include target languages), to get a model that covers all the variability that we may expect in the input utterances of the SLR application. Models of target languages are then trained either by EM, or by Bayesian adaptation, Maximum A Posteriori (MAP) <ref type="bibr" target="#b65">[65,</ref><ref type="bibr" target="#b120">120,</ref><ref type="bibr" target="#b155">155]</ref>, which presents the advantage of requiring less training data for each target model.</p><p>MAP consists of adapting an universal model to the selected data -a specific target language in SLR-. The algorithm combines the parameters of the UBM with the ones estimated from the new data by means of a relevance factor. The factor determines the degree in which the parameters will be transformed into the new ones: a big learning factor would require a higher exposure to parameters in the new data, for the component of the UBM to be adapted. MAP adaptation can retrain means, weights and covariances <ref type="bibr" target="#b120">[120]</ref>. In most approaches only the UBM mean vectors are adapted for each target model. First, training samples are used to compute mean and weight expectations:</p><formula xml:id="formula_5">n k = T t=1 P (k|f t , λ) (2.4) E k (f ) = 1 n k T t=1 P (k|f t , λ)f t (2.5)</formula><p>where f t is the input feature vector, λ are the parameters of the GMM, and P denotes the posterior probability (this estimation is the same in the E step on the EM algorithm). Mean adaptation is performed following:</p><formula xml:id="formula_6">µ k = α k E k (f ) + (1 − α k )µ k (2.6)</formula><p>where α k is the adaptation coefficient, defined as:</p><formula xml:id="formula_7">α k = n k n k + r (2.7)</formula><p>where r is the relevance factor.</p><p>The concatenation of MFCC-SDC features in <ref type="bibr" target="#b149">[149]</ref> under the GMM-UBM modeling approach proved to be a successful way of making use of acoustic information for SLR.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Support Vector Machines</head><p>In the mid 2000 years, Support Vector Machine (SVM) modeling was introduced for acoustic SLR <ref type="bibr" target="#b30">[30]</ref> using MFCC-SDC as features.</p><p>Support Vector Machines, unlike GMMs, are discriminative models that try to find the boundaries between two classes. SVMs search for the separating hyperplane h(x) defined as:</p><formula xml:id="formula_8">h(x) = N i=1 α i c i K(x, x i ) + d (2.8)</formula><p>where K(·, ·) is a kernel function; c i = ±1 corresponds to the class label; α i are weights so that N i=1 α i c i = 0 and α i &gt; 0; x i are the support vectors obtained from the training data and d is a learning factor. SVMs must consider that classes might not be linearly separable. For that reason, the input data is mapped into a high dimensional space by b(x). The Kernel must satisfy Mercer's condition, which can be expressed as K(x, y) = b(x) t b(y). The value of h(x) above or below a given threshold will determine the class that corresponds to the input data. The SVM searches for the hyperplane that maximizes the margin between classes. Geometrically, the margin is the smallest distance between the training data of each class and the hyperplane (see <ref type="figure" target="#fig_36">Figure 2</ref>.4). Therefore, the metric used in each case will condition the solution found for the hyperplane.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>GMM-SVM</head><p>In <ref type="bibr" target="#b27">[27]</ref>, GMM supervectors were introduced for a SVM-based speaker recognition system, and <ref type="bibr" target="#b33">[33]</ref> extended this work for spoken language recognition. GMM supervectors are built by concatenating Baum-Welch statistics which map the utterances into a high dimensional space.</p><p>Given a GMM G ≡ {ω k , µ µ µ k , Σ Σ Σ k |k = 1..K} consisting of K Gaussians in a D-dimensional space, with diagonal covariance matrices Σ Σ Σ k , the zero order statistics are computed as:</p><formula xml:id="formula_9">n k = k γ k (t) (2.9)</formula><p>where γ k (t) = P (k|f t , G) is the posterior probability of the component k of the GMM, given the parameters and the feature vector f t at time t.</p><p>The first order statistics are defined as follows: </p><formula xml:id="formula_10">x k = k γ k (t)Σ − 1 2 k (f t − µ k ) (2.</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Phonotactic approaches</head><p>Assuming 1-best decoding and given L models for L target languages and the mostlikely sequence of phones Θ * provided by a phone decoder, this approach aims to find the target language which maximizes <ref type="bibr" target="#b29">[29]</ref>:</p><formula xml:id="formula_11">L * = argmax L log p(Θ * |L) (2.11) = argmax L 1 N N i=1 log p(w i |w i−(n−1) , ..., w i−1 , L) (2.12)</formula><p>where w 1 , ..., w N are the N phones contained in Θ * and n is the n-gram order. The joint probability of the sequence of phones can be also expressed in terms of n-gram counts as follows:</p><formula xml:id="formula_12">p(ŵ i , w i |Θ * ) = count(ŵ i , w i |Θ * ) J j=1 count(ŵ j , w j |Θ * ) (2.13)</formula><p>where J are all the unique n-grams in the utterance. Therefore, using this notation, under the 1-best decoding, this approach aims to find the language that maximizes:</p><formula xml:id="formula_13">L * = argmax L s L (Θ * ) (2.14) s L (Θ * ) = i p(ŵ i , w i |Θ * ) log p(w i |ŵ i , L) (2.15)</formula><p>In the case of using phone lattices, the summation involves many different uniquê ww n-grams for which the expected likelihood is computed as an average over all possible paths in the lattice <ref type="bibr" target="#b29">[29]</ref>: </p><formula xml:id="formula_14">E W [s L (Θ)] = ŵw E W [p(ŵ, w|Θ)] log p(w|ŵ, L) (2.</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Phone-lattice-SVM</head><p>SVMs have also been used under the approach known as Phone-lattice-SVM, combining phonotactic features with SVM classifiers <ref type="bibr" target="#b26">[26]</ref>.</p><p>The phone lattice produced by a decoder i is stored for each target language l j , then feature vectors are built from expected counts of phone n-grams. A SVM model ψ(i, l j ) is estimated on the outputs of the phone decoder i for the training dataset, taking j as the target language. Variations of this approach have been recently proposed leading to improved performance <ref type="bibr" target="#b112">[112,</ref><ref type="bibr" target="#b146">146]</ref>. At the feature level, several methods have dealt with dimensionality reduction issues to try to decrease the feature n-gram order using different criteria depending on SVM-modeling parameters, feature discrimination merits, projections, or dynamic selections <ref type="bibr" target="#b110">[110,</ref><ref type="bibr" target="#b121">121,</ref><ref type="bibr" target="#b147">147</ref>].</p><formula xml:id="formula_15">Phone Recognizer P j Language Model M 1 Language Model M i Language Model M n … … B A C K E N D Phone Recognizer P m Language Model M 1 Language Model M i Language Model M n … … B A C K E N D Phone Recognizer P 1 Language Model M 1 Language Model M i Language Model M n … … B A C K E N D … … F U S I O N</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4">Channel Compensation</head><p>The term channel compensation comprises all the techniques that try to suppress or minimize the undesired variabilities contained in speech signals. In the case of spoken language recognition tasks, these variabilities are caused (among others) by environmental noises, variabilities introduced by different recording devices, transmission channels or speakers, and are usually summarized as channel variabilities.</p><p>Channel variabilities pose nowadays one of the main difficulties for system performance. Eliminating the undesired variabilities while keeping the desired features for language modeling is a complicated task, which is addressed at different stages of the recognition system. This section describes the main state-of-the-art techniques applied to solve channel variability issues.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Feature-level channel compensation</head><p>The main disadvantage of spectral features is their sensitivity to noise and channel variabilities. Several compensation techniques have been applied, to minimize these undesired effects.</p><p>Cepstral Mean Subtraction (CMS) deals with slow varying distortions, like stationary noises and the effects caused by the use of stationary filters, like the type of microphone, the distance to the microphone and the acoustics of the area where the recording is made. The technique consists on computing the mean of each cepstral coefficient on the signal file and subtracting it from the corresponding coefficient <ref type="bibr" target="#b89">[89,</ref><ref type="bibr" target="#b136">136]</ref>. Another variability compensation technique applied on the feature extraction stage (more common in speaker recognition) is RASTA filtering <ref type="bibr" target="#b74">[74]</ref>. The technique assumes that channel characteristics have little frequency variations over time, so their spectral elements are in the low-frequency area. RASTA applies a bandpass filtering to each frequency channel to get rid of those variabilities. Another commonly applied technique, called Feature Warping <ref type="bibr" target="#b104">[104]</ref>, maps the parameter probability density function to a normal distribution, which makes the features more robust to linear channel effects.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Nuisance Attribute Projection</head><p>A family of compensation techniques is based on the idea that channel mismatch, environmental noise and other undesired variabilities are contained in a lower dimensional subspace. Methods based on this hypothesis use labeled data (with either language or condition dependent labels) to estimate the different distortions or channel variabilities, and then project features or compensate the models in order to remove those undesired effects.</p><p>Nuisance Attribute Projection (NAP) <ref type="bibr" target="#b141">[141]</ref> was one of the first methods proposed based on this idea. NAP estimates the channel factors on the SVM or supervector spaces, and projects them out leading to features that are more resistant to channel effects <ref type="bibr" target="#b122">[122]</ref>.</p><p>NAP requires data from several sessions for each language. Given the language l and a data matrix with feature vectors f recorded over different conditions, NAP tries to minimize the following function:</p><formula xml:id="formula_16">F (P) = N −1 i=1 N j=i+1 W ij ||P(b(f i ) − b(f j ))|| 2 (2.17)</formula><p>where N is the number of input signals, W is a matrix where W ij = 1 if f i and f j belong to the same language, and 0 when feature vectors belong to different languages, and P is the projection matrix that is assumed to follow the form:</p><formula xml:id="formula_17">P = I − XX t (2.18)</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Eigenchannel Compensation</head><p>After NAP, channel compensation techniques at the modeling stage started to gather strength in SLR. Methods previously applied to speaker recognition like eigenchannel adaptation <ref type="bibr" target="#b82">[82]</ref> were afterwards successfully applied to spoken language recognition <ref type="bibr" target="#b32">[32]</ref> [75] [34] <ref type="bibr" target="#b21">[21]</ref>.</p><p>Under this approach, eigenchannels are estimated and used to move the supervectors in the maximum variability direction, with the aim of adapting the models to the test data.</p><p>In order to perform eigenchannel compensation, first, an uncompensated supervector is computed for each utterance:</p><formula xml:id="formula_18">m = (rI + diag(n)) −1 x (2.19)</formula><p>where I is the identity matrix, diag(n) is a matrix of dimension D × K with the supervector n in the diagonal and r is an heuristic relevance factor.</p><p>Language variabilities (not willing to be modeled) are removed from the supervectors by computing the mean of the training signals belonging to the same language. Then the covariance matrix of the training samples is used to compute the most relevant Q eigenvectors by means of Principal Component Analysis (PCA). Then the eigenchannel matrix W is built by concatenating each eigenvector v i multiplied by its corresponding eigenvalue λ i .</p><formula xml:id="formula_19">W = [v 1 · λ 1 v 2 · λ 2 . . . v Q · λ Q ] (2.20)</formula><p>When data is labeled with regard to the type of recording or transmission channel, different eigenchannels can be estimated depending on the channel variability to be modeled. In that case, the final W matrix is formed by the concatenation of the different sub-matrices.</p><p>Once the eigenchannels are computed, these can be used to perform the compensation in the sufficient statistics space, as follows:</p><formula xml:id="formula_20">x = x − diag(n) · WL −1 W t x (2.21)</formula><p>where L is defined as:</p><formula xml:id="formula_21">L = I + W t diag(n)W = I + K k=1 n k · W t k · W k (2.22)</formula><p>Finally, the compensated supervectors are computed by:</p><formula xml:id="formula_22">m = (rI + diag(n)) −1x (2.23)</formula><p>These compensated supervectors can be then either used to feed a SVM, or directly applied for linear scoring or used to feed other classifiers.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Joint Factor Analysis</head><p>The channel compensation approaches presented so far focused on modeling the channel-related part of the speech signal/utterance and removing it from the utterance.</p><p>The Joint Factor Analysis (JFA) approach <ref type="bibr" target="#b80">[80]</ref>, originally introduced for speaker recognition technology, was successfully applied to spoken language recognition too.</p><p>Joint Factor Analysis took a step forward compared with previous approaches, introducing, along with the supervector models, two channel and language dependent factors as follows:</p><formula xml:id="formula_23">M = m ubm + Ux + Vy + Dz (2.24)</formula><p>where U is the channel subspace, V is the language subspace and D is a diagonal matrix covering the residual variability. The vectors x and y are the channel dependent and language dependent factors, respectively, and z is the residual factor; all of them are assumed to be normally distributed.</p><p>Data from various channel conditions and languages is needed to estimate the model parameters. The language factors are forced to be the same for each language, whereas different channel factors are estimated on each utterance. Channel factors are also estimated on the test step, and used to adapt the language model to the test channel condition.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Total Variability Factor Analysis</head><p>Finally, JFA gave rise to Total Variability Factor Analysis. This approach arose as a result of a set of experiments which proved that, based just on the channel components of the JFA approach, it was possible to perform SR with recognition rates much better than random. The discovery suggested that the channel components were therefore carrying speaker-related information, that was lost in the speaker subspace. Under the Total Variability Factor Analysis approach, all the information is stored in a single feature vector, by projecting data into a low-dimensional subspace. In this way, all the relevant information is conveyed by low-dimensional feature vectors known as i-vectors, which can be then processed using further modeling approaches <ref type="bibr" target="#b40">[40]</ref> [41] <ref type="bibr" target="#b98">[98]</ref>.</p><p>Under the total variability modeling approach <ref type="bibr" target="#b40">[40]</ref>, an utterance dependent GMM supervector M (stacking GMM mean vectors) is decomposed as follows:</p><formula xml:id="formula_24">M = m + Tw (2.25)</formula><p>where m is the utterance independent mean supervector, T is the total variability matrix (a low-rank rectangular matrix) and w is the so called i-vector (a normally distributed low-dimensional latent vector). That is, M is assumed to be normally distributed with mean m and covariance TT . The latent vector w can be estimated from its posterior distribution conditioned to the Baum-Welch statistics extracted from the utterance and using a UBM. The i-vector approach maps high-dimensional input data (a GMM supervector) to a low-dimensional feature vector (an i-vector), hypothetically maintaining most of the relevant information.</p><p>Then, i-vectors can be used to feed different classifiers from simple generative modeling approaches (a single Gaussian to model each target language) or logistic regression to more complex approaches, like feeding neural networks, or training a Probabilistic Linear Discriminant Analysis model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.5">Scoring</head><p>In SLR, given a trial, consisting of a test segment S and a target language l T , the system must decide whether or not the target language is the language spoken in the test segment. The decision is taken according to the value of the score attained by the signal against the model of the language s(S, l T ) with regard to a threshold θ. The trial will be accepted only if s(S, l T ) &gt; θ.</p><p>The scores can be computed in different ways. The outputs of some of the presented modeling approaches can be directly treated as scores (NN, PLDA, PPRLM, etc.) Some other modelings need further processing to transform the outputs into reliable scores. For example, supervectors or i-vectors can be tested against each other using cosine distance, or they can feed other classifiers such as the mentioned SVMs, logistic regression or NNs to get scores.</p><p>In most cases, scores still need further processing, either a fusion step to combine the outputs of several systems or a calibration step also known as backend.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.6">Calibration and Fusion</head><p>Calibration of the scores is required to set a single (general) threshold θ on which the accept/reject decisions rely, that is, scaling the scores for the different language models equalizing them for the whole set. Calibration is also applied in order to produce meaningful scores (e.g. posteriors) <ref type="bibr" target="#b22">[22]</ref>. The calibration stage can involve different techniques, like normalization, backend models or the fusion of several systems.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.6.1">Score Normalization</head><p>Score normalization techniques such as Z-norm and T-norm <ref type="bibr" target="#b10">[12]</ref> can help removing the environmental effects on the score space. Nevertheless, they are rarely applied alone in SLR systems. Instead, they are usually applied before some other backend.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Z-Norm</head><p>The Z-norm aims to compensate for deviations related to the target language. Given two language models: an odd model m 1 (for language l 1 ) given the characteristics of its training signals, and a "standard" model m 2 (for language l 2 ), if the system was to evaluate a clean signal S 1 , containing speech in language l 1 against both models, the system could assign a low score to S 1 on its evaluation against l 1 , compared to the score attained against l 2 . To minimize this effect, scores for each language are normalized with regard to a set of development signals containing non-target languages, as follows:</p><formula xml:id="formula_25">Zscore = s l − µ z (m) σ z (m) (2.26)</formula><p>where µ z (m) and σ z (m) are the mean and standard deviation of the scores of model m against the Z-norm set.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>T-Norm</head><p>The T-norm aims to compensate for deviations related to the test signal. Suppose that S 1 contains speech in language l 1 and S 2 speech from some other language. If S 1 contained singularities (noises, laugh, screams, etc.) and S 2 contained normal speech, the score of S 1 against l 1 would probably be low with regard to the one attained by S 2 against l 1 . To get rid of this effect, the T-norm normalizes test-signal scores using a set of non-target models following:</p><formula xml:id="formula_26">T score = s l − µ t (m) σ t (m) (2.27)</formula><p>where µ t (m) and σ t (m) are the mean and standard deviation of the scores of model ml against the T-norm set.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ZT-Norm</head><p>The score combining both Z-norm and T-norm would be computed as:</p><formula xml:id="formula_27">ZT score = s l −µz(m) σz(m) − µ t (y) σ t (y) (2.28)</formula><p>where µ z (m) and σ z (m) are the mean and standard deviation of the scores of model ml against the Z-norm set and µ t (y) and σ t (y) are the mean and standard deviation of the Z-normalized score yl against the T-norm set.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.6.2">Backend models</head><p>The backend serves as a precalibration stage that transforms the space of scores to get reliable estimates of the class probabilities. Besides, when the set of languages for which models have been trained does not match the set of target languages, the backend maps the available scores to the space of target languages.</p><p>In the case of NIST LRE datasets, separate models can be trained for different dialects of a target language or for different data sources (telephone conversational speech, radio broadcast speech, etc.), and non-target languages can be modeled as well.</p><p>Several kinds of backends can be applied <ref type="bibr" target="#b109">[109]</ref>. In this section we will just outline the ones mostly applied in state-of-the-art systems.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Generative Gaussian Backend</head><p>In a generative Gaussian backend, the distribution of language scores is modeled by a multivariate normal distribution N (µ l T , Σ) for each target language l T , where the full covariance matrix Σ is shared across all target languages. Maximum Likelihood (ML) estimates of the means and the covariance matrix are computed.</p><p>Given a score vector s of size K, the output (calibrated) log-likelihood vectorŝ is obtained by:</p><formula xml:id="formula_28">ŝ = As + b + c (2.29)</formula><p>where the rows of A are:</p><formula xml:id="formula_29">a l T = µ l T Σ −1 (2.30)</formula><p>and the elements of b and c are (note that c is a constant vector):</p><formula xml:id="formula_30">b l T = − 1 2 µ l T Σ −1 µ l T (2.31) c l T = − K 2 log (2π) − 1 2 log |Σ| − 1 2 s Σ −1 s (2.32)</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Discriminative Gaussian Backend</head><p>In this case, ML estimates of the means and the common covariance matrix are used initially, but further reestimates of the means are iteratively computed in order to maximize the Maximum Mutual Information (MMI) criterion:</p><formula xml:id="formula_31">F MMI (λ) = ∀s log p λ (s|l T (s)) C ∀l p λ (s|l) C p (l) (2.33)</formula><p>where p λ (s|l (s)) is the likelihood of the score vector s given the true target language l T (s) and model parameters λ, p(l) is the probability of language l and C is a heuristic factor.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Logistic Regression</head><p>Multiclass logistic regression <ref type="bibr" target="#b150">[150]</ref> can be used to transform the scores following:</p><formula xml:id="formula_32">s = Qs + u (2.34)</formula><p>where Q and u parameters are estimated to optimize the multi-class C LLR (see Section 2.7).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.6.3">Fusion</head><p>In the fusion stage, several systems can be combined. As outlined in <ref type="figure" target="#fig_36">Figure 2</ref>.7, fusion classifiers take several calibrated system outputs (one score per language and system) and combine them to give the final set of calibrated and fused scores.</p><p>Fusion classifiers use development data to evaluate the performance of each system and estimate the weight that will be given to each system output (represented as SP in the <ref type="figure" target="#fig_36">Figure 2</ref>.7). In this way, scores are weighted so that the systems performing better attain a higher relevance in the final decision score (CS), and are adjusted to fit the threshold. Binary logistic regression and multiclass logistic regression are techniques normally used for parameter tuning. Focal <ref type="bibr" target="#b61">[61]</ref> is a useful and versatile toolkit widely used in the community for the fusion of several systems.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.7">Evaluation Metrics</head><p>As in any other classification task, SLR performance metrics rely on combinations of different types of errors. As represented in <ref type="figure" target="#fig_36">Figure 2</ref>.8, spoken language verification systems produce two types of errors:   • Misses: Trials for which the correct answer is Accept (target trials) but the system says Reject.</p><p>• False alarms: Trials for which the correct answer is Reject (non-target trials) but the system says Accept.</p><p>The corresponding error rates can be computed as:</p><p>• Miss error rate, P miss : The fraction of target trials that are rejected.</p><p>• False alarm error rate, P fa : The fraction of non-target trials that are accepted.</p><p>Combinations of the error rates define different cost functions, in terms of which verification systems can be evaluated. The decision of the system may vary according to application dependent parameters of the cost functions: the prior probability of each language model (P T ), the cost of a miss error (C miss ) and the cost of a false alarm (C fa ), which define the operating point of the system. A well calibrated system should be able to automatically adapt the decisions to each particular application.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Equal Error Rate (EER)</head><p>This measure reports system performance at the operation point for which the false alarm error rate (P fa ) is equal to the miss error rate (P miss ). The EER does not measure the global performance of a system (i.e. for a wide range of operating points). Furthermore, since the threshold value is chosen a posteriori by the evaluator, it does not take into account the ability of the system to be positioned at the EER operation point (i.e. the performance loss due to bad calibration).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Detection Error Tradeoff (DET) curve</head><p>Detection Error Tradeoff (DET) curves <ref type="bibr" target="#b92">[92]</ref> provide a straightforward way of comparing global performance of different systems for a given test condition and are used in NIST evaluations to support system performance comparisons.</p><p>A DET curve (as the one showed in <ref type="figure" target="#fig_36">Figure 2</ref>.9) is generated by computing P miss and P f a for a wide range of operation points (thresholds), based on the scores yielded by the analyzed system for a given test set. The axes of a DET curve show P f a and P miss in a lineal scale with regard to the normal distribution, given that both kinds of errors are assumed to follow that distribution. The scale is helpful to increment the resolution in low error regions, and makes lines representing systems look (usually) straight, making the comparison among systems easier. In each curve, two operating points are shown: the system operating point (given by system decisions, which depend on the chosen threshold) and the optimal (or minimum cost) operating point, which corresponds to the threshold that minimizes the cost function. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Average Cost (C avg )</head><p>This measure is a combination of P miss and P fa and is computed as follows:</p><formula xml:id="formula_33">C avg = 1 L L i=1      C miss ·P T ·P miss(i) + 1 L−1 L N C fa ·(1 − P T − P oos )·P fa (l T , l N ) +C fa P oos P fa (l T , l O )      (2.35)</formula><p>where L is the number of target languages, l T , l N , and l O stand for target, non-target and out-of-set languages, respectively; and the application-dependent parameters are: P T the target prior, P oos the prior for out of set languages (which takes the value 0 in closed set evaluations), C miss the miss error cost and C fa the false alarm error cost.</p><p>C avg accounts for the calibration loss, but it is still limited to a single operation point.</p><p>Since it was the primary measure in NIST evaluations until 2011, many authors have historically reported system performance in terms of C avg , for this reason, though other measures will be also used to report SLR performance in this thesis, C avg will be primarily used when commenting and comparing results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>NIST 2011 LRE metric, C 24 avg</head><p>In the NIST 2011 LRE, an alternative metric was used to evaluate system performance, based on a pairwise cost function defined by:</p><formula xml:id="formula_34">C(l 1 , l 2 ) = C l1 P l1 P miss (l 1 ) +C l2 (1 − P l1 )P miss (l 2 ) (2.36)</formula><p>where l 1 and l 2 denote languages 1 and 2, respectively.</p><p>The overall C 24 avg measure was defined as the mean of the C(l 1 , l 2 ) values over the 24 language pairs for which the C min values were greatest 1 . In the evaluation, the application parameters were set to the following values: C l1 = C l2 = 1 and P l1 = 0.5. For further details, see <ref type="bibr" target="#b6">[8]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Log-Likelihood Ratio Cost (C LLR )</head><p>When the scores represent (or can be interpreted as) log-likelihoods, systems can be evaluated in terms of the so called C LLR <ref type="bibr" target="#b20">[20]</ref>, which has been used as alternative performance measure in some NIST evaluations. C LLR allows us to evaluate the system performance globally by means of a single numerical value. It only depends on the scores (it does not depend on application dependent parameters), on their ability to discriminate amongst target languages from each other and on how well they are calibrated, the two key features of a SLR system. On the other hand, it has higher statistical significance than EER or C avg , since it is computed from verification scores (in contrast to EER or C avg , which depend only on Accept/Reject decisions). Let us now recall how C LLR is computed.</p><p>Let LR(S, l i ) be the likelihood ratio corresponding to segment S and target language l i . The likelihood ratio can be expressed in terms of the conditional probabilities of X with regard to the alternative target and non-target hypotheses, as follows:</p><formula xml:id="formula_35">LR(S, l i ) = p(S|l i ) p(S|¬l i ) (2.37)</formula><p>Let E be an evaluation dataset, consisting of the union of L disjoint subsets: E lj (j ∈ [1, L]) containing speech segments in the target language l j . Pairwise costs C LLR (l i , l j ), for i, j ∈ [1, L], are defined as follows:</p><formula xml:id="formula_36">C LLR (l i , l j ) =      1 |E l i | S∈E l i log 2 (1 + LR(S, l i ) −1 ) j = i 1 |E l j | S∈E l j log 2 (1 + LR(S, l i )) j = i (2.38)</formula><p>Finally, the average C LLR is computed by adding the pairwise costs for all the combinations of target and non-target languages, as follows:</p><formula xml:id="formula_37">C LLR = 1 L L i=1 {P T · C LLR (l i , l i ) + L j=1 j =i P N · C LLR (l i , l j )} (2.39)</formula><p>where P T is the prior probability of target languages and P N = (1 − P T )/(L − 1) is the prior probability of non-target languages.</p><p>The C LLR takes unbounded non-negative values expressed in information units (bits), with lower values representing better performance, the value 0 corresponding to a perfect system and the value log 2 (L) corresponding to a system which just relies on priors, thus providing no information to decide a trial. C LLR can be computed by means of the FoCal toolkit <ref type="bibr" target="#b61">[61]</ref>. Further details about the reasons for using this measure and its interpretation can be found in <ref type="bibr" target="#b20">[20]</ref>  <ref type="bibr" target="#b22">[22]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Actual Relative Confusion, F act</head><p>In the Albayzin 2012 LRE, a new evaluation metric was proposed to measure system performance: F act . This new metric measures the information provided by a SLR system through a set of log-likelihoods and does not require making hard decisions (see <ref type="bibr" target="#b124">[124]</ref> for details).</p><p>To compute the metric, a prior distribution over language classes is specified, so that Bayes' rule can be used to map the submitted log-likelihoods to language class posteriors. The goodness of these posteriors is then evaluated by means of a logarithmic cost function. A weighted average of the logarithmic cost over all audio segments forms the cross-entropy criterion. In the following paragraphs the crossentropy criterion is presented in the form of relative confusion, a measure closely related to perplexity.</p><p>Logarithmic cost function: for every audio segment, S t , the system under evaluation submits the log-likelihood-vector, t . The evaluator has access to the true class label for segment S t , which we denote l true(t) ∈ {l 1 , . . . , l L }. This allows the evaluator to compute a measure of goodness for t , in the form of the logarithmic cost function:</p><formula xml:id="formula_38">C log (Π t |L true(t) ) = − log P (l true(t) | t , π) (2.40)</formula><p>where Π t = P (l 1 | t , π), . . . , P (l L | t , π) is the whole posterior distribution, estimated according to the prior distribution π defined in <ref type="bibr" target="#b124">[124]</ref> and using the softmax function to map the log-likelihood vector into posterior distributions.</p><p>Multiclass cross-entropy: the evaluation criterion, known as multiclass crossentropy, is formed by a weighted average of the logarithmic cost:</p><formula xml:id="formula_39">C mce = m i=1 π i T i t∈Ti − log P (l i | t , π) (2.41)</formula><p>where T i is the subset of indices for segments of class i. By T i we mean the number of segments of language class i.</p><p>The default system: the one that cannot make up its mind about the language class and outputs it = k t for every t. This gives P (l i | t , π) = π i for every i, t.</p><formula xml:id="formula_40">C def = L i=1 −π i log π i (2.42)</formula><p>which is just the prior entropy. If a submitted system has C mce ≥ C def , then it does not improve upon the default system.</p><p>Confusion: to facilitate interpretation of cross-entropy, we define the confusion of the system under evaluation as:</p><formula xml:id="formula_41">F mce = exp(C mce ) − 1 (2.43)</formula><p>Similarly, the prior confusion (confusion of the default system) is:</p><formula xml:id="formula_42">F def = exp(C def ) − 1 (2.44)</formula><p>The actual relative confusion is defined as:</p><formula xml:id="formula_43">F act = F mce F def (2.45)</formula><p>The relative confusion is the factor by which the system has changed (hopefully reduced) the prior confusion. The reference value for relative confusion is 1. Badly calibrated systems that have relative confusion greater than one are doing worse than the default system. Good systems must have relative confusion below 1. A perfect system would have relative confusion of zero.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Chapter 3</head><p>Phone Log-Likelihood Ratios</p><p>As exposed in Section 1, the main objective when searching for a new set of features was to find a way of conveying phonetic and spectral information into a single set of features.</p><p>In this chapter, the choice of features used in this work is presented and defined.</p><p>Once the reader is familiar with the basic extraction procedure, details about the main approach in which the features are used are given.</p><p>After that, with the aim of optimizing the extraction of the new set of features, a detailed study is carried out using the NIST 2007 LRE dataset. This benchmark provides a good compromise between database size and reliability and generalization of the results, and was therefore selected as the primary benchmark for all the studies in this work.</p><p>Once the optimal configuration of the feature extraction procedure is found, the approach is compared with other acoustic and phonotactic approaches, to test the goodness of the representation. Besides, fusions at the score level are tried, involving all the mentioned approaches, to check the complementarity among different systems. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Definition of Phone Log-Likelihood Ratios (PLLR)</head><p>The computation of the features is based on a phone decoder that is assumed to provide a reasonable coverage of the phonetic content of most languages. Even if phoneme inventories in different languages range from 11 (e.g. for the East Papuan language Rotokas and the Amazonian language Pirahã) to 141 (e.g. for the African language !Xũ), the number of phonemes of most languages is between 30 and 60 phonetic units <ref type="bibr" target="#b37">[37]</ref>. Let us consider one such phone decoder including N phone units, each of them represented typically by means of a model of S states. Given an input sequence of acoustic observations X, we assume that the acoustic posterior probability of each state s (1 ≤ s ≤ S) of each phone model i (1 ≤ i ≤ N ) at each frame t, p(i|s, t), is output by the phone decoder.</p><p>The number of units of the phone decoder, N , multiplied by the number of states that phone decoders provide for each phone unit would give a considerable feature vector size, intractable when combined with several post-processing steps usually applied on top of feature vectors. Therefore, to compute the features, first, the acoustic posterior probability for each phone unit i at each frame t is computed by adding the posteriors of its states:</p><formula xml:id="formula_44">p(i|t) = ∀s p(i|s, t) (3.1)</formula><p>This way, we obtain an N dimensional vector, the one that according to the phone decoder parameters, best describes the spectral content of the analysis window.</p><p>Geometrically, this vector can be seen as a point inside the standard N − 1 simplex. The N − 1 simplex is a subset of R N determined by:</p><formula xml:id="formula_45">∆ (n−1) = {(x 0 , . . . , x n−1 ) ∈ R N | n−1 i=0 x i = 1 ∧ x i ≥ 0 ∀i} (3.2)</formula><p>The vertices of this subset are given by the vectors v i ∈ R N , where: In our case, each vertex corresponds to a pure phonetic unit p i .  According to the phone decoder, the closer the point represented by the feature vector is to each of the vertices, the closer the content of the analysis window is to that phone sound.</p><p>This vector already provides frame-by-frame information that could be used as a feature vector. The question at this point was, is it suitable as a feature vector? Most of the modeling approaches used in systems based on frame level features assume that they are normally distributed. When analyzing frame-level distributions of phone posteriors (see <ref type="figure" target="#fig_17">Figure 3</ref>.2, row 1) they show really sparse behaviors, as it is expected for values representing probabilities. In order to compensate for this undesirable effect, we seek for ways of transforming the features. Among the vast number of possibilities, we selected the following, given the simplicity and result of the transformations: First, as shown in <ref type="figure" target="#fig_17">Figure 3</ref>.2, row 2, the frame-level logposteriors are computed, whose distributions seem to be closer to Gaussian than those of posteriors, but still featuring some singularities. Then, after computing the phone posterior log-likelihood ratios <ref type="figure" target="#fig_17">(Figure 3.2, row 3)</ref>, the distributions attained are seemingly Gaussian.  Assuming a classification task with flat priors, phone log-likelihood ratios are computed from phone posterior probabilities as follows:</p><formula xml:id="formula_46">P LLR(i|t) = log p(i|t) 1 (N −1) (1 − p(i|t)) i = 1, ..., N (3.4)</formula><p>This way we obtain a feature vector carrying the same information as the feature vector output by the phone decoder, but featuring seemingly Gaussian distributions. We denote these features Phone Log-Likelihood Ratios (PLLRs).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Configuration of a SLR System Based on PLLR Features</head><p>In our approaches to compute the PLLRs, we use the open software Temporal Patterns Neural Network (TRAPs/NN) phone decoders, developed by the Brno University of Technology (BUT) <ref type="bibr" target="#b137">[137]</ref>. Phone posterior probabilities are computed by adding the values corresponding to the three states of each phone unit (see eq. 3.1). BUT phone decoders provide posteriors for three non-phonetic units, which correspond to int (intermittent noise), pau (short pause) and spk (non-speech speaker noise). These three non-phonetic units were integrated into a single phone unit, by simply adding the values corresponding to their posteriors. Then, PLLRs were computed according to Equation 3.4, getting 43 (CZ), 59 (HU) and 50 (RU) loglikelihood ratios per frame, respectively, which we call PLLR features.</p><p>In <ref type="bibr" target="#b5">[6]</ref> we provide open software to compute the PLLRs, either from phone posteriors, or from BUT phone decoder outputs.</p><p>Voice activity detection was performed by removing the feature vectors whose highest PLLR value corresponded to the non-phonetic unit.</p><p>The PLLR-based baseline system follows the Total Variability Factor Analysis (i-vector ) approach (see section 2.4). A 1024-mixture gender-independent UBM with diagonal covariance matrix is trained with the maximum likelihood criterion, using binary mixture splitting, orphan mixture discarding and variance flooring.</p><p>A 500 dimensional total variability matrix is estimated as described in <ref type="bibr" target="#b50">[50]</ref>, using data from target languages. To model the spoken language, a generative model is defined in the i-vector feature space (as in <ref type="bibr" target="#b98">[98]</ref>), the set of i-vectors of each language being represented by a single Gaussian.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Search for the Optimal PLLR Feature Configuration</head><p>Several configuration options could be tested on top of PLLRs. This section presents the studies carried out in order to contrast results of the systems trained with PLLR features modified or augmented with different techniques. Experimentation is carried out using the NIST 2007 LRE dataset (the one selected as development set for this work). In this study, among BUT TRAP/NN phone decoders, the one trained for Hungarian was selected for experimentation, given that it had provided good results in several speech and language recognition tasks <ref type="bibr" target="#b43">[43,</ref><ref type="bibr" target="#b112">112,</ref><ref type="bibr" target="#b143">143]</ref>. Results will be provided in terms of both C avg and C LLR , though performance will be mostly compared using C avg , given that this is the primary measure used historically in most NIST LRE evaluations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.1">Phone Log-Likelihoods vs PLLRs</head><p>First, we compare the results attained by systems trained with PLLR features to those attained using phone log-posteriors (PL). Results in <ref type="table" target="#tab_11">Table 3</ref>.1 clearly show that the PLLR transformation enhances the performance of the system. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.2">Dynamic Coefficients</head><p>Dynamic coefficients have led to significant gains when applied on top of MFCC features in language verification systems, as they provide a larger temporal context, which augments the information carried out by the features. In our attempt to optimize the configuration of the PLLR-based SLR system, we tested the effect of computing dynamic coefficients on top of the PLLRs. In the experiments reported in this section, Deltas and double Deltas were estimated using equation 2.1 with values D=2 and D=1, respectively, and different systems were trained using PLLR, PLLR+∆ and PLLR+∆+∆∆ feature sets. Results are shown in <ref type="table" target="#tab_11">Table 3</ref>.2.</p><p>Using PLLR plus first order deltas yielded a 23% relative improvement in terms of C avg with regard to using only PLLRs. Second order deltas, instead, degraded the performance of the system. This degradation could possibly be due to the high dimensionality of the feature set. When computing double Deltas with the HU BUT TRAP/NN phone decoder, the feature vector reaches dimensionality 59 × 3 = 177, which combined with the 1024 dimensional GMM we use in our system, gives a 181248 dimensional supervector. This could pose a problem, as the training data might not be enough for properly estimating all the parameters. In the experiments reported hereafter in this thesis, PLLR+∆ features will be used as baseline.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.3">Variability Compensation</head><p>Several channel compensation techniques can be applied at the feature extraction stage (see Section 2.4). Among them, Feature Normalization and Feature Warping are two of the most extensively applied on features for language (and speaker) verification systems. <ref type="table" target="#tab_11">Table 3</ref>.3 presents results for the baseline systems, and systems based on PLLR+Feature Normalization (FN), PLLR+Feature Warping (FW) and PLLR+RASTA. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Overall Performance of PLLR Based Systems</head><p>Once the optimal configuration was selected, that is, PLLR features augmented with first order deltas and with no variability compensation techniques applied at the feature extraction stage, we studied the performance of the systems trained on the features obtained with different decoders and on different benchmarks.</p><p>First, results are presented for the NIST 2007 LRE dataset using the BUT TRAP/NN CZ, HU and RU phone decoders. Results are also presented for different state-ofthe-art systems: an acoustic system, which shares the modeling part with the PLLR approach, and three phone-lattice-SVM systems, which use the same phone decoders as the ones used for the PLLR approaches, thus sharing the origin of the features. System fusions have been also explored to check the complementarity between approaches. Finally, results are also presented for the NIST 2009 LRE, NIST 2011 LRE and Albayzin 2010 LRE datasets.</p><p>Details about system configuration, the applied backends and fusion procedures are given below:</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Baseline MFCC-SDC i-vector System</head><p>The concatenation of MFCC and SDC coefficients under a 7-2-3-7 configuration was used as acoustic representation for the baseline acoustic i-vector system (like in previous works <ref type="bibr" target="#b109">[109]</ref>  <ref type="bibr" target="#b114">[114]</ref>). Voice activity detection was performed by removing the feature vectors whose highest PLLR value corresponded to the non-phonetic unit using the Brno University of Technology decoder for Hungarian (see Section 2.2.2 for details).</p><p>The GMM configuration, the estimation of the total variability matrix and scoring were also performed as for the PLLR i-vector system, that is: A gender independent 1024-mixture UBM was estimated by the Maximum Likelihood criterion on the training dataset, using binary mixture splitting, orphan mixture discarding and variance flooring. The total variability matrix T was estimated according to the procedure defined in <ref type="bibr" target="#b40">[40]</ref>, using only data from target languages. A generative modeling approach was applied in the i-vector feature space, the set of i-vectors of each language being modeled by a single Gaussian distribution.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Baseline Phonotactic Systems</head><p>The three phonotactic systems applied in this work were developed under the Phonelattice-SVM approach (see Section 2.3). Given an input signal, an energy-based voice activity detector was applied. Then, the open software BUT Temporal Patterns Neural Network (TRAPs/NN) CZ, HU and RU phone decoders were applied.</p><p>Regarding channel compensation, noise reduction, etc. the three systems relied on the acoustic front-end provided by BUT decoders.</p><p>BUT decoders were configured to produce phone posteriors that were converted to phone lattices by means of HTK <ref type="bibr" target="#b156">[156]</ref> along with the BUT recipe <ref type="bibr" target="#b137">[137]</ref>. Then, expected counts of phone n-grams were computed using the lattice-tool of SRILM <ref type="bibr" target="#b144">[144]</ref>. Finally, an SVM classifier was applied, SVM vectors consisting of expected frequencies of phone n-grams (up to n = 3), weighted as in <ref type="bibr" target="#b121">[121]</ref>. A sparse representation was used, which involved only the most frequent features according to a greedy feature selection algorithm <ref type="bibr" target="#b111">[111]</ref>. L2-regularized L1-loss support vector regression was applied, by means of LIBLINEAR <ref type="bibr" target="#b59">[59]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Backend and Fusion Models</head><p>In this work, the backend setup was optimized in preliminary experiments on the development set of each database, and then applied to the corresponding evaluation set (see Appendix A for dataset configuration details  <ref type="bibr" target="#b62">[62]</ref>. When combining systems, discriminative logistic regression fusion parameters were estimated also using FoCal. <ref type="table" target="#tab_11">Table 3</ref>.4 presents results for this dataset. Regarding individual system performances, the acoustic system reaches 2.85 C avg . Performance of phonotactic systems ranges from 2.08 C avg for the one using the HU phone decoder, to 2.94 C avg for the one using the CZ phone decoder. The same happens with the PLLR systems, the best results are attained with the features based on the HU phone decoder, 2.66 C avg , and performance degrades up to 4.18 C avg when using the CZ decoder. Analyzing the results, we see that the best PLLR result is in between the performance of the best phone-lattice-SVM system and the one attained by the acoustic system.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.1">Results on the NIST 2007 LRE dataset</head><p>Looking at results of fusions, as expected, acoustic and phonotactic approaches combine well, HU-Phonotactic + acoustic-i-vector reaches 1.08 C avg . Acoustic and PLLR based i-vector systems, though sharing all the modeling part, also combine well, attaining 1.40 C avg (when combined with the HU decoder based PLLRs). The result of fusing phonotactic and PLLR based approaches, even though they share the origin of the features, provides a significant gain, getting up to 1.20 C avg . Most remarkably, when fusing the three approaches, the system gets 0.82 C avg , meaning that PLLR features provide complementary information with regard to both, acoustic and phonotactic approaches. These results are consistent also when using other decoders, gains being more remarkable when comparing C LLR metrics.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Statistical Significance</head><p>To measure the statistical significance of performance improvements (in terms of C avg , which is the primary performance measure in this work), a series of twotailed paired T-tests was carried out <ref type="bibr" target="#b68">[68]</ref>, which gives an idea of the variability of performance improvements (and thus, the robustness of such improvements) across randomly defined sets of data. To that end, the NIST LRE 2007 evaluation dataset was split into 20 language-balanced disjoint random subsets. Then, C avg values were computed on each subset for baseline systems (a), (b2) and (a)+(b2) and for the same  systems fused with the PLLR HU system: (a)+(c2), (b2)+(c2) and (a)+(b2)+(c2). <ref type="figure" target="#fig_17">Figure 3</ref>.3 provides the number of false alarm and misses for the systems. <ref type="figure" target="#fig_17">Figure 3</ref>.4 shows the mean and the confidence interval at 95% confidence level of the relative C avg improvements, revealing that they are statistically significant in all cases.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.2">Results on the NIST 2009 LRE dataset</head><p>When analyzing the results attained in this benchmark, presented in <ref type="table" target="#tab_11">Table 3</ref>.5, we find that the best single system is the Phonotactic-RU, yielding 2.24 C avg , followed by the PLLR i-vector HU phonotactic system with 2.42 C avg , and finally the MFCC-SDC i-vector, which obtains 2.70 C avg . Regarding fusions, once again the acoustic and phonotactic approaches combine well, followed closely by the fusion of the phonotactic and PLLR i-vector systems. The fusion of the two i-vector systems (acoustic and PLLR-based) yields similar figures.</p><p>The fusion of the acoustic system with any pair of phonotactic and PLLR i-vector systems leads to improved performance in all cases. Other fusions are also presented in <ref type="table" target="#tab_11">Table 3</ref>.5, remarkably the fusion of the three phonotactic systems, which achieves better performance than the fusion of the PLLR-based systems. The fusion of all 7 systems attains a remarkable 1.28 C avg .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.3">Results on the NIST 2011 LRE dataset</head><p>Performance on the NIST 2011 LRE dataset is presented in Tables 3.6 (old C avg metric) and 3.7 (new NIST 2011 C 24 avg metric).</p><p>In this benchmark, the best single system is PLLR-RU, which reaches 4.70 C avg . The MFCC-SDC i-vector system obtains 5.96 C avg and the best phonotactic system gets 6.85 C avg . Best pairwise fusions are obtained when combining both i-vector approaches, reaching 3.77 C avg . The fusion of the three systems yields up to 3.37 C avg . In NIST 2011 LRE the fusion of the three PLLR-based systems clearly outperforms the fusion of phonotactic systems. The overall fusion reaches 3.01 C avg .</p><p>Results measured with the new metric are consistent with the ones obtained using C avg , but with the new metric differences among systems are more noticeable, and relative improvements when fusing systems are (overall) more significant. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.4">Results on the Albayzin 2010 LRE dataset</head><p>With the aim of exploring possible performance differences when dealing with wideband signals, and specially in noisy environments, SLR experiments were also carried out on the Albayzin 2010 LRE dataset. Results are consistent with the ones obtained in the NIST benchmarks (remind that all of them involved 8 kHz telephone-channel speech). Focusing first on clean speech results (see <ref type="table" target="#tab_11">Table 3</ref>.8), the MFCC-SDC i-vector system reaches 2.12 in terms of C avg . Comparing the results attained by the systems trained with different decoders, we see that this time the best phonotactic system is the one trained with the CZ phone decoder (2.15 C avg ), followed closely by the HU one (2.35 C avg ). Among the PLLR i-vector approaches, instead, the HU attains a remarkable 1.41 C avg , making it the best individual system. Performance on the noisy condition suffers (obviously) a severe degradation, more pronounced in phonotactic systems. The acoustic i-vector system obtains 3.95 C avg . Among phonotactic systems, the one trained with the RU decoder provides the best performance (6.54 C avg ). On the other hand, the HU i-vector PLLR system outperforms all the individual systems also in noisy speech, attaining 3.17 C avg . <ref type="table" target="#tab_11">Table 3</ref>.9 shows the performance attained by the baseline MFCC-SDC i-vector system, the baseline phonotactic-HU system, the PLLR-HU i-vector system and fusions of them. As in other benchmarks, all pairwise system combinations obtain good results. In clean speech, the fusion of the MFCC-SDC i-vector system and phonotactic-HU led to great improvements with regard to single system performance (1.10 C avg ). Similarly, the combination of the phonotactic-HU and PLLR-HU systems attained 1.09 C avg , followed closely by the fusion of the two i-vector systems (1.20 C avg ). The fusion of the three systems still provided some further improvements, leading to 0.97 C avg , which means a 12% relative improvement with regard to the best pairwise fusion. In the noisy condition, all pairwise fusions obtain similar performance. The best out of them is the combination of MFCC-SDC i-vector and phonotactic systems (2.43 C avg ), followed by the other two, which obtain the same figure (2.65 C avg ). The fusion of the three systems reaches 1.86 C avg , providing a 23% relative improvement with regard to the best pairwise fusion.</p><p>In this benchmark, the fusion of all 7 systems still provides a slight improvement in the overall performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5">Chapter Summary</head><p>In this chapter, we have defined the PLLR features, integrated them in a language recognition system, presented an study focusing on the optimization of a PLLRbased system and evaluated its performance in several benchmarks.</p><p>The studies have revealed that PLLR features are easy to extract and integrate in state-of-the-art systems. Their usefulness has been extensively validated, as systems based on PLLR features attain competitive results in the four analyzed benchmarks.</p><p>Furthermore, the high performance attained by fusions of the PLLR-based system with baseline acoustic and phonotactic approaches reveals a complementarity with state of the art techniques, and the suitability of using PLLR features to improve overall system performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Chapter 4</head><p>Dimensionality Reduction on PLLRs</p><p>As exposed in the previous chapter, PLLRs are an effective way of conveying acoustic and phonetic information into frame level vectors. This, along with the fact that they can be computed using open-software, makes them easy to integrate in other stateof-the-art approaches based on frame level representations. Nevertheless, PLLR features have higher dimensionality than the acoustic representations that they are replacing in the systems. This can pose a computational problem when trying to deal with certain post-processing or modeling approaches.</p><p>MFCC feature vectors usually range from 7 to 19 dimensions, which are then augmented with Dynamic coefficients or with SDC, which doubles, triples or even multiplies by 7 the size of the feature vectors <ref type="bibr" target="#b139">[139]</ref>, <ref type="bibr" target="#b17">[18]</ref>. The size of PLLR feature vectors depends on the phone decoder and the phonetic inventory of the language it's trained on. As outlined in Section 3.1, though most languages have around 30 phonemes, the number of units of the phone decoders used in this work ranges from 30 to 60 (see Section 3.2 for details).</p><p>This Chapter deals with the high dimensionality issue, by studying different reduction techniques that can be applied to PLLR features.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Supervised and Unsupervised Dimensionality Reduction Techniques</head><p>There is a handful of works in different fields of speech recognition literature, aiming to reduce the number of phone models into smaller broad classes, either by clustering or by selection techniques. On the one hand, some works apply supervised clustering, which requires knowledge of the language/phonemes to define phone families, as in <ref type="bibr" target="#b79">[79]</ref>, where several phone sets are defined for a PRLM approach used in a SLR task, or in <ref type="bibr" target="#b143">[143]</ref>, where a reduced phone set is also used to reduce n-gram counts on a phonotactic SLR i-vector approach. On the other hand, unsupervised clustering based on different distance metrics like the confusion among phonemes <ref type="bibr" target="#b157">[157]</ref> or mutual information based merging and selection <ref type="bibr" target="#b88">[88]</ref> have also been applied to improve speech or language recognition.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.1">Supervised Techniques</head><p>In phonotactic SLR approaches, it is a common practice to take advantage of the phonetic knowledge to reduce the set of phone units <ref type="bibr" target="#b79">[79]</ref>, <ref type="bibr" target="#b143">[143]</ref>. Different clusterings can be performed in the phone posterior probability space (on which PLLRs are computed) based on expert knowledge of the properties that make each phoneme different from others (manner and place of articulation, voicing, etc.).</p><p>The International Phonetic Alphabet (IPA) [9] is a phonetic notation system built by linguists in order to provide a normalized and unique way of representing all the possible sounds of spoken languages. It contains 107 symbols and 55 modifiers. These symbols are mostly based on the Latin alphabet (using as few non-Latin forms as possible) and they are classified in different categories: letters represent basic sounds, that is pulmonic consonants, non-pulmonic consonants and vowels; diacritics specify these sounds, suprasegmentals point out special qualities of the sounds such as stress or durations, tones and word accents are specified by their level and contour and the rest of possible variations are covered by other symbols (see <ref type="figure">Figure 4</ref>.1 for details).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>PULMONIC CONSONANTS</head><p>Bilabial Lab. dent. Dental Alveolar P-alveo. Retroflex Palatal Velar Uvular Pharyng. Glottal   In our study, we used these tables as reference, and applied them to classify phonetic units in the BUT TRAP/NN HU phone decoder. The Hungarian language consists of the set of phonemes shown in <ref type="table" target="#tab_25">Table 4</ref>.1 (plus long forms of some of them, denoted by the suprasegmental ":") and <ref type="figure">Figure 4</ref>.2.  Four different phone classifications were considered in this study:</p><formula xml:id="formula_47">Plosive p b t d ú ã c é k g q å P Nasal m M n ï ñ N ð Trill à r ö Tap/Flap R ó Fricative F B f v T D s z S Z ù ü ç J x G X K èQ h H Lat.Fric. ìÐ Approx V ô õ j î Lat.</formula><formula xml:id="formula_48">3• @ 8 • 9• 0 • 1• u • W• o • 7• O • 2• 6 • A• OE • a• oe • E• ø • e• y • i• DIACRITICS Voiceless n¨Breathy voiced b " Dental t " Voiced š˜Creaky voiced b " Apical t " h Aspirated t h Linguolabial t « Laminal t « » More rounded O » w Labialized t w˜N asalizedẽ - Less rounded O - j Palatalized t j n Nasal release d n ff Advanced u ff G Velarized t G l Lateral release d l Retracted ē Q Pharyngealized t Q^N o</formula><p>• Family-R: The set of Reduced (R) phones used in <ref type="bibr" target="#b143">[143]</ref> 1 to reduce the number of n-gram counts in a phonotactic approach, resulting in 33 phone classes.</p><p>• Family-SL: A set of phonemes defined by merging all Short and Long (SL) phonemes (for m, n, b, t, s, z, r, l, S, c, é, ñ, j and k), the allophones (for m, n and h), and vowels according to their basic sound, resulting in 31 phone classes.</p><p>• Family-MP : A set of phonemes defined according to phonetic categories following IPA charts. Phones produced with the same Manner and Place (MP) of articulation were merged. Vowels belonging to the same regions of the IPA chart were also merged. This merging provided 23 phone classes. <ref type="table" target="#tab_1">Table 4.2</ref> shows by different colorings the clusters defined for consonant phonemes in this approach.</p><p>• Family-M : A more generic phonetic classification, where consonants produced with the same Manner (M) of articulation were merged. Vowels belonging  For each of the above families, phones included in the same phonetic class were used to define a single unit by adding the posteriors obtained in Equation 3.1, before computing the log-likelihood ratios.  (with around the same size as the Family-R feature set) suffers higher degradation (3.46 C avg ). Instead, the clustering performed according to the Family-MP criteria, which provides a feature vector whose size is almost a third of the original baseline vector size, suffers around the same (or less) degradation than the Family-R set, attaining 2.98 C avg . Finally, The Family-M feature set, reducing the PLLR vector to only 14 dimensions, performs significantly worse than the rest of the approaches, meaning that the number of clusters selected is probably too low, causing a higher loss of information.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.1.1">Results and Selection of the Optimal Supervised Technique</head><p>Given the results obtained and according to the relation between feature size and performance, Family-MP was selected as the best approach among supervised dimensionality reduction (phone merging) techniques.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.2">Unsupervised Techniques</head><p>Supervised techniques provide some benefits: they are based on phonetic information or expert knowledge criteria and can be therefore useful approximations a priori. At the same time, they pose some problems: knowledge of each language is needed, and therefore studies must be done for each phone decoder; furthermore, there is no freedom to select the dimensionality of the resulting set of phones, given that a predefined set of rules must be applied. Unsupervised techniques instead are more flexible and easily tunable, so that the output dimensionality of the sets of phones can be arbitrarily defined <ref type="bibr" target="#b157">[157]</ref>, <ref type="bibr" target="#b88">[88]</ref>. As a con, the clusters obtained with these techniques can not be easily interpreted. In this work, several clustering approaches were studied considering mutual information, correlation or covariance between phonemes, and several phoneme selection criteria, such as the overall frequency of phonemes, standard deviations of the distributions of the phonemes between languages, etc. Finally the following criteria have been applied:</p><p>• Correlation: An iterative clustering algorithm is used. In each step, the algorithm merges the closest phone pair (or phone group pair) according to the correlation among the phone posterior probabilities. The clustering provided a singular feature set, with a few sets composed of a relatively high number of phonemes, and a lot of non-grouped phonemes.</p><p>• Frequency: The N phones with the highest posterior probabilities overall in the training set are selected as most relevant, and therefore used as (reduced) phone set. With this criteria, half of the vowel phonemes were discarded, as well as most of the long, affricate and palatal consonants.</p><p>Finally, Principal Component Analysis (PCA) <ref type="bibr" target="#b12">[14]</ref> was also tested. Since PCA is an orthogonal transformation that is assumed to deal with normally distributed data ranging in (−∞, ∞), it was not a suitable transformation to be applied on the phone posterior probability space, which ranges in [0,1]. Instead, PCA can be directly applied on the normally distributed PLLR space, which ranges in [−∞, ∞].</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.2.1">Results and Selection of the Optimal Unsupervised Technique</head><p>For studying the unsupervised dimensionality reduction techniques, the baseline and the best supervised clustering technique were taken as reference. In <ref type="table" target="#tab_25">Table 4</ref>.5, the performance figures attained by those reference approaches and the systems trained on the feature sets obtained with the unsupervised techniques are shown. To provide a fair comparison among approaches, and given that the dimensionality could be easily set in these experiments, unsupervised techniques were configured to provide feature sets of 23 dimensions (matching the size of the Family-MP feature set).</p><p>As shown in <ref type="table" target="#tab_25">Table 4</ref>.5, the system trained on the Correlation reduced set of features performs much worse than the one based on the Family-MP set (3.76 vs 2.98 C avg ). The same happens with the approach trained on the Frequency reduced set of features (3.56 C avg ). Surprisingly, the feature projection method (PCA) not only outperforms other dimensionality reduction approaches, but it also outperforms the baseline system reaching 2.45 C avg .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.3">Combination of Systems using Different Decoders</head><p>This section shows results for the baseline system and the two best dimensionality reduction approaches: supervised Family-MP clustering and PCA projection, on different datasets and for different phone decoders, to check the consistency of the conclusions attained. In the set of experiments presented in this section, 10 iterations were applied to compute the Total Variability Matrix.  Even though the best performance is attained with the HU phone decoder, conclusions are the same with all decoders. The Family-MP approach degrades system performance between 6% (RU) and 16% (HU) in terms of C avg . On the other hand, PCA always provides a significant gain with regard to the baseline approach, ranging from 18% (HU) to 25% (CZ) relative improvements in terms of C avg . When fusing the systems using different decoders trained on the same set of features (meaning that we use the same baseline or clustering approach), performance improves in all cases. Fusion provides a relative improvement of 21% with regard to the best individual system when fusing the baseline approaches, 27% for the Family-MP approaches and 18% when fusing systems trained on the features projected by PCA.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.3.2">Results on the NIST 2011 LRE dataset</head><p>Results on the NIST 2011 LRE for the systems trained on the baseline, Family-MP and PCA-projected set of features are outlined in <ref type="table" target="#tab_25">Table 4</ref>.7. Results are consistent with the ones attained on the NIST 2007 LRE. On this dataset the relative degradation of the systems trained on the Family-MP set with regard to the ones trained on the baseline features range from 4% (HU) to 9% (RU) in terms of C avg . Still, when fusing the systems trained on different decoders, the performance attained by both sets of fused systems is comparable (3.79 C avg for the baseline vs 3.82 of the Family-MP ). Regarding the PCA approach, performance relative improvements range from 11% (RU) to 16% (CZ) in terms of C avg when applying this technique. The fusion of the different PCA systems using the three phone decoders reaches 3.21 C avg .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">PCA Dimensionality Optimization</head><p>Given the conclusions attained on the PLLR dimensionality reduction study, we decided to follow the study of the application of PCA on top of the features, to try to get an insight of the behavior of the features when applying this technique. Moreover, we wanted to find how far the dimensionality could be reduced, without getting a high performance loss, in order to (possibly) be able to compute Shifted Deltas on top of PLLRs. Therefore, the two main objectives were:</p><p>• To find the best dimensionality to which the PLLR feature set could be projected to optimize performance.</p><p>• To find the smallest dimensionality to which the PLLR feature set could be reduced without a high performance degradation, in order to compute shifted deltas on top of the features. <ref type="figure">Figure 4</ref>.3 shows C LLR and C avg performance for the baseline PLLR system and various systems trained on PLLR features projected by means of PCA to different dimensionalities. Baseline reaches 2.86 C avg . Note that performance is significantly enhanced by simply projecting the features (without dimensionality reduction). This could be explained by the whitening of the data attained when applying PCA, which makes the features more suitable for the diagonal covariance models used in our approaches.</p><p>There is a wide range of dimensionalities in which performance is not significantly degraded. Different ranges can be found when analyzing results. The optimal range, that is, the one in which performance is mostly enhanced, is between 47 and 27, best results being attained when using PCA to project PLLR features into 33 dimensions, with 2.11 C avg . Performance degrades slightly after that range, and starts being severe around dimension 15, where a significant loss of information is revealed.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Shifted Delta PLLRs</head><p>The dimensionality reduction study showed that PLLRs could be reduced to around 15 dimensions without strongly harming performance, which is a manageable dimension to compute Shifted Deltas.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.1">Shifted Delta Parameter Optimization</head><p>The study presented in this section focused on checking whether the application of Shifted Delta (SD) on top of PLLRs could enhance system performance. A first series of experiments searched for the parameter N , which sets the number of coefficients from which derivatives are computed at each frame (see 2.2.3 for details on SD configuration parameters). Given that 15 was the dimensionality to which features could be projected before strongly harming performance, values around this number were tested <ref type="bibr">(13, 15 and 17)</ref>. To begin with, standard configuration parameters were selected for the rest of SD parameters (the ones commonly used in our MFCC-based systems, that is d=2, P=3 and k=7). Results are outlined in <ref type="table" target="#tab_25">Table 4</ref>.8.</p><p>This first set of figures showing performance of SD-PLLR feature based systems reveal that performance can actually be enhanced by applying SD on top of PLLRs, as the approaches trained with SD-PLLRs outperform the ones trained on PCAprojected PLLR features. Furthermore, they also outperform the best result attained by projecting the features by means of PCA (PLLR-PCA to 33 dimensions, 2.11 C avg ). Even though the performance of the PCA-projected PLLR system degraded more noticeably when projecting the features to 13 dimensions (2.59 C avg ), this was Once confirmed that SD could be applied to extract relevant dynamic information from PLLR features, optimal values for the remaining SD parameters were explored.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.2">Results on NIST 2011 LRE dataset</head><p>The optimal configuration found for SD parameters on the NIST 2007 LRE dataset was used on the NIST 2011 LRE. Performance is compared to the baseline trained on standard PLLR features (not projected). Results are shown in <ref type="table" target="#tab_25">Table 4</ref>.11. <ref type="figure">Figures</ref> attained reveal that the application of SD on PLLRs provide a a 21% relative improvement in terms of C avg with regard to the baseline. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Chapter Summary</head><p>We have outlined how the PLLR feature set can be reduced to almost a third of its original size with a little performance degradation, based on a supervised clustering technique applied on the phone posterior space, which still keeps the meaning of the clusters, making it a suitable approach for other possible techniques. Furthermore, the studies presented have revealed that the sole application of PCA can enhance the performance of the systems, probably due to the whitening of the data, making them more suitable to the diagonal covariance models used in our approaches.</p><p>The search for an optimal (reduced) dimension has shown that PLLR features perform similarly when projected to a wide range of dimensions, reaching the optimal performance for around 33 dimensions.</p><p>Finally, it has been found that shifted deltas are a nice way of introducing larger temporal context information into the feature vector, providing significant performance improvements in both NIST 2007 and 2011 LRE datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>PLLR Feature Projection</head><p>The analysis presented in Chapter 4 dealing with several dimensionality reduction issues, posed a couple of questions. First, why was performance so enhanced when applying PCA? It was straightforward to understand that the decorrelation of the features would be providing a significant improvement given the diagonal covariance GMM models that we use in our experiments, yet, some additional underlying reasons could be contributing to the enhancements. Second, why was it possible to reduce PLLR dimensionality so much without degrading performance?</p><p>These issues brought us to start a study in order to get a better understanding of the way the PLLRs carry information and their behavior, exploring the multidimensional distribution of the features.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Analysis of the PLLR Feature Space</head><p>To get an insight into the PLLR feature behavior, let's go back to the definition of the features. The way PLLRs are computed from the phone posteriors was defined in Equation 3.4. That is, a PLLR is defined as the logarithm of the posterior probability ratio of a phoneme, normalized by a 1 (N −1) term. The normalization term of the definition is useful to get an intuitive idea of the values that PLLR features may take. With the normalization term, a P LLR &gt; 0 would be representing a phoneme with a posterior probability higher than the average, under the assumption that the probability mass was equally distributed among all the phonemes. On the other hand, a P LLR &lt; 0 would represent a phoneme with a lower posterior probability than the average under the same conditions. In any case, the normalization term just introduces a constant offset in the value of the features (and has therefore no effect on the results). So, for the sake of clarity, in the remaining studies, let us redefine the features, suppressing the unnecessary normalization term 1 <ref type="figure">(N −1)</ref> . Therefore, PLLRs can be simply redefined as phone posterior logits. Given a phone decoder that outputs an N -dimensional vector of phone posteriors at each frame: p = (p 1 , p 2 , . . . , p N ), such that N i=1 p i = 1 and p i ∈ [0, 1] for i = 1, 2, . . . , N , PLLRs are now re-defined as: However, when the distribution of two (or more) PLLRs is analyzed, these seemingly Gaussian distributed features show a strongly bounded shape. lie in an (N − 1) dimensional region defined as standard (N-1) simplex, which is the subset of points defined by:</p><formula xml:id="formula_49">r i = logit(p i ) = log p i (1 − p i ) i = 1, ..., N<label>(5.</label></formula><formula xml:id="formula_50">∆ (N −1) = {p ∈ R N | F(p) = N i=1 p i − 1 = 0 ∧ p i ≥ 0 ∀i} (5.2)</formula><p>where F(p) is the implicit hyper-plane function.</p><p>Given that phone posteriors range in [0,1], PLLRs would seemingly range in [−∞, ∞], but the constraint among phone posteriors is transferred into the PLLR space. From Equations 1 and 2, we derive the hyper-surface S where PLLRs lie as:</p><formula xml:id="formula_51">S (N −1) = r ∈ R N G(r) = N i=1 1 1 + e −ri − 1 = 0 (5.3)</formula><p>where G(r) is the implicit hyper-surface function.  The normal vector to the hyper-surface S is:</p><formula xml:id="formula_52">n = ∇G(r) (5.4)</formula><p>where each component n i of n is given by:</p><formula xml:id="formula_53">n i = e ri (1 + e ri ) 2 (5.5)</formula><p>Let us consider the case in which a subset of phones I ⊂ {1, 2, ..., N } accounts for most of the probability mass, that is, ∀i∈I p i = 1 − . As these phones tend to take all the probability mass, it follows that ∀i∈I p i → 1 and = ∀i / ∈I p i → 0 (therefore, r i → −∞ ∀i / ∈ I). Accordingly, for the normal vector n it holds:</p><formula xml:id="formula_54">lim →0 n i = 0 ∀i / ∈ I (5.6)</formula><p>That is, the normal vector tends to lie in the subspace Q where the set of phones I are confined. Hence, the surface is asymptotically perpendicular to any basis defined on Q. <ref type="figure" target="#fig_28">Figures 5.4(b)</ref> and 5.4(c) illustrate the surface, and its asymptotic behavior, for the case of a phone decoder with three phonetic units.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Projection of the Features</head><p>To avoid the bounding effect described in Section 5.1, we propose to project PLLRs into the hyper-plane tangential to the surface at the point where all the posteriors take the same value p i = 1 N , that is, the top of the convex surface 2 , where the normal vector is:</p><formula xml:id="formula_55">n| ri=−log(N −1) = (N − 1) N √ N ·1 (5.7) where1 = 1 √ N [1 1 , 1 2 , ..., 1 N ].</formula><p>By inspecting <ref type="figure" target="#fig_28">Figures 5.4</ref>(b) and 5.4(c), it can be seen that, for the case of a decoder consisting of 3 phonetic units, the top of the convex surface is normal to [1, 1, 1].</p><p>The tangential plane at that point is not orthogonal to any of the three asymptotic planes defined by the PLLR surface, meaning that PLLR projections on such plane will not be bounded.</p><p>In the general case (N dimensions), the kernel (null space) of the desired projection is1, then the matrix P used to project the data into the selected hyper-plane is given by:</p><formula xml:id="formula_56">P = I −1 * 1 (5.8) 2</formula><p>In the case of the original PLLR definition, this point would be located at the origin [0,0,...,0] in the PLLR space. In the case of the new the logit re-definition of PLLRs, this point is located at </p><formula xml:id="formula_57">[ 1 N −1 , 1 N −1 ,..., 1 N −1 ].</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.1">Feature Decorrelation</head><p>Finally, in order to decorrelate the parameters, we apply PCA on the transformed PLLRs, so that they are more suitable for the diagonal covariance Gaussian Mixture Model (GMM) that we are using as Universal Background Model (UBM) in our approaches. Since the projected features lie on an (N − 1)-dimensional hyper-plane, the number of non-zero eigenvalues of the PCA projection matrix will be N − 1. Therefore, the dimensionality of the feature vectors, after PCA, will be reduced by one. <ref type="table" target="#tab_35">Table 5</ref>.1 shows results for the baseline system, trained on the original set of PLLR features, and the ones attained for the system trained on the proposed projected features, as well as the figures attained after application of PCA.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.2">Results on the NIST 2007 LRE dataset</head><p>The projection of the features clearly enriches the information retrieved by the set of features, as it leads to a 19% relative improvement in terms of C avg with regard to the baseline, reaching 2.31% C avg . The decorrelation of the features helps enhancing system performance, providing a further 7% relative improvement with regard to the baseline, attaining 2.10% C avg .</p><p>More experimentation was performed to check the benefits of the method. It could be the case that PCA would not only decorrelate the feature space, but that as a side effect of the rotation it produces, it may be also suppressing the bounding effects in an unsupervised way. To check this, two experiments were carried out by applying PCA on the original (non-projected) features.</p><p>First, PCA was applied on the original (non-projected) features without dimensionality reduction, which does provide a gain comparable to that attained with the projection method, presumably attributed to a combination of both effects: decorrelation and rotation. Applying PCA and reducing the dimensionality to 58 dimensions provides no gain with regard to PCA without dimensionality reduction, that is, reducing one dimension by PCA has not the effect achieved by the projection method. This means that reducing one dimension by PCA is not equivalent to the projection method (because the latter is a singular linear transformation). This can be further confirmed by the fact that the minimum variability direction estimated on the original feature space (the one removed by PCA) is not related to the kernel of the projection method proposed in our approach (i.e. vector1 in Eq. 5.7). Actually, the direction removed by the proposed method is closely related to the maximum variability direction identified by means of PCA.</p><p>Different results are obtained by applying PCA on the original and on the projected features. The combination of the proposed projection, which ensures the removal of the bounding effect, and subsequent decorrelation by means of PCA yields the best result overall.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.3">Results on the NIST 2009 LRE dataset</head><p>The merits of the projected PLLR features were also tested in other NIST LRE benchmarks. <ref type="table" target="#tab_35">Table 5</ref>.2 presents results for the baseline system, trained on the original set of features, and the results for the system trained on the projected+PCA PLLR features, as well as figures for acoustic and phonotactic systems, and different fusions of them, to check the complementarity of the new set of features and the possible benefits it could provide compared to the baseline. As for the NIST 2007 LRE, the system trained on the projected PLLRs performs significantly better than the baseline. The fusion of the acoustic and phonotactic systems with the projected-PLLR system attains a slight improvement with regard to the combination of the acoustic, phonotactic and baseline PLLR systems, more pronounced on when comparing them in terms of C LLR .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.4">Results on the NIST 2011 LRE dataset</head><p>A comparison of SLR performance of systems trained on different sets of PLLR features for the NIST 2011 LRE is shown in <ref type="table" target="#tab_35">Table 5</ref>.3. Fusions of PLLR systems with baseline acoustic and phonotactic systems are shown too.</p><p>The relative improvement attained with the projection of the features is pronounced in this benchmark, obtaining a 17% relative improvement with regard to the baseline. Regarding fusions, once again, the fusion of the acoustic, phonotactic and projected PLLR system gets a remarkable improvement with regard to the fusion of the acoustic and phonotactic systems with the baseline PLLR system, a relative 9% in terms of C avg . </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Projected PLLRs in Noisy Environments</head><p>In collaboration with the Brno University of Technology, the projected PLLR features were tested on the challenging noisy RATS benchmark <ref type="bibr" target="#b117">[117]</ref>.</p><p>Given that despite the RATS evaluation program comprised 120, 30, 10 and 3s segments, LDC only provided 120s signals for training and testing (see Section 2.1 for details), this study <ref type="bibr" target="#b116">[116]</ref> made use of specific training and development sets constructed by making cuts of 120s signals, where main training, extended training, development and calibration sets were defined <ref type="bibr" target="#b99">[99]</ref>.</p><p>Two hybrid NN/HMM phone decoders were used to estimate three state frame-byframe posteriors. The two decoders were trained for Levantine Arabic (LE) and Czech, using RATS LE keyword search data and Czech CTS data, respectively, including data corrupted with noise at 10dB level, respectively. The phone decoders feature 36 (LE) and 38 (CZ) phonetic units.</p><p>To compute the PLLRs, first the states corresponding to the same phonetic unit were merged, following 3.1. PLLRs were computed according to <ref type="bibr">Equation 5</ref>.1, augmented with first order deltas, and projected using the procedure presented in Section 5.2, leading to a 74 dimensional feature vector for CZ and a 70 dimensional feature vector for LE.</p><p>The systems were based on an i-vector approach, using a diagonal covariance 2048 dimensional GMM-UBM, and the total variability matrix was trained on the main training set, featuring 600 dimensional i-vectors.</p><p>Two different classifiers were used then to build two different systems: Logistic Regression (LR) and Neural Networks (NN). The multiclass regularized logistic regression classifier (based on <ref type="bibr" target="#b12">[14,</ref><ref type="bibr" target="#b17">18]</ref>) was trained on the main training set with within-class covariance normalization conditioned i-vectors. The NN classifier was based on a three layer NN, with 300 neurons in the hidden layer and 6 outputs (5 target + 1 non-target languages). The NN was trained on the extended training set.</p><p>Logistic regression calibration parameters were estimated on the calibration set.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.1">Baseline Systems</head><p>The acoustic baseline system was based on the i-vector approach, using 20 cepstral PLP2 coefficients <ref type="bibr" target="#b63">[63,</ref><ref type="bibr" target="#b90">90]</ref> augmented with ∆ and ∆∆s, obtaining a 60 dimensional feature vector. The system used the same configuration and training sets as the ones used by the PLLR-based system, that is, diagonal covariance 2048 component GMM-UBM and 600 dimensional i-vectors.</p><p>The phonotactic system was based on the Subspace n-gram Modeling (SnGM) approach with a 600 dimensional subspace estimated over trigram counts trained using regularized multinomial space, as described in <ref type="bibr" target="#b142">[142]</ref>. Hard pruning of low-frequency trigrams was applied to reduce problems caused by data sparsity. 600 dimensional i-vectors were estimated, as point estimates of latent variables representing the input utterance dependent n-gram model.</p><p>Both systems were used also to train LR and NN classifiers. Logistic regression calibration parameters were estimated on the calibration set.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.2">Results on the RATS dataset</head><p>Results attained with all the systems under the LR-classifier approach are presented in <ref type="table" target="#tab_35">Table 5</ref>.4. Figures show that both LE and CZ PLLR systems outperform the PLP2-based i-vector approach. Also, PLLR systems outperform their respective phonotactic approaches (that is, the one based on the same phone decoder). Overall, the PLLR-LE system attains the best performance in most conditions, that is, 120s, 30s and 10s, and is only beaten (by a small margin) on the 3s condition by the PLLR-CZ system.</p><p>Performance of the systems based on NN classifiers are shown in <ref type="table" target="#tab_35">Table 5</ref>.5. The approaches based on NN outperform in all cases the systems based on LR classifiers. Performance figures are consistent with those attained using the LR classifier. Once again, PLLR systems outperform the PLP2-based i-vector approach, and also the phonotactic approaches in most cases. PLLR-LE is the system achieving the best performance in all conditions.</p><p>Regarding fusions, the fusion of the two PLLR systems attains good results in all conditions, as well as the fusion of both LE phone decoder-based systems. The fusion of the LE systems with the PLP2 approach attains a significant gain for short duration utterances (10s and 3s). Finally, the fusion of all systems, still provides some slight improvements in some conditions. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4">Shifted Delta Projected PLLRs</head><p>Given the results attained with systems trained on the reduced set of PLLR features augmented with Shifted Deltas, the same transformation was applied on top of the projected set of PLLR features, to check the possible enhancement it could provide.  <ref type="table" target="#tab_35">Table 5</ref>.6 shows results of the system trained with the projected features, the one trained with the projected and reduced set of features, and the one trained with projected, reduced and augmented with SD features on the NIST 2007 LRE dataset. As expected, performance degraded when reducing the dimensionality of the set of features, from 2.10 to 2.43 in terms of C avg , and significantly enhanced when using SD, reaching 1.52 C avg . <ref type="figure" target="#fig_28">Figure 5</ref>.6 represents graphically the performance of the baseline system in terms of C avg and improvements attained at each step, that is, the gain attained when projecting the features (a relative 19%), after the decorrelation of the parameter space (a further 7% with regard to the baseline) and after augmenting the features with SD (another 20%). The system trained with the fully transformed features (after the three step process) attains an overall 47% relative improvement with regard to the system trained with the original PLLR features.  <ref type="table" target="#tab_35">Table 5</ref>.7 shows results of the system based on projected+SD features on the NIST 2011 LRE dataset. Comparing the results attained by the SD-PLLR based system, using 13-2-3-7 SD parameter configuration with those attained by the system trained on the projected PLLR features, SD-PLLR features appear to be less informative. All the experimentation performed with the features in previous studies has shown that SD computation on top of PLLRs is beneficial to improve system performance. Therefore, the intuition for the reason behind these results was that the dimensionality reduction applied on top of the projected features was degrading system performance further than in other datasets, and that SD application was not improving performance enough to recover that loss. With the aim of testing this fact, other two systems were trained, based on SD-PLLR features, but computed from PLLRs reduced to not so small dimensions, using 15-2-3-7 and 17-2-3-7 SD parameter configurations. The figures attained by the SD-PLLR based system using 15-2-3-7 SD parameter configuration range close to those of the baseline system. The system trained on SD-PLLR features with 17-2-3-7 SD parameter configuration outperforms the baseline, reaching 3,80% in terms of C avg , which is the best result attained by a single system for this benchmark, and confirms that the PCA dimensionality reduction should be optimized for each dataset, before SD computation. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4.1">Results on the NIST 2007 LRE dataset</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4.2">Results on the NIST 2011 LRE dataset</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.5">Chapter Summary</head><p>The projection of PLLR features has been found as an effective way of extracting relevant information for SLR tasks, providing significant performance improvements in all benchmarks.</p><p>The study of the use of PLLR features (under different approaches) in noisy environments has revealed that these features are robust against channel mismatch and noisy conditions.</p><p>Finally, the application of shifted deltas on top of the projected features has also brought further improvements in performance, confirming that the use of dynamic information in large temporal contexts (regardless of the feature representation) provides relevant information for SLR.</p><p>The studies performed in previous chapters, have focused on the search for an optimal PLLR extraction procedure. According to the results attained, to summarize, the optimal approach for PLLR computation would comprise:</p><p>• Selection of a phone decoder with high phonetic coverage</p><p>• Estimation of PLLRs as defined in Equation 5.1</p><p>• PLLR feature projection following the procedure described in Section 5.2, using the projection matrix defined in Equation 5.8</p><p>• PCA projection performing dimensionality reduction optimization for the database • SD computation on top of the projected and reduced set of PLLRs Chapter 6</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>PLLRs for Speaker Recognition</head><p>Speaker and language recognition are closely related pattern recognition tasks that share not only the main structure of the systems, but, as it was outlined in Chapter 2, many processing aspects such as features, modeling techniques, scoring procedures, etc. Despite all the similarities, common methods and mutual resemblances that can be found among them, there are yet significant differences, which make both tasks challenging in different ways.</p><p>• The amount of training data that can be gathered for different speakers is not comparable to the training data that can be found for different languages. It is therefore straightforward that the training stages must differ significantly, as the speaker related tasks pose an extra difficulty for not having that much data (meaning positive samples) to rely on.</p><p>• Feature extraction, even when based on the same kind of features, uses different tunings and configurations on the extraction stage, as the information that is willing to be found is speaker-specific information. That is, features related with the speaker physiological characteristics such as vocal folds, length and shape of the vocal tract, pitch, energy, etc. Other high-level characteristics are related to the personal lexicon, accent, pronunciation, etc. that could help differentiating them from others. These high level features are not very used in the literature given the level of complexity that the extraction implies.</p><p>• Language Recognition has been usually treated as a multi-class recognition task while speaker recognition is established in most challenges as a binary decision task, which makes a difference in the scoring and backend stages as well as in the metrics used to evaluate the systems.</p><p>Besides, both tasks are conceptually different, given that speaker recognition systems are usually trained with data of speakers different to those of the test set, whereas language recognition systems normally rely on data of the languages they are going to be tested with. A language recognition task aiming to recognize a language without training data available for it would render the usual SR scenario in a SLR task, as proposed in the Albayzin 2012 LRE <ref type="bibr" target="#b123">[123]</ref>.</p><p>This Chapter explores the possible benefits of using PLLR features in a speaker recognition task. To that purpose, first, we provide a short overview of state-of-theart speaker recognition, focusing on the differences with regard to spoken language recognition. Then, we proceed to check test the performance of PLLR features in SR tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1">State-of-the-art Speaker Recognition</head><p>This section will cover state-of-the-art Speaker Recognition (SR) techniques not shared with spoken language recognition. Details are provided for datasets, feature extraction, modeling techniques and evaluation metrics.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1.1">Datasets</head><p>The available data for speaker recognition has grown considerably in terms of number of datasets and also regarding database size, specially considering the number of speakers per database. LDC has a relevant collection of databases built for this purpose, which date from 1993. Great efforts have been made in terms of data collection, from those first datasets used on the nineties, which involved around 2400 conversations among 500-600 speakers, to the ones that can be found nowadays, amounting to tens of thousands signals and thousands speakers, and involving test sets with upper bounds rounding 10 6 or even 10 8 trials. There are plenty of resources in different languages designed for SR tasks <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b102">102]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>NIST SRE Benchmarks</head><p>As happened for SLR, NIST also contributed significantly to the advances of SR providing benchmarks and challenging evaluations in a regular basis to the research community. Starting in 1997, NIST organized speaker recognition evaluations yearly until 2006, and every two years ever since, until 2012.</p><p>NIST SRE started focusing only on speaker detection tasks, which would involve either same handset or different handset test trials and test segments of 3, 10 or 30s. In the following years the evaluations evolved, not only in terms of the amount of training data and test segments provided, which kept growing over the years, but also in terms of evaluation tracks and challenges.</p><p>In 1997 speaker detection was introduced, in which data segments would include conversational speech from telephone calls. Speaker tracking was introduced in 1999, in which participating groups had to detect a specific speaker as a function of time.</p><p>In 2000, speaker segmentation was launched as a new task, in which several (an unknown number of) speakers had to be identified and tracked in each test segment.</p><p>Other interesting conditions were also evaluated over the years, which would vary the amount of available training data for each target speaker, speaker detection in speech signals of other languages, etc. In 2002, speaker detection tests started covering multi-device and/or multi-channel conditions. Besides telephone data, interviews (such as broadcasts and meetings) were also used for the speaker segmentation tracks.</p><p>In 2003, SRE finally concentrated simply on speaker detection tasks, with distinctions between limited data and extended data training. After that, evaluations focused on the same task, providing several training and test conditions, with one as the core test mandatory track for all participants and others as optional. The conditions covered different durations for training and test utterances, different number of single channel conversation sides for each speaker (2004), and two-channel or summed channel conversations <ref type="bibr">(2005,</ref><ref type="bibr">2006)</ref>. The NIST 2008 SRE included, besides the usual telephone conversational excerpts recorded over telephone channels, telephone data recorded over microphone channels and conversational speech data from interviews recorded over room microphone channels. The inclusion of this kind of data also allowed a new evaluation track, involving longer utterances recorded from microphone channels. In 2010, NIST introduced high vocal effort and low vocal effort speech.</p><p>In the last SRE, carried out in 2012, even though the main task remained the same, several factors changed compared to previous evaluations. For the first time, knowledge of all target trials was allowed to obtain the trial score. Besides, new tracks were included, which made a difference among known and unknown test trials, that is, test segments for which the system could assume that the non-target trials were produced by known or unknown speakers. In this evaluation, some test segments included also additive noise, which posed a new demanding challenge.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>MOBIO</head><p>MOBIO is a challenging audio and video database created on collaboration between six sites in five different countries <ref type="bibr" target="#b91">[91,</ref><ref type="bibr" target="#b101">101]</ref>, containing speech from 152 speakers, covering both spontaneous and non-spontaneous speech captured by mobile devices (mobiles and laptops). A system based on PLLR features was trained and presented to the speaker recognition MOBIO 2013 (for details see Appendix B).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1.2">Feature Extraction</head><p>Most SR systems are based on short-term spectral low-level features, which model vocal-tract properties and the spectral envelope of the sounds <ref type="bibr" target="#b86">[86]</ref>. Around the nineties, when the use of high level features gained strength in SLR, numerous works were carried out with the aim of making use of phonotactic information also for speaker recognition, and loads of efforts were made to optimize these features for SR based on the idea that different speakers can be characterized by their vocabulary, accent, pronunciation and other linguistic and speaker dependent behavioral features <ref type="bibr" target="#b86">[86]</ref>. Systems trained on these high level features proved to provide complementary information <ref type="bibr" target="#b119">[119]</ref>, but didn't reach the performance that systems based on low-level acoustic features can attain in SR tasks. In the literature, there can be found studies about speaker specific vocabulary <ref type="bibr" target="#b58">[58]</ref>, the application of phone/word n-grams obtained from phonetic/word decoders <ref type="bibr" target="#b25">[25]</ref>, and more recently the use of prosodic features <ref type="bibr" target="#b9">[11,</ref><ref type="bibr" target="#b87">87]</ref>. Nowadays, MFCC still stand out as the most common representation <ref type="bibr" target="#b23">[23]</ref>, though competitive state-of-the-art systems tend to rely on the combination of several low-level features <ref type="bibr" target="#b60">[60,</ref><ref type="bibr" target="#b72">72]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1.3">Modeling</head><p>Speaker and language recognition tasks share most of the modeling techniques. Actually, most of the spectral feature based modeling techniques applied in language recognition have been historically introduced for SR tasks <ref type="bibr" target="#b82">[82]</ref>, <ref type="bibr" target="#b80">[80]</ref>, <ref type="bibr" target="#b40">[40]</ref>, and then adapted for SLR. This way, the modeling and channel compensation techniques presented in sections 2.3 and 2.4, such as GMM-UBM, SVMs, NAP, eigenchannels, JFA or i-vectors, are extensively used also for speaker recognition.</p><p>Despite the similarities, some modeling approaches have been specifically developed and largely applied to speaker recognition. That's the case of Probabilistic Linear Discriminant Analysis (PLDA).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Probabilistic Linear Discriminant Analysis</head><p>The Gaussian PLDA approach, introduced for face detection in <ref type="bibr" target="#b118">[118]</ref>, aims to separate the undesired variability contained in the observed data and model only the desired variability information to perform recognition. In our case, an observation j corresponding to an i-vector of a speaker i is supposed to be modeled by:</p><formula xml:id="formula_58">w ij = Vy i + Ux ij + z ij (6.1)</formula><p>where:</p><formula xml:id="formula_59">y i ∼ N (0, I) (6.2) x ij ∼ N (0, I) (6.3) z ij ∼ N 0, D −1 (6.4)</formula><p>where D is a diagonal precision matrix and the hidden variables y i and x ij are the speaker and channel factors, respectively <ref type="bibr" target="#b13">[15]</ref>, while z ij is the noise term accounting for the rest of the variability. The model M = (V, U, D) is estimated by EM.</p><p>Extensive work has been made to optimize PLDA for SR. For instance, heavy tailed distributions were introduced to try to deal with some problems produced by outliers when using Gaussian modeling <ref type="bibr" target="#b81">[81]</ref>. In <ref type="bibr" target="#b24">[24]</ref>, discriminative training for PLDA function parameters is introduced. In [153] the effect of using a common speaker space distribution for all channels, but channel dependent channel space distributions, has been also explored.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1.4">Evaluation Metrics</head><p>This section will briefly describe the evaluation measures that are typically used in SR. The EER and DET curves which have already been presented as metrics in state-of-the-art SLR (see Section 2.7 for details), are evaluation metrics commonly used in (and actually more suitable for) SR tasks. The new metrics presented in this section are based on measures presented in section 2.7 and terminology will be used accordingly.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Detection Cost Function (ActDCF)</head><p>The detection cost function has been the main evaluation measure in speaker detection tasks from NIST evaluations <ref type="bibr" target="#b3">[4]</ref>. The metric is defined as a weighted sum of miss and false alarms, as follows:</p><formula xml:id="formula_60">C Det = C miss × P miss × P T + C f a × P f a × (1 − P T ) (6.5)</formula><p>To get a meaningful value (easy to understand and compare), C Det is normalized by the best (lowest) cost it could be attained without processing the trials:</p><formula xml:id="formula_61">C Def ault = min C miss × P T C f a × (1 − P T ) (6.6) C N orm = C Det /C Def ault (6.7)</formula><p>NIST evaluations used as cost model parameters C miss =C f a =1 and P T =0.001 until 2008. In NIST 2010, cost model parameters were set to C miss =10, C f a =1 and P T =0.01, focusing system performance on a low false alarm error region.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Primary Cost Function in 2012 SRE</head><p>In 2012 SRE, NIST proposed a new metric dependent on the a priori probabilities for the known/unknown non-target speakers, that took into account the cost functions attained at both operating points: the old values used until 2008 (α 1 ) and the new parameter cost values defined in 2010 (α 2 ) <ref type="bibr" target="#b4">[5]</ref>, in this way:</p><formula xml:id="formula_62">C Det = C miss × P miss × P T + C f a × (1 − P T ) × (P f a|K × P K + P f a|N K × P N K ) (6.8)</formula><p>where P K is the a priori probability for a non-target speaker to be one of the evaluation target speakers (and P N K = 1 − P K ). The final metric is defined as:</p><p>C primary = C N orm (α 1 ) + C N orm (α 2 ) 2 (6.9)</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2">Phone Posteriors for Speaker Characterization</head><p>Several works have explored how to make use of high level features for speaker recognition In the 2003 Johns Hopkins University (JHU) Summer Workshop, an extensive study was made with the aim of exploiting these features for SR <ref type="bibr" target="#b119">[119]</ref>, with results not as successful as in language recognition (compared to acoustic approaches in SR). Phone decoder phoneme posterior features have been used to build n-gram models also for speaker recognition <ref type="bibr" target="#b25">[25]</ref>. Other works aiming to employ high-level information are also focusing on prosodic features for SR <ref type="bibr" target="#b39">[39,</ref><ref type="bibr" target="#b85">85,</ref><ref type="bibr" target="#b138">138]</ref>.</p><p>With regard to the use of phonetic features for SR, in a work where the merits of multilayer perceptron based phoneme recognizer features for a SLR system are presented <ref type="bibr" target="#b154">[154]</ref>, it is stated that phoneme posterior features are speaker independent if enough amount of all kinds of speakers are used in the training stage. However, each speaker, due to unique physical characteristics, produces speech sounds in a different way. Speech disfluencies are also distinctive of each speaker, and can be used for discrimination. A phone decoder can be seen as a reference system for representing the speech sounds of any speaker in terms of the activation of its phonetic units, which include both static and dynamic information that would help catching the subtle differences between sounds uttered by each speaker. The goodness of this representation will depend on the richness of the inventory of phonetic units handled by the phone decoder, which can be selected regardless of the spoken language.</p><p>To check whether speaker dependent information is present in phonetic posteriors (or how much), a proof-of-concept experiment was carried out, using the TIMIT dataset <ref type="bibr" target="#b161">[161]</ref> and the open software TRAPs/NN phone decoder for Hungarian, developed by the Brno University of Technology (BUT) <ref type="bibr" target="#b137">[137]</ref>. where T (s) is the number of frames of the sentence uttered by speaker s. Next, as some phonemes are more frequent than others (e.g. vowels would be more frequent than consonants), phone usages were scaled to the same range, obtaining normalized average posteriors for the set of speakers:</p><formula xml:id="formula_63">p(i|s) = p(i|s) S s=1 p(i|s) (6.11)</formula><p>where S is the number of speakers.  .1 reveals significant usage differences for some phones. When comparing the usage of consonants, results suggest that one speaker tends to use "f" over "v". Regarding the vowels, there seems to be a set of 2-3 speakers that overuse or prolongate the vowels "e:", ":2" and " 2" with regard to the rest of speakers, whereas the vowels "O" and "u" are almost equally used/pronounced by all of them. These could be intrinsic characteristics of the speakers and could therefore be used for identification.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.3">Experimental setup</head><p>In this section system configuration details are provided.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Datasets</head><p>For the SR experiments with PLLR-based systems, two benchmarks were used: the NIST 2010 and 2012 SRE datasets. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>PLLR Feature Extraction</head><p>As for the spoken language recognition experiments, the BUT TRAPs/NN phone decoder for Hungarian was used to compute the PLLR features. Voice activity detection was performed by removing the feature vectors whose highest PLLR value corresponded to the integrated non-phonetic unit (see section 3.2 for details).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>MFCC Feature Extraction</head><p>MFCC features were computed with a standard configuration for SR systems. Frames of 25 ms at intervals of 10 ms were considered to estimate 13 MFCC coefficients, including the zero (energy) coefficient. Cepstral Mean Subtraction (CMS) and Feature Warping <ref type="bibr" target="#b104">[104]</ref> were applied on cepstral coefficients. The feature vector was augmented with dynamic coefficients (first-order and second-order deltas), resulting in a 39-dimensional feature vector. Voice activity detection was performed as in the PLLR system, using the integrated non-phonetic PLLR unit.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>i-vector PLDA Configuration</head><p>No special treatment was applied to the signals containing additive noise, the Qualcomm-ICSI-OGI (QIO) <ref type="bibr" target="#b8">[10]</ref> noise reduction technique (based on Wiener filtering) was independently applied to all audio streams.</p><p>Gender dependent 1024-mixture GMMs were trained as UBMs, with diagonal covariance matrix and using binary mixture splitting, orphan mixture discarding and variance flooring. Gender dependent 500 dimensional Total Variability matrices were estimated for each system on each training set. The i-vectors were centered, whitened and length-normalized <ref type="bibr" target="#b64">[64]</ref>.</p><p>A standard PLDA modeling approach was used, where gender dependent PLDA systems <ref type="bibr" target="#b15">[16]</ref> were estimated using a speaker subspace of size 150, a channel subspace of size 400 and 20 Expectation Maximization/Minimum Divergence (EM-MD) iterations.</p><p>PLDA outputs were directly used as scores for the NIST 2010 experiments.</p><p>For the NIST 2012 SRE, scores were post-processed, given that knowledge of other target speakers could be used on the scoring stage. PLDA system scores s (u, t) corresponding to a test utterance u and a training signal t were interpreted as loglikelihoods, that is: s (u, t) ≡ log p (u | t), and the likelihood p (u | i) for a speaker i was computed as the average likelihood over all the training signals of that speaker <ref type="bibr" target="#b49">[49]</ref>:</p><formula xml:id="formula_64">p (u | i) = 1 |Train (i)| t∈Train(i) e s(u,t) (6.12)</formula><p>Finally, verification scores were computed as closed-set log-likelihood ratios:</p><formula xml:id="formula_65">s (u, i) = log p (u | i) 1 N −1 j =i p (u | j) (6.13)</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Fusion and Calibration</head><p>The BOSARIS toolkit <ref type="bibr" target="#b18">[19]</ref> was used to estimate and apply calibration and fusion parameters. The whole training set was used for the estimation of calibration and fusion parameters. Given that each evaluation had different application costs, calibration parameters were optimized for each dataset. For NIST 2010 SRE experiments, the system was calibrated using P fa = 0.01, C miss = 10, C fa = 1. For NIST 2012 SRE experiments, the system was calibrated using P fa = 0.001, C miss = 1, C fa = 1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.4">Search for the Optimal PLLR Feature Configuration</head><p>As for spoken language recognition, a development study was carried out to optimize the PLLR feature extraction parameters for SR. The study was performed on the NIST 2010 SRE dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Dynamic coefficients</head><p>First of all, the effect of dynamic coefficients was tested on top of the PLLRs. <ref type="table" target="#tab_43">Table  6</ref>.1 shows results for the system trained only on PLLR features, PLLRs augmented with first order (∆) dynamic coefficients and PLLRs augmented with first and second order dynamic coefficients (∆∆).    Unlike for SLR, in SR the projection of the features does not enhance the performance of the system. This fact, must be related to the information contained in the direction [1,1,...,1] that is "suppressed" when projecting the features, the one related to the normal vector of the hyperplane to which the features are projected (see Section 5.2 for details). Given that the projection is a reversible transformation, the information remains in all the other directions after the projection (the info is not discarded nor deleted), but it cannot be directly modeled. The inability to directly model that information seems to be disadvantageous for SR.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.5">Overall Performance of PLLR Based Systems</head><p>This section presents and briefly analyzes results for PLLR based systems using the optimal configuration discussed in the previous section, on the NIST 2010 and 2012 SRE datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.5.1">Results on the NIST 2010 SRE dataset</head><p>First, the PLLR system was tested on the NIST 2010 SRE dataset, compared and fused with the MFCC-based system. Figures attained are shown in <ref type="table" target="#tab_43">Table 6</ref>.4. The performance of the PLLR-based i-vector system is far from the one attained by the MFCC-based i-vector system. Depending on the condition, the EER doubles or even triples the one attained by the acoustic system. Nevertheless, quite good performance is attained by the PLLR-based system compared to those usually attained by phonotactic systems on SR tasks <ref type="bibr" target="#b25">[25]</ref>.</p><p>When focusing on the results attained by the fusion of both systems, the contribution of PLLRs is remarkable, as the help improving MinDCF performance in all conditions with a relative improvement with regard to the MFCC-based system ranging from 7% to 16%. They also help improving EER in all but core condition 4, providing up to a 25% relative improvement (core condition 1).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.5.2">Results on the NIST 2012 SRE dataset</head><p>The same tests were carried out on the NIST 2012 SRE dataset to check the contribution of the PLLR-based i-vector system. Results are shown in <ref type="table" target="#tab_43">Table 6</ref>.5 for core conditions 2 and 5, which involve telephone recordings with no added noise and recorded in noise, respectively. Results are similar to those attained on the NIST 2010 SRE dataset. The performance of the PLLR-based system is worse than the one of the MFCC-based system, but their fusion provides a gain with regard to the  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.6">Chapter Summary</head><p>In this chapter, the utility of PLLR features for SR tasks has been studied. As experiments for NIST 2010 and 2012 evaluation datasets have exhibited, the performance attained by systems based on PLLRs is far from the performance attained by similar systems based on other more common spectral features (such as MFCCs or PLP).</p><p>However, PLLR features do provide a way of extracting further information for SR tasks. The complementarity of PLLR features with regard to other spectral approaches has been empirically shown when fusing PLLR and MFCC based systems.</p><p>Chapter 7</p><p>Conclusions and future work</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.1">Conclusions</head><p>The research carried out to fulfill this thesis has followed a natural structure, as the conclusions attained after each study set the course of the following experimentation. The manuscript has been organized following that time-line structure, and the conclusions of each experimental chapter have been the unifying thread with the next one. The main conclusions of the work have been therefore introduced along the development of this manuscript.</p><p>The Phone Log-Likelihood Ratios have been formally defined. The former experimentation using the features integrated in a spoken language recognition i-vector system proved that the features provide an effective way to incorporate acousticphonetic information into frame-level features. PLLRs are easy to compute and to integrate in state-of-the-art language recognition systems, which makes them useful in different contexts and applications.</p><p>The effectiveness of these features was first tested in four different benchmarks, in which the systems making use of the features have consistently yielded competitive performances, outperforming, in most cases, those of the baseline acoustic (MFCC-SDC i-vector) and phonotactic (phone lattice-SVM) systems.</p><p>Pairwise fusions of the PLLR-based system with the baseline approaches showed that PLLRs provide complementary information to both, acoustic and phonotactic approaches. Furthermore, the fusion of the three systems still provides gains with regard to the fusion of the baseline approaches, proving the complementarity of PLLR features with state-of-the-art features.</p><p>Next studies focused on reducing the dimensionality of feature vectors, so that common techniques applied on top of features like SDC could be also tested on PLLRs. Dimensionality reduction by means of supervised techniques, making use of information related to the phonetic categories in IPA charts, provided a way of decreasing the feature vector size into almost a third of the original size, with just a small system performance degradation. Among unsupervised techniques, PCA was successfully applied, showing that the set of features could be reduced to almost a third of their original size, not only without information loss, but actually enhancing system's performance.</p><p>The application of shifted delta transformation on top of the reduced set of PLLRs proved to be a convenient method to increase the potential of the features, and confirmed that (as previously found for MFCCs) using a larger spectro-temporal context expands the information conveyed the information carried out by the features.</p><p>The discovery that a reduced feature set, obtained by means of PCA, was more informative than the original led us to the analysis of the feature space. The research carried out revealed that the features were bounded by an hyper-surface, which was asymptotically tangential to the axes of the reference system. This would presumably limit the movement of the PLLR features with regard to the axes, limiting the information they provide. A projection method was developed to get rid of this effect, which projected the features to the top of the bounding surface, removing the asymptotic behavior. The system based on the projected set of features revealed that the method was a suitable transformation to be applied on the PLLRs. The combination of this technique with PCA further increased the effectiveness of the system.</p><p>Features were then tested on the noisy database RATS, using i-vector modeling approaches and logistic regression and neural networks as classifiers. This new series of experiments provided a new set of conditions to test the features, as the benchmark includes short time and noisy signals. Systems based on PLLR features attained high performance in all conditions.</p><p>Finally, the projected features were also tested in combination with the shifted delta transformation. This system, which combined the most significant improvements found in the presented studies, was tested on a NIST LRE benchmark and achieved the best result among all the experimentation carried out.</p><p>Research was extended to speaker recognition. The optimization of the PLLR extraction for a state-of-the-art i-vector PLDA approach revealed that similar configurations are optimal for both, SLR and SR systems, with an important difference: the projection method does no improve the performance of the PLLR-based systems in SR tasks. Experimentation with the i-vector PLDA systems showed that the PLLR features are not as informative as the ones used in state-of-the-art systems, such as MFCCs. Nevertheless, when fusing PLLR-based and MFCC-based systems, there is still a gain in performance, revealing a complementarity between both approaches that could be useful to improve the overall system robustness.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.2">Future Work</head><p>Extensive studies have been presented exploring the usefulness of PLLR features in different SLR benchmarks. Yet, the versatility of these features could be further analyzed, testing their merits in other systems and scenarios, which suggests several possible future research lines to give continuity to this project.</p><p>The approach has been widely tested under i-vector and Gaussian modeling techniques. Research could also focus on extracting PLLRs from other phone decoders, based on the phonetic inventory of other languages, or even trying to combine the outputs of different phone decoders.</p><p>Regarding speaker recognition, the fact that projected PLLR features do not enhance system performance opens an experimentation track. Studies should focus on trying to understand why the speaker-related information contained in the direction used for the projection cannot be properly modeled, or even whether (and why) speaker information is being lost after projecting PLLRs.</p><p>It would be interesting analyzing also the merits of PLLR features in Text Dependent Speaker Recognition (TDSR) tasks. The nature of this task, performing SR in utterances with fixed phonetic content, could be an optimal scenario to further understand the behavior of PLLR features for SR. Analysis of the performance of PLLR-based systems in TDSR tasks could help optimizing the speaker dependent information of the features, e.g. by discarding phonetic (utterance-dependent) content.</p><p>Besides the above mentioned research lines, Deep Neural Networks seem to be the way to go in SR and SLR fields. Latest works using DNNs have attained outstanding results in both tasks, outperforming other state-of-the-art techniques. In particular, given the success of PLLRs, which are typically extracted from the output layer of a neural network trained on phonetic classes (or states), a promising line of research involves extracting and using bottleneck features, or other kind of DNN-based features, at the frame level, just in the same way as it was done with PLLRs.</p><p>have been computed on the subset of 30-second speech segments of the test set for the closed-set condition (2158 segments), which was the primary task in the NIST 2007 LRE.  for training models. Each of them was mapped either to a target language or to nontarget languages 5 . For example, Mainland and Taiwan Chinese from NIST 2007 LRE and Mandarin Chinese from VOA were all mapped to Mandarin Chinese, whereas Arabic was mapped to non-target languages. Persian and Farsi were mapped to the same language, as was properly pointed out in <ref type="bibr" target="#b77">[77]</ref>.</p><p>For languages appearing in VOA recordings, the longest speech segments out of each file were posted to the training dataset, using no more than 2 segments per file, and a minimum of 225 segments per language. The number of segments extracted per file was relaxed (augmented) for those languages with few files in VOA.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.3 NIST 2011 LRE</head><p>In the NIST 2011 LRE, 24 target languages were considered (see <ref type="table" target="#tab_1">Tables A.3, A.4)</ref>. Among them, 9 languages had never been used before in NIST LRE. Development data specifically collected for these 9 languages were sent to participants, including 100 30-second segments per language. For a better coverage, these subsets were randomly split into two disjoint subsets (each having approximately half the segments for each language/dialect): the first half was used to train specific models for  <ref type="table" target="#tab_11">225  80  294  390  -404  Hindi  174  160  178  667  -213  Lao  ----41  62  Mandarin  331  158  230  1015  -360  Panjabi  -32  -9  45  299  Pashto  --281  395  -383  Polish  ----46  267  Russian  66  160  299  511  -441  Slovak  ----56  280  Spanish  531  240  242  385  -419  Tamil  165  160  ---414  Thai  64  80  -188  -375  Turkish  --289  394  -276  Ukrainian  --281  388  -170  Urdu  69  80  299  379  -478  Non-Target  ------TOTAL  1989  1470  3135  6623  446  7616</ref> the new languages, and the second half was used to estimate backend and fusion parameters <ref type="bibr" target="#b111">[111]</ref>.</p><p>To train more robust models for the target languages, additional data was included from databases distributed by the Linguistic Data Consortium (LDC), some of them containing conversational telephone speech (LDC2006S45 for Arabic Iraqi, LDC2006S29 for Arabic Levantine) and others containing broadcast speech (LDC2000S89 and LDC2009S02 for Czech). For these latter, only automatically detected telephone-speech segments were used.</p><p>The remaining materials were extracted from wide-band broadcast news recordings, dowsampling them to 8 kHz and applying the Filtering and Noise Adding Tool 6 (FANT) to simulate a telephone channel. The COST278 Broadcast News database <ref type="bibr" target="#b151">[151]</ref> was used to get speech segments for Czech and Slovak. Arabic MSA was extracted from Al Jazeera broadcasts included in the KALAKA-2 database created for the Albayzin 2010 LRE <ref type="bibr" target="#b128">[128]</ref>. Finally, broadcasts were also captured from video </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.4 Albayzin 2010 LRE (KALAKA-2)</head><p>The Albayzin 2010 LRE dataset (KALAKA-2) included wide-band 16 kHz TV broadcast speech signals for six target languages (see <ref type="table" target="#tab_50">Table A</ref>.5). The Albayzin 2010 LRE <ref type="bibr" target="#b128">[128]</ref> featured two main evaluation tasks, on clean and noisy speech, respectively. In this thesis, acoustic processing involved downsampling signals to 8 kHz, since all the systems were designed to deal with narrow-band signals.</p><p>The training, development and evaluation datasets used for this benchmark matched exactly those defined for the Albayzin 2010 LRE. For the primary clean-speech language recognition task, more than 10 hours of clean speech per target language were used for training. For the noisy-speech language recognition task, besides the clean speech subset, more than 2 hours of noisy/overlapped speech segments were used for each target language. The distribution of training data, which amounts to around 82 hours, is shown in <ref type="table" target="#tab_50">Table A</ref>.5. Only 30-second segments were used for development purposes. The development dataset used in this thesis consisted of 1192 segments, amounting to more than 10 hours of speech. Results reported in this thesis were computed on the 30-second, closed set condition (for both clean speech and noisy speech conditions) of the Albayzin 2010 LRE evaluation corpus. The distribution of segments in the development and evaluation datasets is shown in <ref type="table" target="#tab_50">Table A</ref>.5. For further details, see <ref type="bibr" target="#b129">[129]</ref>.   These datasets were used to support the development of different parts of the system. The partition was performed as follows:</p><p>• NIST 2004, 2005 and 2006 SRE: These datasets were used to obtain the training signals for the Universal Background Model, Channel Compensation, Impostor, ZNorm and TNorm sets. <ref type="table" target="#tab_50">Table A</ref>.7 displays the number of signals assigned to each set.  • NIST 2008 Follow Up: Some of the signals present in this dataset were recorded with the same microphone types used for NIST 2004-2008 datasets, whereas others were recorded using new microphone types. To take advantage of this particularity, that could provide robustness against channel variabilities, signals were divided into three subsets: Channel compensation, ZNorm and TNorm. <ref type="table" target="#tab_50">Table A</ref>.9 shows the number of signals included in each subset.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.7 RATS</head><p>Experiments on RATS dataset <ref type="bibr" target="#b117">[117]</ref>, were performed using the data configuration and partitions as defined in <ref type="bibr" target="#b99">[99,</ref><ref type="bibr" target="#b116">116]</ref>. and channel type condition dependent ZT normalization was performed on trial scores.</p><p>Side-info-conditional fusion and calibration was performed with FoCal <ref type="bibr" target="#b62">[62]</ref>, using channel type and gender conditioning. Fused scores were calibrated to be interpreted as detection log-likelihood-ratios, and a Bayes threshold of 6.907 was applied to make the hard decisions.</p><p>Contribution:</p><p>• Development of both GMM-SVM subsystems.</p><p>• Sufficient statistic compensation.</p><p>• Development of JFA subsystem.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Results:</head><p>GMM-SVM and dot-scoring approaches stood out as the best performing individual systems, followed by the GLDS-SVM and JFA systems. The fusion of the systems attained the best results, not being yet significantly better than the best individual systems. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.2 NIST LRE 2011</head><p>Evaluation: In the challenge organized in 2011, the main task differed from previous NIST LRE evaluations, as it focused on language pair recognition. The evaluation comprised 24 target languages, 9 of which had never been used before. For more details, see the evaluation plan <ref type="bibr" target="#b6">[8]</ref> and the paper reporting the evaluation analysis <ref type="bibr" target="#b69">[69]</ref>.</p><p>System: 5 subsystems were trained for the evaluation, which were then fused using four different configurations, to build one primary and three contrastive systems. The five mentioned subsystems were designed as follows:</p><p>• Three high-level (phonotactic) phone-lattice SVM subsystems, based on the BUT decoders for Czech, Hungarian and Russian.</p><p>The three subsystems followed the same configuration. First, energy based VAD was applied to remove non-speech segments. BUT TRAPS/NN CZ, HU and RU phone decoders were used to perform phone tokenization. The three non-phonetic units (int, short, pau) were integrated into a single non-phonetic unit. Phone state posteriors and phone-lattices were computed by means of HTK 3 <ref type="bibr" target="#b156">[156]</ref>. The lattice-tool from SRILM 4 <ref type="bibr" target="#b144">[144]</ref> was used to estimate phone n-grams. LIBLINEAR 5 <ref type="bibr" target="#b59">[59]</ref> was used to train SVM classifiers, where SVM vectors consisted of expected counts of n-grams extracted from the lattices, weighted by their background probabilities.</p><p>• Two low-level (acoustic) subsystems: a Linearized Eigenchannel GMM subsystem based on channel compensated statistics and Generative i-vector subsystem computed from channel compensated statistics.</p><p>Both subsystems relied on MFCC-SDC features, computed under a 7-2-3-7 configuration. A gender independent 1024-mixture GMM-UBM was trained by EM-ML using binary mixture splitting, orphan mixture discarding and variance flooring. For each input utterance, UBM-MAP adaptation was applied and zero and first order statistics were computed and used as features. where τ = 16 is the relevance factor, n l are the zero order statistics of language l, andx l are the channel compensated first order statistics of the language model.</p><p>i-vector system: An i-vector approach, as defined in Total Variability Factor Analysis in Section 2.4, was used, in which 500 dimensional i-vectors were computed from channel compensated sufficient statistics using only training data from target languages. A generative Gaussian approach was used for scoring (each language being modeled by a single Gaussian).</p><p>Subsystems were combined in four different ways, depending on the normalization applied, and the data used for training the backend and fusion parameters. Details of these four configurations are summarized in <ref type="table" target="#tab_50">Table B</ref>.1: Contribution:</p><p>• Development of both acoustic subsystems.</p><p>• System calibration and fusion tunning.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Results</head><p>: <ref type="table" target="#tab_50">Table B</ref>.2 shows the performance of all individual systems submitted on the 30s test set. All phonotactic and acoustic individual systems performed similarly. The fusion of three phonotactic systems outperformed the fusion of the acoustic approaches. The fusion of all 5 systems attained still a significant gain, achieving the best performance overall.  <ref type="table" target="#tab_50">Table B</ref>.3 shows the results of the fused system in all test sets (30s, 10s and 3s) as well as the results for the contrastive systems. No significant differences can be found between their performances except on the 3s condition, where contrastive system 3 yielded slightly better figures. Further details can be found in <ref type="bibr" target="#b113">[113]</ref> and <ref type="bibr" target="#b114">[114]</ref>. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.3 NIST SRE 2012</head><p>Evaluation:</p><p>NIST 2012 SRE focused, as previous evaluations, on speaker detection tasks, but included new conditions and challenges making the evaluation different from previous ones.</p><p>NIST 2012 SRE included noisy speech in some test segments, making the detection task more demanding. The evaluation task also included tracks with distinctions regarding known/unknown trials. Besides, or the first time in NIST SRE, the use of information from other target trials was allowed to compute the trial score. For more details on the evaluation, see <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b70">70]</ref>.</p><p>System:</p><p>The primary system submitted by EHU consisted of the fusion of two subsystems. Both subsystems were based on an i-vector PLDA approach (see Total Variability Factor Analysis in Section 2.4 and Section 6.1.3 for details), using MFCC and PLLR features, respectively.</p><p>MFCC features were computed following the procedure described in 6.3 resulting in a 39-dimensional feature vector. PLLRs were computed using the BUT TRAPs/NN phone decoder for Hungarian, as shown in Section 3.3, producing a 59-dimensional feature vector at each frame t. Voice activity detection was performed by removing the feature vectors whose highest PLLR value corresponded to the integrated nonphonetic unit.</p><p>Training and evaluation sets were defined as described in A.6. The Qualcomm-ICSI-OGI (QIO) <ref type="bibr" target="#b8">[10]</ref> noise reduction technique was independently applied to the audio streams. The full audio stream was taken as input to estimate noise characteristics.</p><p>Two gender dependent UBMs, each consisting of 1024 mixture components, were estimated on the same dataset used for the EHU NIST 2010 SRE submission (see A.6), using the Sautrela toolkit. Two gender dependent Total Variability matrices were estimated on the whole training set, by means of Sautrela. The i-vector dimensionality was set to 500. Gaussian PLDA was also estimated on the whole training set <ref type="bibr" target="#b17">[18]</ref>.</p><p>PLDA system scores s(T , S) were used as log-likelihoods, so that the likelihood of a test utterance S given a speaker i was computed as the average likelihood of S over all the training signals T of that speaker, as follows:</p><formula xml:id="formula_66">p(S|i) = 1 |T rain(i)| T ∈T rain(i) e s(T ,S) (B.4)</formula><p>Finally, speaker log-likelihood ratios were computed from speaker log-likelihoods using flat priors.</p><p>Calibration and fusion were estimated and applied by means of the Bosaris toolkit <ref type="bibr" target="#b18">[19]</ref>, using the whole training set to estimate calibration/fusion parameters.</p><p>Contribution:</p><p>• Development of both subsystems.</p><p>• System calibration and fusion tunning.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Results:</head><p>Results on the SRE 2012 dataset are shown in <ref type="table" target="#tab_50">Table B</ref>.4. Note that no special treatment for noisy conditions was performed to obtain these results. Once again, the result attained by the acoustic system is better that the one obtained with the PLLR-based approach in all conditions. However, the fusion of both approaches attains up to a 38% relative improvement in terms of MinDCF and up to a 29% relative improvement in terms of ActDCF with regard to the acoustic approach, which reveals a complementarity between the features. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.4 MOBIO 2013</head><p>Evaluation:</p><p>MOBIO (MObile BIOmetry) is a face and speaker database containing speech utterances (and videos) of 52 female and 100 male speakers. All the data in this benchmark was collected using mobile devices <ref type="bibr" target="#b101">[101]</ref>. The audio signals comprise both planned and spontaneous speech.</p><p>The MOBIO 2013 evaluation proposed a speaker recognition task in a highly demanding benchmark, with signals extracted from telephone conversations obtained with mobile devices, that contained audio segments of short durations and speech in noisy conditions.</p><p>System:</p><p>The EHU system was based on a i-vector-PLDA approach (see Total Variability Factor Analysis in Section 2.4 and Section 6.1.3 for details). The feature extraction and the VAD were done using the Sautrela toolkit <ref type="bibr" target="#b105">[105]</ref>. PLDA <ref type="bibr" target="#b100">[100]</ref> was applied directly on the extracted 500 dimensional i-vector space. A gender independent 1024 component UBM, an i-vector extractor, and gender dependent PLDA systems were trained on the background set of the MOBIO database. The development dataset was used only to estimate the calibration parameters. Also, a collaboration was made with Laboratorio de sistemas de Língua Falada (L2F), fusing the systems submitted by both groups to the MOBIO evaluation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Contribution:</head><p>• System development.</p><p>• System Calibration and fusion tunning.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Results</head><p>: <ref type="table" target="#tab_50">Table B</ref>.5 shows the EER of the systems submitted by EHU and L2F, as well as the best performing primary systems submitted to the competition (considering the evaluation set). All EHU systems performed significantly better on the male test set than on the female test set. The performance of the L2F-EHU fused system was comparable to that of the most competitive systems. The fusion of all the systems (11 in total) attained a significant gain with regard to any of the individual approaches. <ref type="figure">Figure B</ref>.2 shows the DET curves for all the submitted systems on the female and male test sets.  </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>Unibertsitateko Elektrizitate eta Elektronika Saileko Software Teknologien Lan Taldean garatutako tesi hau, hizkuntza eta hiztun ezagutze arloen ikerketan oinarritzen da. Zehazki, ikerketak, informazio akustiko-espektrala eta fonotaktikoa daramaten ezaugarri multzo bat aztertzen du, erauzketa konfigurazio hoberena bilatuz, egungo hizkuntza-ezagutze sistemetan integratzeko aukerak aztertuz eta beste sistema akustiko eta fonotaktikoekin duen osagarritasuna aztertuz. Ikerketak erakusten du fonemen log-probabilitate erlazio (ingelesetik, PLLRak) izendatutako ezaugarri hauekin entrenatutako sistemak, datu-base ugarietan, oso emaitza onak lortzen dituztela, gaur eguneko beste sistema askorekin konparatuz. Gainera, ezaugarri hauetan oinarritutako sistemak beste hurbilketa akustiko eta fonotaktikoekiko informazio osagarria eskaintzen dutenez, sistema orokorren errendimendua hobetzeko egokiak dira. Ezaugarri hauen erabilpena hiztun-ezagutze sistemetan ere ikertzen da. Kontextu honetan, PLLRak erabiltzen dituzten sistemen emaitzak ez dira sistema akustikoak lotzen dituztenak bezain ikusgarriak, baina hurbilpenak oraindik informazio osagarria eskaintzen du, sistema orokorren ekimena hobetzeko erabilgarria izan litekeena ere.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>89 5.5 Chapter Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . 90 6 PLLRs for Speaker Recognition 91 6.1 State-of-the-art Speaker Recognition . . . . . . . . . . . . . . . . . . 92 6.1.1 Datasets . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 92 6.1.2 Feature Extraction . . . . . . . . . . . . . . . . . . . . . . . . 94 6.1.3 Modeling . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 94 6.1.4 Evaluation Metrics . . . . . . . . . . . . . . . . . . . . . . . . 95 6.2 Phone Posteriors for Speaker Characterization . . . . . . . . . . . . . 97 6.3 Experimental setup . . . . . . . . . . . . . . . . . . . . . . . . . . . . 98 6.4 Search for the Optimal PLLR Feature Configuration . . . . . . . . . 101 6.5 Overall Performance of PLLR Based Systems . . . . . . . . . . . . . 103 6.5.1 Results on the NIST 2010 SRE dataset . . . . . . . . . . . . 103 6.5.2 Results on the NIST 2012 SRE dataset . . . . . . . . . . . . 103 6.6 Chapter Summary. . . . . . . . . . . . . . . . . . . . . . . . . .. . 105 5.1 Distributions of (a) frame-level phone posteriors and (b) frame level phone log-likelihood ratios for the Hungarian phone a:. . . . . . . . . . . . . . 78 5.2 Distributions of (a) phone posteriors and (b) PLLRs, for three pairs of phones, a: vs E (red), i vs i: (black) and dz vs h (blue). . . . . . . . . . 79 5.3 Distributions of (a) phone posteriors and (b) PLLRs, for the set of phones (a:, E, O). . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 79 5.4 (a) Standard 2-simplex defined by phone posteriors in the case of a phone decoder with 3 phonetic units. Graphs (b) and (c) show the hyper-surface where PLLRs lie for the case of a phone decoder with 3 phonetic units. . 80 5.5 Distribution of the PLLRs shown in Figures 2(b) and 3(b) after projecting them into the defined hyper-plane tangential to the surface at the point [1/N, 1/N, ..., 1/N]. . . . . . . . . . . . . . . . . . . . . . . . . . . . . 82 5.6 Cavg × 100 performance for the Baseline (blue), Projected PLLR (red), Projected PLLR + PCA 58 (green) and Projected PLLR + PCA 13 + SD (purple) approaches. . . . . . . . . . . . . . . . . . . . . . . . . . . . . 89 6.1 Normalized average posteriorsp(i|s) (see Equation 6.11) of seven Hungarian phones on the same utterance (sx9) of the TIMIT dataset for 7 different speakers. The subset of phones represents fricative labiodental consonants (f and v) and a subset of vowels (e:, :2, 2, O, u), as defined in the International Phonetic Alphabet. . . . . . . . . . . . . . . . . . . . . . . . . . 98 B.1 DET curves of the EHU fused system for the NIST 2010 SRE core test conditions. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 124 B.2 DET curves of the primary systems submitted to MOBIO 2013 (images taken from [84]) . . . . . . . . . . . . . . . . . . . . . . . . . . 133 3.1 C avg × 100 and C LLR performance for i-vector systems using phone log-posteriors (PL) features and PLLRs computed with the HU BUT decoder, on the NIST 2007 LRE primary evaluation task. . . . . . . 50 3.2 C avg × 100 and C LLR performance for i-vector systems using PLLR, PLLR+∆ and PLLR+∆+∆∆ features computed with the HU BUT decoder, on the NIST 2007 LRE primary evaluation task. . . . . . . 51 3.3 C avg ×100 and C LLR performance for i-vector systems using PLLR features computed with the BUT HU decoder, with: (a) no noise reduction technique, (b) Feature Normalization (FN), (c) Feature Warping (FW) and (d) RASTA, on the NIST 2007 LRE primary evaluation task. 51 3.4 C avg ×100 and C LLR performance for the MFCC-SDC i-vector baseline system, i-vector systems using PLLR features, phonotactic baseline systems and the fusion of them, for each of the BUT decoders, on the NIST 2007 LRE primary evaluation task. . . . . . . . . . . . . . . . 54 3.5 C avg × 100 and C LLR performance for the baseline phonotactic and i-vector systems, the PLLR i-vector system and the fusion of them, on the NIST 2009 LRE primary evaluation task. . . . . . . . . . . . 56 3.6 C avg × 100 and C LLR performance for the i-vector baseline system using acoustic features (MFCC-SDC), i-vector systems with PLLR+∆ features, phonotactic baseline systems and the fusion of them, for each of the BUT decoders, on the NIST 2011 LRE primary evaluation task. 58 3.7 min C 24 avg ×100 and actual C 24 avg ×100 performance for the phonotactic and acoustic i-vector baseline systems, the PLLR+∆ i-vector system and the fusion of them, on the NIST 2011 LRE primary evaluation task. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 59 3.8 C avg × 100 and C LLR performance for the baseline systems, and the PLLR i-vector systems using phone decoders for CZ, HU and RU on the Albayzin 2010 LRE primary task on clean and noisy speech. . . 60 3.9 C avg × 100 and C LLR performance for the baseline systems, the PLLR i-vector system and different fusions on the Albayzin 2010 LRE primary task on clean and noisy speech. . . . . . . . . . . . . . . . . .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 1 . 1 :</head><label>11</label><figDesc>Structure of a recognition system.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Finally</head><label></label><figDesc>, conclusions and open issues to be addressed in future work are discussed in Chapter 7. Appendix A provides details about database configuration, covering all the databases used in the experiments reported in this thesis: NIST 2007, 2009 and 2011 LRE, Albayzin 2008 and 2010 LRE, and NIST 2010 and 2012 SRE.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 2 . 1 :</head><label>21</label><figDesc>Classification of features for spoken language recognition.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 2 . 2 :</head><label>22</label><figDesc>Window parameter definitions for the estimation of the first order deltas that conform SDC features.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 2 . 3 :</head><label>23</label><figDesc>Representation of a phone lattice of a utterance using a phone decoder of 5 phone units. The 1-best decoding output is marked as the optimal path in the lattice.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 2 . 4 :</head><label>24</label><figDesc>Two dimensional representation of a SVM</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head></head><label></label><figDesc>, . . . , n 1 , ..., D n K , . . . , n K ] and x = [x 1 , . . . , x K ] are known as the zero and first order sufficient statistic supervectors, respectively. In the GMM-SVM approach, these MAP-adapted GMM supervectors are used to train the SVM classifiers.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>2 Figure 2 . 5 :</head><label>225</label><figDesc>phone decoders as feature extractors have relied on different approaches, either using a single Phone Recognizer followed by Language Modeling (PRLM) (see Figure 2.5) or using multiple Parallel Phone Recognizers followed by Language Modeling (PPRLM), in which several language dependent phone recognizer based systems are fused, as shown in Figure Diagram of a PRLM system</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head>Figure 2 . 6 :</head><label>26</label><figDesc>Diagram of a PPRLM system</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_12"><head>Figure 2 . 7 :</head><label>27</label><figDesc>Calibration and fusion of several SLR systems</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_13"><head>Figure 2 . 8 :</head><label>28</label><figDesc>Target and non-target trials and classification errors</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_14"><head>Figure 2 . 9 :</head><label>29</label><figDesc>DET curves for two systems. The system operating point is marked with (x) whereas the optimal operating point is marked with (o).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_16"><head>Figure 3 . 1 :</head><label>31</label><figDesc>Standard 2-simplex defined by phone posteriors in the case of a phone decoder with 3 phonetic units.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_17"><head>Figure 3 .</head><label>3</label><figDesc>1 shows the Standard 2-simplex for the case of a phone decoder with 3 phonetic units. In the graphic, each vertex represents one of the three pure phonetic units, and edges represent mixtures of the two phone units connected by them.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_19"><head>Figure 3 . 2 :</head><label>32</label><figDesc>Distributions of frame-level phone posteriors (first row), phone logposteriors (second row) and phone log-likelihood ratios (third row) for 5 phonetic units (A:, E, e:, i, O) of the Brno University of Technology decoder for Hungarian, computed on a subset of the NIST 2007 LRE test set.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_20"><head>Figure 3 . 3 :</head><label>33</label><figDesc>False alarm and miss errors on the NIST LRE 2007 primary task for baseline systems: (a) acoustic MFCC-SDC i-vector system, (b2) Phone-Lattice-SVM system and (a)+(b2) the fusion of the two latter, taken alone (dark gray) and fused with (c2) the HU PLLR i-vector system (light gray).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_21"><head>Figure 3 . 4 :</head><label>34</label><figDesc>Means of Cavg relative improvements and their corresponding intervals at 95% confidence level, on the NIST LRE 2007 primary task, when fusing the HU PLLR i-vector system (c2) with baseline systems: (a) acoustic MFCC-SDC i-vector system, (b2) Phone-Lattice-SVM system and (a)+(b2) the fusion of the two latter.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_22"><head>Figure 4 . 1 :</head><label>41</label><figDesc>IPA charts</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_23"><head>Figure 4 . 2 :</head><label>42</label><figDesc>IPA chart for vowel phonemes of Hungarian</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_24"><head>Figure 4 . 3 :</head><label>43</label><figDesc>Cavg and 10×CLLR performance for the PLLR-based baseline system (with feature dimensionality=59) and systems trained on the set of PLLR features obtained after PCA projection into different dimensionalities, on the NIST 2007 LRE primary task.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_25"><head>Figure 5 . 1 :</head><label>51</label><figDesc>Distributions of (a) frame-level phone posteriors and (b) frame level phone log-likelihood ratios for the Hungarian phone a:.As exposed in Section 3.1, PLLRs seem to overcome the non-Gaussian nature of phone posteriors for each individual phone model.Figure 5.1 shows an example of the distribution of phone posteriors and PLLR features for the Hungarian phone a: (as defined in SAMPA 1 ), for a subset of NIST 2007 LRE data. Clearly, phone posteriors show a non-Gaussian distribution, whereas PLLR features are apparently Gaussian distributed.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_26"><head>Figure 5 .Figure 5 . 2 :Figure 5 . 3 :</head><label>55253</label><figDesc>2 shows two dimensional distributions of (a) phone posteriors, and (b) PLLRs, for three pairs of phones. These distributions are clearly bounded by a line.Figure 5.3 illustrates the three-dimensional distribution of (a) phone posteriors and (b) PLLRs, for the set of phones (a:, E, O). In this case, PLLRs appear to be bounded by a convex function. Phone posteriors range in [0, 1], but they must satisfy the second axiom of probability, N i=1 p i = 1. As a result, (and as exposed in Section 3.1) phone posteriors Distributions of (a) phone posteriors and (b) PLLRs, for three pairs of phones, a: vs E (red), i vs i: (black) and dz vs h (blue). Distributions of (a) phone posteriors and (b) PLLRs, for the set of phones (a:, E, O).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_27"><head>Figure 5 . 4 :</head><label>54</label><figDesc>(a) Standard 2-simplex defined by phone posteriors in the case of a phone decoder with 3 phonetic units. Graphs (b) and (c) show the hyper-surface where PLLRs lie for the case of a phone decoder with 3 phonetic units.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_28"><head>Figures 5 .</head><label>5</label><figDesc>4(b) and 5.4(c) show two partial views of the hyper-surface S for the case of a phone decoder with 3 phonetic units. The hyper-surface S is asymptotically perpendicular to the basis of PLLRs, which explains the bounded distributions shown in Figures 5.2(b) and 5.3(b). Let's prove it.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_29"><head>Figure 5 Figure 5 . 5 :</head><label>555</label><figDesc>Distribution of the PLLRs shown in Figures 2(b) and 3(b) after projecting them into the defined hyper-plane tangential to the surface at the point [1/N, 1/N, ..., 1/N].</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_30"><head>Figure 5 . 6 :</head><label>56</label><figDesc>Cavg × 100 performance for the Baseline (blue), Projected PLLR (red), Projected PLLR + PCA 58 (green) and Projected PLLR + PCA 13 + SD (purple) approaches.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_31"><head>Figure 6 .</head><label>6</label><figDesc>1 shows the comparison of phone usage/recognition for a set of 7 speakers uttering the same English sentence. The Figure shows usage differences among speakers in some phones, covering different kinds of sounds including sets of consonants and vowels. To compute the values represented in the Figure, the following procedure was used: Let p(i|t, s) be the phone posterior probability of a phone model i (1 ≤ i ≤ N ) at the frame t, for a sentence uttered by speaker s. First, phone posteriors of each phone were averaged: p(i|s) = 1 T (s) T (s) t=1 p(i|t, s) (6.10)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_32"><head>Figure 6 . 1 :</head><label>61</label><figDesc>Normalized average posteriorsp(i|s) (see Equation 6.11) of seven Hungarian phones on the same utterance (sx9) of the TIMIT dataset for 7 different speakers. The subset of phones represents fricative labiodental consonants (f and v) and a subset of vowels (e:, :2, 2, O, u), as defined in the International Phonetic Alphabet.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_33"><head>Figure 6</head><label>6</label><figDesc>Figure 6.1 reveals significant usage differences for some phones. When comparing the usage of consonants, results suggest that one speaker tends to use "f" over "v". Regarding the vowels, there seems to be a set of 2-3 speakers that overuse or prolongate the vowels "e:", ":2" and " 2" with regard to the rest of speakers, whereas the vowels "O" and "u" are almost equally used/pronounced by all of them. These could be intrinsic characteristics of the speakers and could therefore be used for identification.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_34"><head>Figure B. 1 Figure B. 1 :</head><label>11</label><figDesc>shows the DET curve of the fused system for the main evaluation conditions. As shown in theFigure,there was a noticeable calibration error in the condition tests involving microphone signals (conditions 1, 2 and 4). On the other hand, systems were well calibrated for test conditions related to telephone speech. DET curves of the EHU fused system for the NIST 2010 SRE core test conditions.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_35"><head>-</head><label></label><figDesc>Dot-Scoring (LE-GMM) system: Channel compensation was performed as outlined in Eigenchannel Compensation in Section 2.4. The scoring was computed in the following way:score(S, l) = log P (S|λ l ) P (S|λ ubm ) =m t l ·x S (B.2)where λ l and λ ubm are the target language model and the UBM model, respectively,x S are the channel compensated first order statistics of the target signal S, andm l are the centered and normalized channel compensated MAP-means of language l, computed according to Equation 2.23: m l = (τ I + diag(n l ))</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_36"><head>Figure B. 2 :</head><label>2</label><figDesc>DET curves of the primary systems submitted to MOBIO 2013 (images taken from<ref type="bibr" target="#b84">[84]</ref>)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" validated="false"><head></head><label></label><figDesc>Context . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1 1.1.1 Structure of Recognition Systems . . . . . . . . . . . . . . . . 2 1.1.2 Evolution . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 3 1.1.3 Applications . . . . . . . . . . . . . . . . . . . . . . . . . . . 5 1.2 Motivation and Objectives of the Work . . . . . . . . . . . . . . . . 6 1.3 Structure of the Manuscript . . . . . . . . . . . . . . . . . . . . . . . 8 Datasets . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 11 2.1.1 NIST LRE Benchmarks . . . . . . . . . . . . . . . . . . . . . 11 2.1.2 Albayzin LRE datasets . . . . . . . . . . . . . . . . . . . . . 13 2.1.3 RATS. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 14 2.2 Feature Extraction . . . . . . . . . . . . . . . . . . . . . . . . . .. . 14 2.2.1 Signal processing . . . . . . . . . . . . . . . . . . . . . . . . . 16 2.2.2 Voice Activity Detection . . . . . . . . . . . . . . . . . . . . . 17 2.2.3 Spectral and Acoustic low-level Features . . . . . . . . . . . . 18 Phonetic and Phonotactic high-level Features . . . . . . . . . 20 2.3 Modeling . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 23 2.4 Channel Compensation . . . . . . . . . . . . . . . . . . . . . . . . . 29 2.5 Scoring . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 34 2.6 Calibration and Fusion . . . . . . . . . . . . . . . . . . . . . . . . . . 34 2.6.1 Score Normalization . . . . . . . . . . . . . . . . . . . . . . . 34 2.6.2 Backend models . . . . . . . . . . . . . . . . . . . . . . . . . 36 2.6.3 Fusion . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 37 2.7 Evaluation Metrics . . . . . . . . . . . . . . . . . . . . . . . . . . . . 37 Definition of Phone Log-Likelihood Ratios (PLLR) . . . . . . . . . . 46 3.2 Configuration of a SLR System Based on PLLR Features . . . . . . 49 3.3 Search for the Optimal PLLR Feature Configuration . . . . . . . . . 49 3.3.1 Phone Log-Likelihoods vs PLLRs . . . . . . . . . . . . . . . . 50 3.3.2 Dynamic Coefficients . . . . . . . . . . . . . . . . . . . . . . . 50 3.3.3 Variability Compensation . . . . . . . . . . . . . . . . . . . . 51 3.4 Overall Performance of PLLR Based Systems . . . . . . . . . . . . . 52 3.4.1 Results on the NIST 2007 LRE dataset . . . . . . . . . . . . 53 3.4.2 Results on the NIST 2009 LRE dataset . . . . . . . . . . . Chapter Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . 61 Supervised and Unsupervised Dimensionality Reduction Techniques 64 4.1.1 Supervised Techniques . . . . . . . . . . . . . . . . . . . . . . 64 4.1.1.1 Results and Selection of the Optimal Supervised Technique . . . . . . . . . . . . . . . . . . . . . . . . . . 67 4.1.2 Unsupervised Techniques . . . . . . . . . . . . . . . . . . . . 68 4.1.2.1 Results and Selection of the Optimal Unsupervised Technique . . . . . . . . . . . . . . . . . . . . . . . . 69 4.1.3 Combination of Systems using Different Decoders . . . . . . PCA Dimensionality Optimization . . . . . . . . . . . . . . . . . . . 72 4.3 Shifted Delta PLLRs . . . . . . . . . . . . . . . . . . . . . . . . . . . 73 4.3.1 Shifted Delta Parameter Optimization . . . . . . . . . . . . . 73 4.3.2 Results on NIST 2011 LRE dataset . . . . . . . . . . . . . . . 75 4.4 Chapter Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . 75 Analysis of the PLLR Feature Space . . . . . . . . . . . . . . . . . . 77 5.2 Projection of the Features . . . . . . . . . . . . . . . . . . . . . . . 81 5.2.1 Feature Decorrelation . . . . . . . . . . . . . . . . . . . . Projected PLLRs in Noisy Environments . . . . . . . . . . . . . . . . 85 5.3.1 Baseline Systems . . . . . . . . . . . . . . . . . . . . . . . . . 86 5.3.2 Results on the RATS dataset . . . . . . . . . . . . . . . . . . 86 5.4 Shifted Delta Projected PLLRs . . . . . . . . . . . . . . . . . . . . Results on the NIST 2011 LRE dataset . . . . . . . . . . . .</figDesc><table>1 Introduction 
1 
1.1 2 State of the Art 
11 
2.1 xi 

xii 

Contents 

2.2.4 3 Phone Log-Likelihood Ratios 
45 
3.1 . 56 
3.4.3 Results on the NIST 2011 LRE dataset . . . . . . . . . . . . 57 
3.4.4 Results on the Albayzin 2010 LRE dataset . . . . . . . . . . 60 
3.5 4 Dimensionality Reduction on PLLRs 
63 
4.1 . 69 
4.1.3.1 Results on the NIST 2007 LRE dataset . . . . . . . 70 
4.1.3.2 Results on the NIST 2011 LRE dataset . . . . . . . 71 
4.2 Contents 

xiii 

5 PLLR Feature Projection 
77 
5.1 . . 82 
5.2.2 Results on the NIST 2007 LRE dataset . . . . . . . . . . . . 82 
5.2.3 Results on the NIST 2009 LRE dataset . . . . . . . . . . . . 84 
5.2.4 Results on the NIST 2011 LRE dataset . . . . . . . . . . . . 84 
5.3 . 88 
5.4.1 Results on the NIST 2007 LRE dataset . . . . . . . . . . . . 88 
5.4.2 </table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" validated="false"><head>4 . 1</head><label>.</label><figDesc>IPA chart for the consonant phonemes of Hungarian . . . . . . . . . 66 4.2 IPA chart for the consonant phonemes of Hungarian merged according to Family-MP criteria . . . . . . . . . . . . . . . . . . . . . . . . . . 67 4.3 IPA chart for the consonant phonemes of Hungarian merged according to Family-M criteria . . . . . . . . . . . . . . . . . . . . . . . . . . . 67 4.4 %C avg and C LLR performance for the PLLR i-vector system with different knowledge-based phone merging approaches, on the NIST 2007 LRE primary task. . . . . . . . . . . . . . . . . . . . . . . . . . 68 4.5 %C avg and C LLR performance for the PLLR i-vector system with different unsupervised dimensionality reduction approaches, on the NIST 2007 LRE primary task. . . . . . . . . . . . . . . . . . . . . . 70 4.6 %C avg and C LLR performance for PLLR i-vector baseline system, and systems using PLLR features reduced to the Family-MP set and projected with PCA, for each of the BUT decoders, and their fusion, on the NIST 2007 LRE primary task. . . . . . . . . . . . . . . . . . . . 70 4.7 %C avg , C LLR and C 24 avg ×100 performance for the PLLR i-vector baseline system, and systems using PLLR features reduced to the Family-MP set and projected with PCA, for each of the BUT decoders, and their fusion, on the NIST 2011 LRE primary task. . . . . . . . . . . 71 4.8 SLR performance of an i-vector system based on SD-PLLR features using different N values on the NIST 2007 LRE 30s test set. . . . . . 74 4.9 SLR performance of an i-vector system based on SD-PLLR features, using different P values, on the NIST 2007 LRE 30s test set. . . . . 74 4.10 SLR performance of an i-vector system based on SD-PLLR features, using different d values, on the NIST 2007 LRE 30s test set. . . . . . 74 4.11 SLR performance of i-vector systems based on PLLR and SD-PLLR features, on the NIST 2011 LRE 30s test set. . . . . . . . . . . . . . 75 5.1 %C avg and C LLR performance (and relative improvements) for the PLLR i-vector baseline system, and systems using projected PLLR features on the NIST 2007 LRE primary evaluation task. . . . . . . 83 5.2 %C avg and C LLR performance for the PLLR i-vector baseline systems, systems using PLLR projected features, acoustic MFCC and phonotactic systems and fusions of them on the NIST 2009 LRE primary evaluation tasks. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 84 5.3 %C avg and C LLR performance for the PLLR i-vector baseline systems, systems using PLLR projected features, acoustic MFCC and phonotactic systems and fusions of them on the NIST 2011 LRE primary evaluation tasks. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 85 5.4 %C avg performance for the PLP2, SnGM and PLLR systems with logistic regression classifiers on the RATS evaluation set for the 120s, 30s, 10s and 3s signals. . . . . . . . . . . . . . . . . . . . . . . . . . . 87 5.5 %C avg performance for the PLP2, SnGM and PLLR systems with neural network classifiers on the RATS evaluation set for the 120s, 30s, 10s and 3s signals. . . . . . . . . . . . . . . . . . . . . . . . . . . 87 5.6 Cavg × 100 and CLLR performance for the Projected PLLR + PCA, Projected PLLR + PCA reduced and Projected PLLR + PCA reduced + SD approaches on the NIST 2007 LRE primary evaluation tasks. . . . . . . . 88 5.7 Cavg × 100, CLLR and %C 24 avg performance for the Projected PLLR + PCA, and Projected PLLR + PCA reduced + SD approaches for 13, 15 and 17 dimensionalities on the NIST 2011 LRE primary evaluation tasks. . . . . 90 6.1 MinDCF performance of systems using only PLLR features and PLLR features augmented with dynamic coefficients on the NIST 2010 SRE core conditions. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 101 6.2 MinDCF performance of systems using PLLR features under different configurations on the NIST 2010 SRE core conditions. . . . . . . . . 102 6.3 MinDCF performance of systems using PLLR and projected PLLR features on the NIST 2010 SRE core conditions. . . . . . . . . . . . . 102 6.4 Results of i-vector /PLDA SR systems based on MFCC and PLLR features, and the fusion of them, on the NIST 2010 SRE core conditions.104 6.5 Results of i-vector /PLDA SR systems based on MFCC and PLLR features, and the fusion of them, on the NIST 2012 SRE core conditions 2 and 5. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 104 target languages. . . . . . . . . . . . . . . . . . . . . . . . . . . . 112 A.2 2009 NIST LRE core condition: training data (hours), development and evaluation data (# 30s segments), disaggregated for target and non-target languages. . . . . . . . . . . . . . . . . . . . . . . . . . . . 113 A.3 NIST 2011 LRE core condition: training data (hours) disaggregated for target and non-target languages. . . . . . . . . . . . . . . . . . . . . . . 114 A.4 NIST 2011 LRE core condition: development and evaluation data (30s segments), disaggregated for target and non-target languages. . . . . . . 115 A.5 Albayzin 2010 LRE: Distribution of training data (hours) and development and evaluation data (30s segments). . . . . . . . . . . . . . . 117 A.6 Number of speakers uniquely included in each dataset (diagonal) and shared with other datasets (outside the diagonal). . . . . . . . . . . 117 A.7 NIST 2004, 2005 and 2006 SRE signal distribution. . . . . . . . . . 118 A.8 NIST 2008 SRE signal distribution. . . . . . . . . . . . . . . . . . . 118 A.9 NIST 2008 Follow Up signal distribution. . . . . . . . . . . . . . . . 118 A.10 NIST 2012 SRE iVector and PLDA training signal distribution. . . 119 B.1 Backend and fusion configuration for the EHU systems submitted to the NIST 2011 LRE. . . . . . . . . . . . . . . . . . . . . . . . . . . . 126 B.2 Performance (in terms of C avg ) of the phonotactic and acoustic subsystems and partial and complete fusions on the NIST 2011 LRE 30s test set. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 127 B.3 Official NIST 2011 LRE results for the EHU systems. . . . . . . . . 128 B.4 Results of EHU i-vector-PLDA systems based on MFCC and PLLR features, and the fusion of them, on the NIST 2012 SRE core conditions.131 B.5 Official MOBIO 2013 results for several primary systems. . . . . . . 133 este módulo toma como entrada las características de cada señal de voz y obtiene una serie de puntuaciones, indicando su similitud con cada modelo objetivo. Las puntuaciones deben ser ajustadas en el módulo de calibrado para ecualizarlas de tal modo que puedan enfrentarse a un mismo umbral sobre el que tomar las decisiones finales.La mayor dificultad de las tareas de reconocimiento de la lengua y del locutor reside en extraer de la señal de voz información relevante para el reconocimiento, descartando la relativa a otras fuentes de variabilidad, reunidas generalmente bajo la denominación de variabilidades de canal. Estos cambios o variaciones de la señal pueden ser originados por el aparato de grabación o el canal de transmisión, ruido ambiente, estados deánimo o condiciones físicas del hablante, variabilidad relacionada con el locutor en el caso de la lengua, o de la lengua en el caso del locutor, Esta tesis se centra en la extracción de unas características con información fonética a nivel de trama, con el objetivo de combinar aportes de ambos tipos de aproximaciones en las propias características. El trabajo desarrollado abarca la optimización e integración de las características en sistemas de reconocimiento de la lengua y del locutor.Las características propuestas, denominadas "relaciones de log probabilidades de fonemas" (PLLRs, de sus siglas en inglés), son obtenidas a partir de probabilidades fonéticas. Dada una señal de audio y un decodificador fonético con un inventario de N fonemas, se supone que el decodificador proporciona la probabilidad acústica p para cada una de las unidades fonéticas i (1 ≤ i ≤ N ), en cada ventana (trama) correspondiente al instante de tiempo t, p(i|t). El vector N-dimensional obtenido es aquel que, según los parámetros del decodificador, mejor describe el contenido espectral de la ventana de análisis. Geométricamente, este vector puede verse como un punto dentro del simplex N-1 estándar, donde cada vértice representa unidades fonéticas puras y los bordes mezclas de las unidades fonéticas que unen.</figDesc><table>A.1 2007 NIST LRE core condition: training data (hours), development 
and evaluation data (# 30s segments), disaggregated for target and 
non-TECHINICAL TERMS 

ActDCF 
Actual Detection Cost Function 

Cavg 
Average Cost 

C LLR 
Log-Likelihood Ratio Cost 

CMS 
Cepstral Mean Subtraction 

CTS 
Conversational Telephone Speech 

DCF 
Detection Cost Function 

DCT 
Discrete Cosine Transform 

DET 
Detection Error Tradeoff 

DNN 
Deep Neural Networks 

DFT 
Discrete Fourier Transform 

EER 
Equal Error Rate 

EM 
Expectation Maximization 

Fact 
Acutal Relative Confusion 

FFT 
Fast Fourier Transform 

GMM 
Gaussian Mixture Model 

HMM 
Hidden Marcov Model 

HTK 
Hidden Markov Model ToolKit 

IPA 
International Phonetic Alphabet 

JFA 
Joint Factor Analysis 

LE-GMM 
Linearized Eigenchannel GMM 

LID 
Language IDentification 

LLR 
Log-Likelihood Ratio 

LR 
Logistic Regression 

LRE 
Language Recognition Evaluation 

MAP 
Maximum A Posteriori 

MD 
Minumum Divergence 

MinDCF 
Minimum Detection Cost Function 

ML 
Maximum Likelihood 

MFCC 
Mel Frequency Cepstral 

Coefficients 

MMI 
Maximum Mutual Information 

NN 
Neural Network 

OOS 
Out Of Set 

PCA 
Principal Component Analysis 

PLLR 
Phone Log-Likelihood Ratios 

PLDA 
Probabilistic Linear 

Discriminant Analysis 

PLP 
Perceptual Linear Prediction 

PPRLM 
Parallel Phone Recognition 

Language Modeling 

PRLM 
Phone Recognition Language 

Modeling 

RASTA 
RelAtive SpecTrA 

RATS 
Robust Automatic Transcription 

of Speech 

SD 
Shifted Delta 

SDC 
Shifted Delta Cepstrum 

SLR 
Spoken Language Recognition 

SNR 
Signal Noise Ratio 

SR 
Speaker Recognition 

SRE 
Speaker Recognition Evaluation 

SVM 
Support Vector Machine 

TRAP 
Temporal Patterns 

UBM 
Universal Background Model 

VAD 
Voice Activity Detector 

VOA 
Voice Of America 

xxi 

xxii 

Abbreviations 

INSTITUTIONS 

BUT 
Brno University of Technology 

EHU 
Euskal Herriko Univertsitatea 

University of the Basque Country 

GTTS 
Grupo de Trabajo en Tecnologías 

Software 

Software Techonologies Working 

Group 

ISCA 
International Speech 

Communication Association 

JHU 
John Hopkins Univertisy 

L2F 
Laboratorio de sistemas de Língua 

Falada 

Laboratory of Spoken Language 

systems 

LDC 
Linguistic Data Consortium 

NIST 
National Institute of 

Standard Technologies 

RTTH 
Red Temática en Tecnologías del 

Habla 

Spanish Thematic Network on 

Speech Technology 

SIG IL 
Special Interest Group on Iberian 

Languages 

A los que entienden ablakistriki 
y chuchurriña. 

Sinopsis 

El reconocimiento de la lengua y el reconocimiento de locutor son tareas de re-

conocimiento de patrones que consisten en determinar, mediante métodos computa-

cionales, la lengua hablada y la identidad del hablante en una señal de voz, respec-

tivamente. 

Los sistemas de reconocimiento tienen una estructura general que consta de diferen-

tes módulos. En primer lugar se encuentra el módulo de extracción de características, 

que toma como entrada una señal de audio y obtiene un conjunto de características 

que recogen información, entre otras cosas, sobre la lengua y sobre el hablante. A 

continuación se encuentra el clasificador. Entrenado durante la fase de modelado, 

etc. 

Los estudios realizados en esta tesis se han enfocado principalmente a la tarea de 

reconocimiento de la lengua y han sido extendidos posteriormente al reconocimiento 

del locutor. 

Las diversas aproximaciones desarrolladas para el reconocimiento de la lengua, se 

basan en diferentes tipos de características. En términos generales, estas carac-

terísticas abarcan propiedades espectrales, fonéticas, acústicas, temporales o 

fonotácticas de la señal, prosodia u otras características deíndole más compleja 

como la construcción de palabras y frases. La mayoría de los sistemas hacen uso 

xxv 

de dos conjuntos principales de características: acústico-espectrales y fonéticas. Las 

características acústico-espectrales (también denominadas de bajo nivel ) extraen la 

información analizando el espectro de la señal mediante ventanas de unos 20-30 

milisegundos, intentando reproducir los mecanismos de la percepción humana de los 

sonidos. Las características fonéticas (o de alto nivel ) extraen información relativa a 

los fonemas contenidos en la señal. Dado que cada lengua tiene su propio inventario 

fonético, es fácil intuir cómo la identificación de los fonemas puede ayudar en las 

tareas de reconocimiento. En muchos casos los sistemas finales de reconocimiento 

de la lengua son combinaciones de diversos sistemas (basados en ambos tipos de 

características) fusionados en la parte final del proceso, conocida como calibrado, en 

la que se transforman y combinan las puntuaciones. 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" validated="false"><head></head><label></label><figDesc>Los PLLRs, al ser características obtenidas a nivel de trama, son fácilmente integrables en sistemas del estado del arte basados en otro tipo de características espectrales. En nuestros estudios se han utilizado bajo la aproximación conocida como Total Variability Factor Analysis o i-vector, una de las técnicas que mejores resultados proporcionan, y por lo tanto una de las técnicas más utilizadas.Una vez encontrada la configuraciónóptima, haciendo uso de coeficientes dinámicos de primer orden, los resultados obtenidos por los sistemas basados en las características PLLR son contrastados con aquellos obtenidos por sistemas acústicos basados en MFCCs, que aplican el mismo modelado, y con sistemas fonotácticos, que utilizan los mismos decodificadores como base para la extracción de sus características. Se analizan los resultados en cuatro bases de datos diferentes: las treś ultimas bases de datos proporcionadas por el National Institute of Standards Tech-LRE), de gran relevancia en el campo de investigación de reconocimiento de la lengua; y la base de datos proporcionada por GTTS para Albayzin 2010 LRE, en la que se analizan los resultados en entornos y tipos de señal de diferente naturaleza.Los resultados obtenidos son consistentes en todas las bases de datos. Los sistemas basados en PLLRs obtienen en la mayoría de los casos tasas de error inferiores a las de los sistemas del estado del arte, lo que pone de manifiesto la utilidad de estas características.Por otro lado, también se procede a la fusión de sistemas. Se fusionan las puntuaciones del sistema basado en PLLRs con las del sistema acústico, revelando una alta complementariedad entre ambos sistemas. La fusión del sistema basado en características PLLR con sistemas fonotácticos evidencia también una alta complementariedad. Porúltimo, la fusión de los tres sistemas proporciona mejoras con respecto a cualquier fusión de sistemas por pares. Este resultado es muy significativo, ya que muestra que la información aportada por los PLLRs puede utilizarse para mejorar el rendimiento total de sistemas del estado del arte, cualesquiera que sean las características utilizadas.El siguiente estudio se centra en la reducción de dimensionalidad de los PLLRs, debido a queésta es relativamente grande en comparación con otros vectores de características espectrales. La reducción podría facilitar la aplicación de otras técnicas en la extracción de PLLRs. Se analizan varias aproximaciones supervisadas, basadas en el conocimiento de la naturaleza fonética de la lengua usada en el decodificador. mediante técnicas supervisadas, sin degradar significativamente el rendimiento del sistema. Por otro lado, la aplicación de PCA revela que el vector puede ser reducido mejorando además los resultados obtenidos por el sistema.Una vez encontrada la formaóptima de reducir el vector de características, se procede a estudiar la mínima dimensionalidad que se puede alcanzar mediante PCA (manteniendo un compromiso con los resultados obtenidos) con el objetivo de calcular secuencias de derivadas (Shifted Delta, en inglés) sobre los PLLRs, ya que su uso es común sobre características espectrales (MFCCs) en sistemas de reconocimiento de la lengua. Definida la mínima dimensionalidad, se procede al barrido de parámetros de extracción de las secuencias de derivadas. Finalmente, la aplicación de la combinaciónóptima revela que su uso es acertado también sobre los PLLRs, ya que la información que aportan mejora significativamente los resultados obtenidos por el sistema.El descubrimiento de que la reducción mediante PCA proporciona mejoras en el sistema, ha derivado posteriormente en el análisis del espacio de los PLLRs, con el fin de entender posibles orígenes de este resultado. El estudio revela que los PLLRs están confinados en un sub-espacio limitado por un hiperplano que es asintóticamente perpendicular a los ejes de los PLLRs. Este efecto limitaría presuntamente el movimiento de los PLLRs respecto a los ejes y por tanto, la información que proporcionan. Se diseña un método de proyección que elimine el efecto. La proyección de los PLLRs potencia el rendimiento del sistema de forma significativa, resultados que son mejorados aún más si la proyección se combina con la transformación PCA, debido a la decorrelación de parámetros que realiza estaúltima, que optimiza la distribución de las características para el tipo de modelado utilizado.A continuación, se procede a combinar la proyección de las características con la aplicación de las secuencias de derivadas. Los resultados obtenidos en las bases de datos NIST 2007 y 2011 LRE muestran que la combinación de ambas técnicas es acertada.Finalmente, se estudia el uso de los PLLRs en sistemas de reconocimiento de locutor. Se procede, en primer lugar, a la optimización de parámetros de extracción de PLLRs para estos sistemas. Una vez encontrada la combinaciónóptima, haciendo uso también de coeficientes dinámicos de primer orden, se comprueban los resultados de los sistemas utilizando i-vectors junto con Probabilistic Linear Discriminant Analysis (PLDA). Para ello se emplean las dosúltimas bases de datos proporcionadas</figDesc><table>Los primeros análisis se centran en la optimización de los parámetros de extracción de 

las características PLLR. Se explora el uso de formulaciones próximas a la definición 

presentada, así como la integración de información con un mayor contexto temporal 

alrededor de la ventana, al igual que se hace con otras características espectrales 

como los Mel Frequency Cepstral Coefficients (MFCC). Por otro lado, se estudia 

la aplicación de diferentes técnicas de compensación de variabilidad a nivel de ex-

tracción de características, y el uso de diversos decodificadores fonéticos para el 

cómputo de los PLLRs. 

nology (NIST), para las NIST 2007, 2009 y 2011 Language Recognition Evaluations 

(Por otro lado, se estudian también técnicas no supervisadas, que reducen el tamaño 

del vector basándose en correlaciones entre fonemas o en la frecuencia de fonemas, 

así como la técnica conocida como Principal Component Analysis (PCA), que realiza 

una transformación ortogonal en el espacio de los PLLRs. Se concluye que el vector 

de características puede ser reducido a prácticamente un tercio de su dimensionalidad 

(alcanzando una dimensionalidad comparable con otras aproximaciones espectrales) 

Posteriormente, fruto de una estancia con el grupo Speech@FIT de la universidad 

de Brno, se analiza el rendimiento del sistema basado en PLLRs proyectados, bajo 

aproximaciones basadas también en i-vectors, pero con modelado final utilizando 

regresión logística y redes neuronales, y sobre la base de datos Robust Automatic 

Transcription of Speech (RATS). Los resultados obtenidos con señales ruidosas y de 

duraciones más cortas revelan, una vez más, la alta competitividad de los sistemas 

basados en PLLRs. 

Introduction 

1.1 Context 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" validated="false"><head></head><label></label><figDesc>Decision maker. Provides a final decision (hard decision: accept/reject) for each target model on top of the transformed scores.</figDesc><table>Feature 
extractor 

Model 
estimator 

Backend param. 
estimator 

Training data 
Target 
models 

Backend 
param. 

Development 
data 

Backend 

Decision 
maker 

SPEECH 

Hypothesis 

Classifier 

(3) Test/use 

(2) Development 

(1) Training 

labels 

labels 

features 
scores 

adjusted 
scores 

Test data 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" validated="false"><head></head><label></label><figDesc>The KALAKA database was built for the Albayzin 2008 Language Recognition Evaluation 1 , following the general approach of NIST LRE benchmarks, with some distinctions: while NIST LRE signals consisted of spontaneous conversations collected over telephone (narrow-band) channels, KALAKA datasets consisted of signals extracted from (wide-band) TV shows, including both planned and spontaneous speech in diverse environment conditions involving a varying number of speakers. The target language set focused on the 4 official languages spoken in Spain. The KALAKA2 and KALAKA3 databases, constructed for Albayzin 2010 and 2012 LREs, modified the scope of the evaluations. In the case of Albayzin 2010, an evaluation track was included to evaluate recognition of signals recorded in noisy environments, and included two more target languages. Albayzin 2012 pursued more ambitious challenges, as KALAKA3 consisted of audio data extracted from YouTube videos, and added evaluation tracks containing target languages for which no training materials were provided, increasing the amount of target languages up to ten.The RATS dataset was designed to perform LRE and SRE in challenging scenarios, focusing on noisy environments. RATS provides data retransmitted through 8 different communication channels covering target languages. The utterances consist of selected signals from Callfriend and Fisher collections, previous NIST datasets and new conversational telephone speech.</figDesc><table></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11" validated="false"><head>Table 3 .</head><label>3</label><figDesc></figDesc><table>1: Cavg × 100 and CLLR performance for i-vector systems using phone 
log-posteriors (PL) features and PLLRs computed with the HU BUT decoder, 
on the NIST 2007 LRE primary evaluation task. 

System 

Cavg × 100 

C LLR 

PL 
4.41 
0.604 
PLLR 
3.45 
0.564 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_12" validated="false"><head>Table 3 . 2 :</head><label>32</label><figDesc>Cavg × 100 and CLLR performance for i-vector systems using PLLR, PLLR+∆ and PLLR+∆+∆∆ features computed with the HU BUT decoder, on the NIST 2007 LRE primary evaluation task.</figDesc><table>System 

Cavg × 100 

C LLR 

PLLR 
3.45 
0.564 
PLLR+∆ 
2.66 
0.382 
PLLR+∆+∆∆ 
3.60 
0.506 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_13" validated="false"><head>Table 3 .</head><label>3</label><figDesc></figDesc><table>3: Cavg × 100 and CLLR performance for i-vector systems using PLLR 
features computed with the BUT HU decoder, with: (a) no noise reduction 
technique, (b) Feature Normalization (FN), (c) Feature Warping (FW) and (d) 
RASTA, on the NIST 2007 LRE primary evaluation task. 

System 

Cavg × 100 

C LLR 

PLLR 
2.66 
0.382 
PLLR+FN 
2.95 
0.436 
PLLR+FW 
3.21 
0.435 
PLLR+RASTA 
8.67 
1.149 

Figures show that these techniques provide no gain with regard to the basic PLLR 
feature-based system, thus it was decided not to apply any variability compensation 
technique on top of the PLLRs at the feature extraction stage. 
</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_14" validated="false"><head></head><label></label><figDesc>). For the NIST 2007 and 2009 LRE datasets, a ZT-norm and a discriminative Gaussian backend were applied to scores. For the NIST 2011 LRE dataset a generative Gaussian backend was applied. Raw system scores were used for he Albayzin 2010 dataset.</figDesc><table>In all cases, backend parameters were estimated and applied by means of the FoCal 
toolkit [20] [22] [17] </table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_15" validated="true"><head>Table 3 . 4 :</head><label>34</label><figDesc>Cavg × 100 and CLLR performance for the MFCC-SDC i-vector baseline system, i-vector systems using PLLR features, phonotactic baseline systems and the fusion of them, for each of the BUT decoders, on the NIST 2007 LRE primary evaluation task.</figDesc><table>System 

Cavg × 100 

C LLR 

MFCC-SDC i-vector 
(a) 
2.85 
0.407 

Phonotactic 
(b1) 
2.94 
0.440 
CZ 
PLLR i-vector 
(c1) 
4.18 
0.550 
(a)+(b1) 
1.22 
0.189 
(a)+(c1) 
1.95 
0.280 
(b1)+(c1) 
1.79 
0.257 
Fusion 

(a)+(b1)+(c1) 
1.24 
0.176 

Phonotactic 
(b2) 
2.08 
0.310 
HU 
PLLR i-vector 
(c2) 
2.66 
0.382 
(a)+(b2) 
1.08 
0.152 
(a)+(c2) 
1.40 
0.215 
(b2)+(c2) 
1.20 
0.166 
Fusion 

(a)+(b2)+(c2) 
0.82 
0.124 

Phonotactic 
(b3) 
2.69 
0.383 
RU 
PLLR i-vector 
(c3) 
4.08 
0.549 
(a)+(b3) 
1.13 
0.182 
(a)+(c3) 
1.72 
0.265 
(b3)+(c3) 
1.76 
0.240 
Fusion 

(a)+(b3)+(c3) 
1.10 
0.163 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_16" validated="false"><head>Table 3 .</head><label>3</label><figDesc></figDesc><table>5: Cavg × 100 and CLLR performance for the baseline phonotactic and 
i-vector systems, the PLLR i-vector system and the fusion of them, on the NIST 
2009 LRE primary evaluation task. 

System 

Cavg × 100 

C LLR 
MFCC-SDC i-vector 
(a) 
2.70 
0.535 

Phonotactic 
(b1) 
2.98 
0.578 
CZ 
PLLR i-vector 
(c1) 
3.18 
0.592 
(a)+(b1) 
1.82 
0.365 
(a)+(c1) 
1.99 
0.416 
(b1)+(c1) 
1.95 
0.399 
Fusion 

(a)+(b1)+(c1) 
1.61 
0.388 

Phonotactic 
(b2) 
2.49 
0.502 
HU 
PLLR i-vector 
(c2) 
2.42 
0.505 
(a)+(b2) 
1.67 
0.346 
(a)+(c2) 
1.79 
0.392 
(b2)+(c2) 
1.69 
0.357 
Fusion 

(a)+(b2)+(c2) 
1.48 
0.321 

Phonotactic 
(b3) 
2.24 
0.457 
RU 
PLLR i-vector 
(c3) 
2.74 
0.548 
(a)+(b3) 
1.53 
0.323 
(a)+(c3) 
1.92 
0.404 
(b3)+(c3) 
1.65 
0.344 
Fusion 

(a)+(b3)+(c3) 
1.47 
0.307 

Phonotactics (b1+b2+b3) 
1.66 
0.343 
PLLRs (c1+c2+c3) 
1.89 
0.402 
(a)+(b1+b2+b3) 
1.43 
0.295 
(a)+(c1+c2+c3) 
1.69 
0.361 

Fusion 

ALL 
1.28 
0.282 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_17" validated="false"><head>Table 3 .</head><label>3</label><figDesc></figDesc><table>6: Cavg × 100 and CLLR performance for the i-vector baseline system 
using acoustic features (MFCC-SDC), i-vector systems with PLLR+∆ features, 
phonotactic baseline systems and the fusion of them, for each of the BUT de-
coders, on the NIST 2011 LRE primary evaluation task. 

System 

Cavg × 100 

C LLR 

MFCC-SDC i-vector 
(a) 
5.96 
1.088 

Phonotactic 
(b1) 
7.98 
1.376 
CZ 
PLLR+∆ i-vector 
(c1) 
5.31 
0.978 
(a)+(b1) 
4.34 
0.816 
(a)+(c1) 
4.01 
0.770 
(b1)+(c1) 
4.36 
0.822 
Fusion 

(a)+(b1)+(c1) 
3.64 
0.691 

Phonotactic 
(b2) 
7.15 
1.280 
HU 
PLLR+∆ i-vector 
(c2) 
5.18 
0.982 
(a)+(b2) 
4.34 
0.823 
(a)+(c2) 
4.00 
0.789 
(b2)+(c2) 
4.39 
0.829 
Fusions 

(a)+(b2)+(c2) 
3.63 
0.714 

Phonotactic 
(b3) 
6.85 
1.212 
RU 
PLLR+∆ i-vector (c3) 
4.70 
0.898 
(a)+(b3) 
4.25 
0.780 
(a)+(c3) 
3.77 
0.734 
(b3)+(c3) 
3.91 
0.740 
Fusions 

(a)+(b3)+(c3) 
3.37 
0.647 

Phonotactics (b1)+(b2)+(b3) 
4.57 
0.844 
PLLRs (c1)+(c2)+(c3) 
3.79 
0.720 
(b1+b2+b3)+(c1+c2+c3) 
3.13 
0.607 
(a)+(b1+b2+b3) 
3.58 
0.674 
(a)+(c1+c2+c3) 
3.39 
0.663 

Fusions 

ALL 
3.01 
0.578 
</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_18" validated="true"><head>Table 3 . 8 :</head><label>38</label><figDesc>Cavg × 100 and CLLR performance for the baseline systems, and the PLLR i-vector systems using phone decoders for CZ, HU and RU on the Albayzin 2010 LRE primary task on clean and noisy speech.</figDesc><table>Clean 
Noisy 
System 

Cavg × 100 

C LLR Cavg × 100 C LLR 
MFCC-SDC i-vector 
2.12 
0.176 
3.95 
0.325 
Phonotactic 
2.15 
0.215 
7.00 
0.664 
CZ PLLR i-vector 
2.33 
0.223 
6.66 
0.546 
Phonotactic 
2.35 
0.218 
7.28 
0.621 
HU PLLR i-vector 
1.41 
0.127 
3.17 
0.308 
Phonotactic 
2.85 
0.244 
6.54 
0.571 
RU PLLR i-vector 
2.34 
0.225 
4.38 
0.352 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_19" validated="true"><head>Table 3 . 9 :</head><label>39</label><figDesc>Cavg ×100 and CLLR performance for the baseline systems, the PLLR i-vector system and different fusions on the Albayzin 2010 LRE primary task on clean and noisy speech.</figDesc><table>Clean 
Noisy 
System 

Cavg × 100 

C LLR Cavg × 100 C LLR 
MFCC-SDC i-vector 
(a) 
2.12 
0.176 
3.95 
0.325 
Phonotactic 
(b) 
2.35 
0.218 
7.28 
0.621 
HU 
PLLR i-vector (c) 
1.41 
0.127 
3.17 
0.308 
(a)+(b) 
1.10 
0.106 
2.43 
0.211 
(a)+(c) 
1.20 
0.109 
2.65 
0.227 
(b)+(c) 
1.09 
0.092 
2.65 
0.228 
Fusion 

(a)+(b)+(c) 
0.97 
0.086 
1.86 
0.168 

Fusion ALL (7 systems, Table 3.8) 
0.82 
0.075 
1.74 
0.169 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_22" validated="true"><head>Table 4 . 1 :</head><label>41</label><figDesc>IPA chart for the consonant phonemes of Hungarian</figDesc><table>Bilabial Lab. dent. Dental Alveolar P-alveo. Palatal 
Velar Glottal 

Plosive 
p b 
t d 
c é 
k g 
Nasal 
m 
n 
ñ 
Trill 
r 
Fricative 
f v 
s z 
S Z 
h 
Approximant 
j 
Lat. approximant 
l 

Affricate 
ts dz 
tS dZ 
cç éJ* 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_23" validated="true"><head>Table 4 . 2 :</head><label>42</label><figDesc>IPA chart for the consonant phonemes of Hungarian merged according to Family-MP criteria</figDesc><table>Bilabial Lab. dent. Dental Alveolar 
P-alveo. 
Palatal 
Velar Glottal 

Plosive 
p b 
t d 
c é 
k g 
Nasal 
m 
n 
ñ 
Trill 
r 
Fricative 
f v 
s z 
S Z 
h 
Approximant 
j 
Lat. approximant 
l 

Affricate 
ts dz 
tS dZ 
cç éJ* 

to the same regions in the IPA charts were also merged, attaining 14 phone 
classes. Table 4.3 shows by different colorings the clusters defined for consonant 
phonemes in this approach. 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_24" validated="true"><head>Table 4 . 3 :</head><label>43</label><figDesc>IPA chart for the consonant phonemes of Hungarian merged according to Family-M criteria</figDesc><table>Bilabial Lab. dent. Dental Alveolar 
P-alveo. 
Palatal 
Velar Glottal 

Plosive 
p b 
t d 
c é 
k g 
Nasal 
m 
n 
ñ 
Trill 
r 
Fricative 
f v 
s z 
S Z 
h 
Approximant 
j 
Lat. approximant 
l 

Affricate 
ts dz 
tS dZ 
cç éJ* 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_25" validated="false"><head>Table 4 .</head><label>4</label><figDesc></figDesc><table>4 shows performance of the different supervised dimensionality reduction 
techniques, compared to the baseline system (based on the PLLR+∆ features pre-
sented on Chapter 3 2 ). 

The system trained on the baseline features (with a PLLR feature vector of size 59 
plus Deltas) attains 2.86 C avg . When using the 33 dimensional Family-R feature 
set (decreasing the feature vector size to almost a half), performance is just slightly 
degraded, obtaining 3.07 C avg . The system trained on the Family-SL feature set 
</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_26" validated="true"><head>Table 4 .</head><label>4</label><figDesc>4: %Cavg and CLLR performance for the PLLR i-vector system with different knowledge-based phone merging approaches, on the NIST 2007 LRE primary task.</figDesc><table>HU PLLR System 
Dim 

%Cavg 

C LLR 

Baseline 
59+∆ 2.86 0.389 
R 
33+∆ 3.07 
0.422 
SL 31+∆ 3.46 
0.467 
MP 23+∆ 2.98 0.426 
Supervised 
Merge 
Phones 
Family 

M 
14+∆ 4.22 
0.580 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_27" validated="false"><head>Table 4 .</head><label>4</label><figDesc>5: %Cavg and CLLR performance for the PLLR i-vector system with different unsupervised dimensionality reduction approaches, on the NIST 2007 LRE primary task.Results of the baseline and best dimensionality reduction approaches using multiple decoders on the NIST 2007 LRE are shown inTable 4.6.</figDesc><table>HU PLLR System 
Dim 

%Cavg 

C LLR 

Baseline 
59+∆ 2.86 0.389 

Supervised 
Merge 
Phones 
Family MP 23+∆ 2.98 0.426 

Merge 
Phones 
Correlation 
23+∆ 3.76 
0.523 

Select 
Phones 
Frequency 
23+∆ 3.56 
0.480 
Unsupervised 

PLLR 
Projection 
PCA 
23+∆ 2.45 0.333 

4.1.3.1 Results on the NIST 2007 LRE dataset 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_28" validated="false"><head>Table 4 .</head><label>4</label><figDesc></figDesc><table>6: %Cavg and CLLR performance for PLLR i-vector baseline system, 
and systems using PLLR features reduced to the Family-MP set and projected 
with PCA, for each of the BUT decoders, and their fusion, on the NIST 2007 
LRE primary task. 

PLLR System 

%Cavg 

C LLR 

CZ (43+∆) 
4.18 
0.550 
HU (59+∆) 
2.66 
0.382 
RU (50+∆) 
4.08 
0.549 
Baseline 

CZ+HU+RU 2.09 0.299 

CZ (25+∆) 
4.55 
0.619 
HU (23+∆) 
3.08 
0.424 
RU (21+∆) 
4.30 
0.598 
Family-MP 

CZ+HU+RU 2.24 0.313 

CZ (25+∆) 
3.12 
0.432 
HU (23+∆) 
2.17 
0.320 
RU (21+∆) 
3.29 
0.451 
PCA 

CZ+HU+RU 1.79 0.240 
</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_29" validated="true"><head>Table 4 .</head><label>4</label><figDesc>7: %Cavg, CLLR and C 24 avg × 100 performance for the PLLR i-vector baseline system, and systems using PLLR features reduced to the Family-MP set and projected with PCA, for each of the BUT decoders, and their fusion, on the NIST 2011 LRE primary task.</figDesc><table>PLLR System 

%Cavg 

C LLR %C 24 

avg 

CZ (43+∆) 
5.31 
0.978 12.46 
HU (59+∆) 
5.18 
0.982 12.12 
RU (50+∆) 
4.70 
0.898 11.27 
Baseline 

CZ+HU+RU 3.79 0.720 9.10 

CZ (25+∆) 
5.53 
1.054 13.62 
HU (23+∆) 
5.40 
1.015 12.64 
RU (21+∆) 
5.13 
0.961 11.57 
Family-MP 

CZ+HU+RU 3.82 0.693 9.79 

CZ (25+∆) 
4.46 
0.855 11.20 
HU (23+∆) 
4.48 
0.877 10.88 
RU (21+∆) 
4.20 
0.803 11.01 
PCA 

CZ+HU+RU 3.21 0.634 8.45 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_31" validated="true"><head>Table 4 . 8 :</head><label>48</label><figDesc>SLR performance of an i-vector system based on SD-PLLR features using different N values on the NIST 2007 LRE 30s test set.</figDesc><table>PCA Dim 
C avg C LLR 

PLLR+∆ 
2.59 0.370 
13 
SD-PLLR 13-2-3-7 1.71 0.260 
PLLR+∆ 
2.23 0.330 
15 
SD-PLLR 15-2-3-7 1.94 0.264 
PLLR+∆ 
2.29 0.332 
17 
SD-PLLR 17-2-3-7 1.73 0.241 

the optimal value found to optimize SD-PLLR system performance with regard to 
N , reaching 1.71 C avg . 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_32" validated="true"><head>Table 4 . 9 :</head><label>49</label><figDesc>SLR performance of an i-vector system based on SD-PLLR features, using different P values, on the NIST 2007 LRE 30s test set.</figDesc><table>SD-PLLR configuration 
C avg C LLR 

13-2-1-7 
2.39 0.346 
13-2-2-7 
1.91 0.279 
13-2-3-7 
1.71 0.260 
13-2-4-7 
2.02 0.297 
13-2-5-7 
2.46 0.347 

Results for systems trained using different values for the shift, parameter P , are 
shown in Table 4.9. Figures show a high sensitivity with regard to this parameter, 
optimal performance being found (as usual) for P = 3 (13-2-3-7). 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_33" validated="true"><head>Table 4 .</head><label>4</label><figDesc>10: SLR performance of an i-vector system based on SD-PLLR features, using different d values, on the NIST 2007 LRE 30s test set.</figDesc><table>SD-PLLR configuration 
C avg C LLR 

13-1-3-7 
2.04 0.286 
13-2-3-7 
1.71 0.260 
13-3-3-7 
2.03 0.277 
</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_34" validated="false"><head>Table 4 .</head><label>4</label><figDesc><ref type="bibr" target="#b9">11</ref>: SLR performance of i-vector systems based on PLLR and SD-PLLR features, on the NIST 2011 LRE 30s test set.</figDesc><table>System 
C avg C LLR %C 24 

avg 

Baseline 
5.18 0.982 12.12 
SD-PLLR 13-2-3-7 4.10 0.826 10.48 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_35" validated="false"><head>Table 5 .</head><label>5</label><figDesc></figDesc><table>1: %Cavg and CLLR performance (and relative improvements) for the 
PLLR i-vector baseline system, and systems using projected PLLR features on 
the NIST 2007 LRE primary evaluation task. 

PLLR System 
Dim. 

%Cavg (r.i.) 

C LLR (r.i.) 

Baseline 
59 
2.86 
0.389 

Projection 
59 
2.31 (19%) 
0.320 (18%) 
Projection+PCA 
58 
2.10 (27%) 0.310 (20%) 

59 
2.24 (22%) 
0.321 (17%) 
PCA 
58 
2.26 (21%) 
0.319 (18%) 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_36" validated="false"><head>Table 5 .</head><label>5</label><figDesc></figDesc><table>2: %Cavg and CLLR performance for the PLLR i-vector baseline systems, 
systems using PLLR projected features, acoustic MFCC and phonotactic systems 
and fusions of them on the NIST 2009 LRE primary evaluation tasks. 

Dataset System 

%Cavg 

C LLR 

PLLR 
(a) 
2.42 0.505 
PLLR+Projection+PCA (b) 2.19 0.443 
Acoustic MFCC 
(c) 
2.70 0.535 
Phonotactic 
(d) 2.49 0.502 
(c)+(d) 
1.67 0.346 
(a)+(c)+(d) 
1.48 0.321 

2009 
LRE 

(b)+(c)+(d) 
1.42 0.307 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_37" validated="false"><head>Table 5 .</head><label>5</label><figDesc></figDesc><table>3: %Cavg and CLLR performance for the PLLR i-vector baseline systems, 
systems using PLLR projected features, acoustic MFCC and phonotactic systems 
and fusions of them on the NIST 2011 LRE primary evaluation tasks. 

Dataset System 

%Cavg 

C LLR %C 24 

avg 

PLLR 
(a) 
5.18 0.981 12.12 
PLLR+Projection+PCA (b) 4.30 0.824 11.33 
Acoustic MFCC 
(c) 
5.95 1.088 13.56 
Phonotactic 
(d) 7.15 1.280 14.28 
(c)+(d) 
4.34 0.823 10.43 
(a)+(c)+(d) 
3.63 0.714 
9.14 

2011 
LRE 

(b)+(c)+(d) 
3.33 0.667 
8.91 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_38" validated="false"><head>Table 5 .</head><label>5</label><figDesc></figDesc><table>4: %Cavg performance for the PLP2, SnGM and PLLR systems with 
logistic regression classifiers on the RATS evaluation set for the 120s, 30s, 10s 
and 3s signals. 

System 
120s 
30s 
10s 
3s 
PLP2 
7.72 11.69 16.39 
23.04 
SnGM-LE 5.86 12.28 18.53 
26.45 
SnGM-CZ 8.59 15.76 20.89 
27.95 
PLLR-LE 4.56 7.98 12.61 21.48 
PLLR-CZ 
6.95 10.76 15.13 21.32 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_39" validated="false"><head>Table 5 .</head><label>5</label><figDesc></figDesc><table>5: %Cavg performance for the PLP2, SnGM and PLLR systems with 
neural network classifiers on the RATS evaluation set for the 120s, 30s, 10s and 
3s signals. 

System 
120s 
30s 
10s 
3s 
1 
PLP2 
7.21 
9.21 
12.43 
18.58 
2 SnGM-LE 5.53 
9.34 
15.61 
22.76 
3 SnGM-CZ 7.23 10.46 15.38 
24.05 
4 PLLR-LE 5.37 7.31 11.46 17.63 
5 PLLR-CZ 5.81 
8.83 
12.30 
19.52 

Fusions 
120s 
30s 
10s 
3s 
4+5 
5.19 
6.79 
10.14 
16.04 
1+4 
5.80 
6.43 
8.69 
15.37 
2+4 
5.12 
6.61 
10.48 
16.93 
3+5 
5.74 
8.33 
11.28 
18.11 
1+2+4 
5.38 
6.31 
8.53 
14.90 
1+4+5 
5.29 
6.43 
8.71 
14.65 
1+2+3+4+5 
5.59 
6.21 
8.75 
14.37 
</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_40" validated="false"><head>Table 5 .</head><label>5</label><figDesc></figDesc><table>6: Cavg × 100 and CLLR performance for the Projected PLLR + PCA, 
Projected PLLR + PCA reduced and Projected PLLR + PCA reduced + SD 
approaches on the NIST 2007 LRE primary evaluation tasks. 

PLLR System 

%Cavg 

C LLR 

Projection + PCA 58 
2.10 0.310 
Projection + PCA 13 
2.43 0.330 
Projection + PCA 13 + SD 1.52 0.225 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_41" validated="true"><head>Table 5 . 7 :</head><label>57</label><figDesc>Cavg × 100, CLLR and %C 24 avg performance for the Projected PLLR + PCA, and Projected PLLR + PCA reduced + SD approaches for 13, 15 and 17 dimensionalities on the NIST 2011 LRE primary evaluation tasks.</figDesc><table>PLLR System 

%Cavg 

C LLR %C 24 

avg 

Projection + PCA 58 
4.30 0.824 11.33 
Projection + PCA 13 + SD 4.84 0.916 12.49 
Projection + PCA 15 + SD 4.39 0.833 11.26 
Projection + PCA 17 + SD 3.80 0.756 
9.52 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_43" validated="false"><head>Table 6 .</head><label>6</label><figDesc>Table 6.2 shows the performance of the PLLR-based system using different variability compensation techniques at the feature extraction stage, that is, RASTA filtering, Feature Warping (FW) and Mean and Variance Normalization (MVN). Except for the case of FW in the core condition 5 (telephone in training and test), none of the techniques outperforms the baseline system trained only on PLLR features with no variability compensation technique applied. Therefore, none of the techniques studied in this section will be used when computing the features in the experiments reported below.</figDesc><table>1: MinDCF performance of systems using only PLLR features and 
PLLR features augmented with dynamic coefficients on the NIST 2010 SRE core 
conditions. 

Core Condition 
SYSTEM 
(1) 
(2) 
(3) 
(4) 
(5) 

PLLR 
0.683 
0.811 
0.889 
0.697 
0.864 
PLLR+∆ 
0.653 0.804 0.862 0.690 0.848 
PLLR+∆+∆ 2 
0.702 
0.834 
0.904 
0.745 
0.852 
</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_44" validated="true"><head>Table 6 . 2 :</head><label>62</label><figDesc>MinDCF performance of systems using PLLR features under different configurations on the NIST 2010 SRE core conditions.</figDesc><table>Core Condition 
SYSTEM 
(1) 
(2) 
(3) 
(4) 
(5) 

PLLR+∆ 
0.653 0.804 0.862 0.690 0.848 
(PLLR+∆)+RASTA 0.830 
0.903 
0.951 
0.892 
0.982 
(PLLR+∆)+FW 
0.740 
0.859 
0.883 
0.757 0.794 
(PLLR+∆)+MVN 
0.735 
0.852 
0.869 
0.750 
0.821 

PLLR feature projection 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_45" validated="false"><head>Table 6 .</head><label>6</label><figDesc>3 presents results of the systems based on PLLR and projected PLLR features.</figDesc><table></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_46" validated="true"><head>Table 6 . 3 :</head><label>63</label><figDesc>MinDCF performance of systems using PLLR and projected PLLR features on the NIST 2010 SRE core conditions.</figDesc><table>Core Condition 
SYSTEM 
(1) 
(2) 
(3) 
(4) 
(5) 

PLLR+∆ 
0.653 0.804 0.862 0.690 0.848 
Projected PLLR+∆ 0.671 
0.823 
0.912 
0.802 
0.913 
</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_47" validated="false"><head>Table 6 . 4 :</head><label>64</label><figDesc>Results of i-vector /PLDA SR systems based on MFCC and PLLR features, and the fusion of them, on the NIST 2010 SRE core conditions. acoustic system (up to a 21% relative improvement in terms of EER), suggesting that PLLR features contain complementary information that can be used for SR.</figDesc><table>Condition 
System 
EER MinDCF ActDCF 

MFCC 
1.86 
0.417 
0.439 
PLLR+∆ 4.05 
0.653 
0.854 
(1) 

Interview 
same microphone 
in training and test 
Fusion 
1.40 
0.363 
0.367 

MFCC 
2.99 
0.562 
0.633 
PLLR+∆ 6.39 
0.804 
0.819 
(2) 

Interview 
different microphone 
in training and test 
Fusion 
2.36 
0.492 
0.553 

MFCC 
3.63 
0.625 
0.848 
PLLR+∆ 9.20 
0.862 
0.978 
(3) 
Interview training 
telephone test 
Fusion 
3.25 
0.522 
0.874 

MFCC 
1.71 
0.443 
0.475 
PLLR+∆ 5.52 
0.690 
0.703 
(4) 

Interview training 
telephone test 
rec. over microphone 
Fusion 
1.69 
0.372 
0.406 

MFCC 
4.64 
0.600 
0.712 
PLLR+∆ 8.41 
0.848 
0.869 
(5) 
Telephone 
in training and test 
Fusion 
4.29 
0.560 
0.688 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_48" validated="true"><head>Table 6 . 5 :</head><label>65</label><figDesc>Results of i-vector /PLDA SR systems based on MFCC and PLLR features, and the fusion of them, on the NIST 2012 SRE core conditions 2 and 5.</figDesc><table>Condition 
System 
EER MinDCF ActDCF 

MFCC 
1.77 
0.272 
0.290 
PLLR+∆ 3.12 
0.419 
0.440 
(2) 
Telephone with 
No Added Noise 
Fusion 
1.39 
0.215 
0.246 

MFCC 
1.93 
0.260 
0.294 
PLLR+∆ 3.72 
0.449 
0.481 
(5) 
Telephone 
Recorded in Noise 
Fusion 
1.64 
0.219 
0.283 
</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_49" validated="false"><head></head><label></label><figDesc>Some systems have used PLLR-based i-vectors under LR or NN modeling. The PLLR features could still be tested under other modeling techniques, with the aim of finding the most advantageous modeling for PLLR based systems.Most of the research conducted in this work has experimented on relatively long signals (30s). Results attained in RATS benchmarks for shorter signals (10s and 3s) suggest that PLLRs are suitable for recognition on short duration signals. This fact should be further checked in other benchmarks, like NIST LRE datasets.</figDesc><table></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_50" validated="false"><head>Table A .</head><label>A</label><figDesc>1: 2007 NIST LRE core condition: training data (hours), development and evaluation data (# 30s segments), disaggregated for target and non-target languages.The NIST 2009 LRE featured 23 target languages<ref type="bibr" target="#b93">[93]</ref>, involving 11 target languages for which Conversational Telephone Speech (CTS) was available in the NIST 2007 LRE dataset, plus 12 target languages not seen in previous NIST LRE for which Broadcast Narrow-Band Speech was provided. Most of the speech provided for the latter consisted of telephone calls included in Voice of America (VOA) broadcasts. For the 12 new target languages, NIST distributed between 141 and 199 30-second audited VOA segments per language. Additional non-audited materials were provided for the 23 target languages and for several non-target languages (seeTable A.2).Training and development data used in this thesis were limited to those distributed by NIST to all 2009 LRE participants. A set of 64 languages/dialects was defined</figDesc><table>Hours 
# 30s cuts 
Language 
Train 
Devel 
Eval 
Arabic 
52.59 
179 
80 
Bengali 
5.0 
76 
80 
Chinese 
166.12 
567 
398 
English 
143.7 
288 
240 
Farsi 
46.22 
225 
80 
German 
57.03 
173 
80 
Industani 
64.35 
243 
240 
Japanese 
79.11 
141 
80 
Korean 
72.86 
150 
80 
Russian 
5.0 
66 
160 
Spanish 
117.35 
531 
240 
Tamil 
58.18 
165 
160 
Thai 
5.0 
64 
80 
Vietnamese 
46.7 
205 
160 
Non-Target 
48.74 
-
-

TOTAL 
967.95 
3073 
2158 

A.2 NIST 2009 LRE 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_51" validated="false"><head>Table A .</head><label>A</label><figDesc>2: 2009 NIST LRE core condition: training data (hours), development and evaluation data (# 30s segments), disaggregated for target and non-target languages.</figDesc><table>Hours 
# 30s cuts 
Train 
Devel 
Eval 
2007 (CTS) 
Language 

2007 
(CTS) 

2009 
(VOA) 
devel 
eval 

2009 
(VOA) 
2009 

Amharic 
-
58.31 
-
-
262 
398 
Bosnian 
-
5.63 
-
-
259 
355 
Cantonese 
5.0 
2.45 
83 
80 
104 
378 
Creole 
-
7.21 
-
-
256 
323 
Croatian 
-
6.45 
-
-
190 
376 
Dari 
-
69.05 
-
-
276 
389 
EngAmerican 
130.7 
9.01 
204 
80 
230 
896 
EngIndian 
13.0 
-
84 
160 
-
574 
Farsi/Persian 
46.22 
25.16 
225 
80 
294 
390 
French 
48.74 
67.91 
222 
80 
293 
395 
Georgian 
-
4.32 
-
-
166 
399 
Hausa 
-
48.31 
-
-
274 
389 
Hindi 
59.35 
10.06 
174 
160 
178 
667 
Korean 
72.86 
5.70 
150 
80 
250 
463 
Mandarin 
151.1 
32.4 
331 
158 
230 
1015 
Pashto 
-
184.3 
-
-
281 
395 
Portuguese 
-
25.74 
-
-
240 
397 
Russian 
5.0 
147.76 
66 
160 
299 
511 
Spanish 
117.35 
45.44 
531 
240 
242 
385 
Turkish 
-
6.67 
-
-
289 
394 
Ukrainian 
-
5.59 
-
-
281 
388 
Urdu 
5.0 
36.60 
69 
80 
299 
379 
Vietnamese 
46.7 
9.5 
205 
160 
240 
315 
Non-Target 
266.92 
408.82 
-
-
-
-

TOTAL 
967.95 
1222.39 
2344 
1518 
5433 
10571 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_52" validated="false"><head></head><label></label><figDesc>The whole training dataset (CTS from NIST 2007 LRE and VOA broadcast speech from NIST 2009 LRE) amounted to 2190 hours. For development, some materials taken from the development and evaluation datasets of the NIST 2007 LRE were</figDesc><table></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_53" validated="false"><head>Table A .</head><label>A</label><figDesc>3: NIST 2011 LRE core condition: training data (hours) disaggregated for target and non-target languages.used (seeTable A.1). For languages appearing in VOA, besides the audited segments provided by NIST, additional randomly extracted speech segments, each around 30 seconds long (specifically, between 25 and 35 seconds long), were used. The whole development dataset consisted of 9295 segments. Results reported in this thesis were computed on the NIST 2009 LRE evaluation corpus, specifically on the 30-second, closed-set condition (primary evaluation task).</figDesc><table>Hours 
Train 

Language 

2007 
(CTS) 

2009 
(VOA) 

2011 
(30s audit) 

Other 
sources 
Arabic Iraqi 
-
-
0.48 
20.34 
Arabic Levantine 
-
-
0.47 
27.56 
Arabic Maghrebi 
-
-
0.41 
1.79 
Arabic MSA 
-
-
0.47 
1.87 
Bengali 
5.0 
54.40 
-
-
Czech 
-
-
0.41 
4.19 
Dari 
-
69.05 
-
-
English American 
130.7 
9.01 
-
-
English Indian 
13.0 
-
-
-
Farsi/Persian 
46.22 
25.16 
-
-
Hindi 
59.35 
10.06 
-
-
Lao 
-
-
0.50 
2.22 
Mandarin 
151.1 
32.40 
-
-
Panjabi 
-
-
0.50 
-
Pashto 
-
184.38 
-
-
Polish 
-
-
0.51 
1.79 
Russian 
5.0 
147.76 
-
-
Slovak 
-
-
0.41 
1.69 
Spanish 
117.3 
45.44 
-
-
Tamil 
58.18 
-
-
-
Thai 
5.0 
-
-
-
Turkish 
-
6.67 
-
-
Ukrainian 
-
5.59 
-
-
Urdu 
5.0 
36.60 
-
-
Non-Target 
257.76 
406.93 
-
-
TOTAL 
853.61 
1033.45 
4.16 
61.45 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_54" validated="false"><head>Table A .</head><label>A</label><figDesc>4: NIST 2011 LRE core condition: development and evaluation data (30s segments), disaggregated for target and non-target languages.</figDesc><table># 30s cuts 
Devel 
Eval 
2007 
2009 
Language 
(CTS) (eval) 
(VOA) (eval) 

2011 
(audit) 
2011 

Arabic Iraqi 
-
-
-
-
48 
308 
Arabic Levantine 
-
-
-
-
49 
308 
Arabic Maghrebi 
-
-
-
-
54 
305 
Arabic MSA 
-
-
-
-
51 
306 
Bengali 
76 
80 
296 
43 
-
412 
Czech 
-
-
-
-
56 
261 
Dari 
-
-
276 
389 
-
267 
English American 
204 
80 
230 
896 
-
221 
English Indian 
84 
160 
-
574 
-
387 
Farsi/Persian 
</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_55" validated="false"><head></head><label></label><figDesc>archives in TV websites to get speech segments in Arabic Maghrebi (Arrabia TV, http://www.arrabia.ma) and Polish (Telewizja Polska, TVP INFO, http://tvp.info). TV broadcasts were fully audited, so that only reasonably clean speech segments were selected for training. It was not feasible to collect additional training materials for Panjabi by any means. Therefore, a single model (trained on just 55 segments) was used for this language.A set of 66 languages/dialects was defined for training. Each of them was mapped either to a target language or to non-target languages 7 . The training dataset included the data mentioned above (one-half of the audited segments plus other sources) plus 2007 CTS and 2009 VOA signals (seeTables A.1and A.2). The whole training dataset for the NIST 2011 LRE benchmark amounted to 1953 hours. For development purposes, the second half of the audited segments provided for new target languages, along with the NIST 2007 and 2009 evaluation datasets, and 30second signals used for development in 2007 and 2009 (seeTables A.1and A.2) were used. The whole development dataset consisted of 13663 segments. Results reported in this paper were computed on the NIST 2011 LRE evaluation corpus, specifically on the 30-second, closed-set condition (primary evaluation task) (seeTable A.4 for more details).</figDesc><table></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_56" validated="false"><head>Table A . 5 :</head><label>A5</label><figDesc>Albayzin 2010 LRE: Distribution of training data (hours) and development and evaluation data (30s segments).</figDesc><table>Clean Speech 
Noisy Speech 
Hours 
# 30s cuts 
Hours 
# 30s cuts 
Language 
Train 
Devel 
Eval 
Train 
Devel 
Eval 
Basque 
10.73 
146 
130 
2.25 
29 
74 
Catalan 
11.45 
120 
149 
2.18 
47 
55 
English 
12.18 
133 
135 
2.53 
60 
69 
Galician 
10.74 
137 
121 
2.23 
60 
83 
Portuguese 
11.08 
164 
146 
3.28 
77 
58 
Spanish 
10.41 
136 
125 
3.70 
83 
79 

TOTAL 
66.59 
836 
806 
16.17 
356 
418 

A.5 NIST 2010 SRE 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_57" validated="false"><head>Table A .</head><label>A</label><figDesc>6 shows the speaker distribution across different NIST SRE datasets. The elements in the diagonal show the number of speakers per dataset and the ones outside the diagonal represent the number of speakers shared by the corresponding pair of datasets.</figDesc><table></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_58" validated="false"><head>Table A . 6 :</head><label>A6</label><figDesc>Number of speakers uniquely included in each dataset (diagonal) and shared with other datasets (outside the diagonal).</figDesc><table>SRE04 SRE05 SRE06 SRE08 FU08 
SRE04 
310 
0 
0 
0 
0 
SRE05 
0 
525 
348 
0 
0 
SRE06 
0 
348 
949 
112 
0 
SRE08 
0 
0 
112 
1336 
150 
FU08 
0 
0 
0 
150 
150 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_59" validated="false"><head>Table A . 7 :</head><label>A7</label><figDesc>NIST 2004, 2005 and 2006 SRE signal distribution.• NIST 2008 SRE: Datasets for this series of experiments were defined so that they included disjoint sets of speakers, thus, all the signals of NIST 2008 SRE belonging to the 112 speakers present in previous databases were discarded. The rest of the signals were divided into two groups: dev1 and dev2.Table A.8 shows the number of signals in each of these subsets.</figDesc><table>Women 
Men 
Total 
UBM 
2804 
2119 
4923 
CHC 
4586 
3531 
8117 
IMP 
2780 
2094 
4874 
TNorm 
1479 
960 
2439 
ZNorm 
1403 
1146 
2549 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_60" validated="false"><head>Table A .</head><label>A</label><figDesc>8: NIST 2008 SRE signal distribution.</figDesc><table>SRE08 reduced dev1 
dev2 
training 
3149 
1621 
1528 
test 
6211 
3306 
2905 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_61" validated="false"><head>Table A .</head><label>A</label><figDesc>9: NIST 2008 Follow Up signal distribution.The dataset used for UBM training is a subset ofNIST 2004NIST  , 2005 and 2006 SREs consisting of 4882 speech files, as defined in<ref type="bibr" target="#b108">[108]</ref>. The approaches presented in<ref type="bibr" target="#b46">[46,</ref><ref type="bibr" target="#b106">106,</ref><ref type="bibr" target="#b108">108]</ref> used another subset of signals from the same datasets, that amounted to 8117 speech files, for channel compensation.For the systems using i-vector-PLDA approaches, the Total Variability matrices were trained on the channel compensation subset (including 8117 signals) defined previously.The  PLDA models were trained on a subset of the NIST 2004, 2005, 2006 and 2008 SRE datasets plus NIST 2008 Follow-up signals, created with the signals from the IMP, TNorm, ZNorm and NIST 2008 SRE reduced corpora, which amounted to 23302 speech files. The dataset used for UBM training is the same defined for the NIST 2010 SRE benchmark: a subset of SRE04, SRE05 and SRE06 consisting of 4882 speech files.For the NIST 2012 SRE experiments, the Total Variability matrices and the PLDA models shared a common training set, including speakers from the NIST 2006, 2008 and 2010 SRE datasets and avoiding repeated speech. This file list was the single file per ldcid map provided by NIST. Besides those segments, 590 channelbalanced randomly chosen signals from the NIST 2008 Follow-Up set were used and the 100 single utterance per speaker signals provided for the NIST 2012 SRE. The whole training set amounted to 21176 speech files, as described in<ref type="bibr" target="#b49">[49]</ref>.</figDesc><table>Speakers 
Signals 
Women Men 
Women Men 
All 
CHC 
38 
38 
2432 
1776 4208 
TNorm 
18 
18 
1145 
848 1993 
ZNorm 
19 
19 
1212 
875 2087 

A.6 NIST 2012 SRE 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_62" validated="false"><head>Table A .</head><label>A</label><figDesc>10: NIST 2012 SRE iVector and PLDA training signal distribution.</figDesc><table>Speakers 
Signals 
Women Men Women Men 
All 
NIST 2006 SRE 
72 
38 
754 
434 
1188 
NIST 2008 SRE 
799 
461 
7693 
4419 12112 
NIST 2010 SRE 
260 
240 
3746 
3240 7186 
NIST 2008 Follow Up 
86 
63 
339 
251 
590 
NIST 2012 SRE Single spk 
60 
40 
60 
40 
100 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_63" validated="false"><head>Table B . 1 :</head><label>B1</label><figDesc>Backend and fusion configuration for the EHU systems submitted to the NIST 2011 LRE.</figDesc><table>Backend and Fusion Training dataset 
System zt-norm 
30s 
10s 
3s 
Pri 
No 
dev30 
dev10 
dev03 
Con1 
No 
dev30 dev10+dev30 dev03+dev10+dev30 
Con2 
Yes 
dev30 
dev10 
dev03 
Con3 
Yes 
dev30 dev10+dev30 dev03+dev10+dev30 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_64" validated="false"><head>Table B . 2 :</head><label>B2</label><figDesc>Performance (in terms of Cavg) of the phonotactic and acoustic subsystems and partial and complete fusions on the NIST 2011 LRE 30s test set.</figDesc><table>C 24 

avg 

C avg 
min 
act 
min 
act 
Phone-CZ 
12.15 14.02 2.97 3.76 
Phone-HU 
11.96 14.28 2.71 3.62 
Phone-RU 
11.38 13.76 2.57 3.46 
Phonotactic 
7.73 
10.13 1.47 2.28 

Dot-Scoring 
11.62 14.18 2.19 3.17 
i-vector 
11.58 14.15 2.60 3.50 
Acoustic 
11.18 13.30 2.00 2.85 

All 
6.15 
8.95 
0.93 1.69 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_65" validated="false"><head>Table B . 3 :</head><label>B3</label><figDesc>Official NIST 2011 LRE results for the EHU systems.</figDesc><table>30s 
min C 24 

avg 

act C 24 

avg 

min C avg act C avg 
Pri 
6.15 
8.95 
0.93 
1.69 
Con1 
6.15 
8.95 
0.94 
1.69 
Con2 
6.08 
9.09 
0.91 
1.75 
Con3 
6.07 
9.07 
0.91 
1.75 

10s 
min C 24 

avg 

act C 24 

avg 

min C avg act C avg 
Pri 
12.99 
14.77 
3.37 
4.08 
Con1 
12.44 
14.55 
3.23 
4.03 
Con2 
12.72 
14.68 
3.31 
4.12 
Con3 
12.36 
14.36 
3.14 
3.95 

3s 
min C 24 

avg 

act C 24 

avg 

min C avg act C avg 
Pri 
25.54 
27.25 
11.60 
12.88 
Con1 
23.97 
25.34 
11.07 
12.05 
Con2 
25.52 
27.05 
11.62 
12.86 
Con3 
23.31 
25.28 
10.87 
12.06 
</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_66" validated="false"><head>Table B . 4 :</head><label>B4</label><figDesc>Results of EHU i-vector-PLDA systems based on MFCC and PLLR features, and the fusion of them, on the NIST 2012 SRE core conditions.</figDesc><table>Condition 
System 
EER MinDCF ActDCF 

MFCC 
9.00 
0.514 
0.716 
PLLR + ∆ 13.85 
0.620 
0.751 
Interview with 
No Added Noise 
Fusion 
9.38 
0.486 
0.548 

MFCC 
1.83 
0.277 
0.296 
PLLR + ∆ 
3.12 
0.419 
0.440 
Telephone with 
No Added Noise 
Fusion 
1.39 
0.213 
0.239 

MFCC 
9.98 
0.533 
1.024 
PLLR + ∆ 14.98 
0.615 
0.842 
Interview with 
Added Noise 
Fusion 
10.57 
0.514 
0.726 

MFCC 
7.05 
0.510 
0.519 
PLLR + ∆ 
7.73 
0.611 
0.619 
Telephone with 
Added Noise 
Fusion 
5.60 
0.430 
0.465 

MFCC 
2.11 
0.312 
0.404 
PLLR + ∆ 
3.72 
0.449 
0.481 
Telephone 
Recorded in Noise 
Fusion 
1.69 
0.198 
0.562 
</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_67" validated="false"><head>Table B . 5 :</head><label>B5</label><figDesc>Official MOBIO 2013 results for several primary systems.</figDesc><table>System 
Female EER Male EER 
Alpineon 
10.678 
7.076 
GIAPSI 
12.813 
8.865 
Mines-Telecom 
11.633 
9.109 
EHU 
19.511 
10.058 
L2F 
22.140 
11.129 
L2F-EHU 
17.266 
8.191 

Fusion 
(11 systems) 
6.986 
4.767 

</table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">The Albayzin evaluations were framed in the "Jornadas en Tecnologías del Habla" organized by the Spanish Thematic Network on Speech Technology (RTTH), and the ISCA Special Interest Group on Iberian Languages (SIG-IL)</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">C min (l 1 , l 2 ): minimum of the pairwise cost function, found for the optimal operation point (threshold).</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">The set of merged phonemes was kindly provided by the authors.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">Note that baseline performance reported in this Chapter is worse than that reported in Chapter 3, due to the lower number of iterations performed to obtain the Total Variability matrix (5 instead of 10).</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">http://www.phon.ucl.ac.uk/home/sampa/hungaria.htm</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">See http://www.ldc.upenn.edu/. 2 OHSU Corpora, http://www.ohsu.edu/. 3 See http://www.itl.nist.gov/iad/mig/tests/lre/2007/. 4 French was the only non-target language used for NIST 2007 LRE.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5">The set of non-target languages defined for the NIST 2009 LRE included: Arabic, Bengali, German, Japanese, Tamil and Thai from CTS recordings, and Albanian, Azerbaijani, Bangla, Burmese, Greek, Indonesian, Khmer, Kinyarwanda/Kirundi, Kurdish, Macedonian, Ndebele, Oromo, Serbian, Shona, Somali, Swahili, Tibetan, Tigrigna, and Uzbek from VOA broadcasts.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="6">Available online: http://dnt.kr.hsnr.de</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="7">The set of non-target languages defined for the NIST 2011 LRE included: French, German, Japanese, Korean and Vietnamese from CTS recordings, and Albanian, Amharic, Creole, French, Georgian, Greek, Hausa, Indonesian, Kinyarwanda/Kirundi, Korean, Ndebele, Oromo, Shona, Somali, Swahili, Tibetan and Tigrigna from VOA broadcasts.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">In NIST evaluation submissions EHU is used as acronym for our systems 2 http://gtts.ehu.es/TWiki/bin/view/Sautrela</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3">http://htk.eng.cam.ac.uk/docs/docs.shtml 4 http://www.speech.sri.com/projects/srilm/ 5 http://www.csie.ntu.edu.tw/∼cjlin/liblinear</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgements</head></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.1 NIST 2007 LRE</head><p>The NIST 2007 LRE <ref type="bibr" target="#b96">[96]</ref> defined a spoken language recognition task for conversational speech across telephone channels, involving 14 target languages.</p><p>Training and development data used in this thesis were limited to those distributed by NIST to all 2007 LRE participants: (1) the Call-Friend Corpus 1 ; (2) the OHSU Corpus provided by NIST for the 2005 LRE 2 ; and (3) the development corpus provided by NIST for the 2007 LRE 3 . A set of 23 languages/dialects was defined for training, including target and non-target 4 languages. For development purposes, 10 conversations per language were randomly selected, and the remaining conversations (amounting to around 968 hours) were used for training. Development conversations were further divided into 30-second speech segments. The total number of 30-second segments was 3073 (see <ref type="table">Table A</ref>.1 for more details). Results reported in this thesis Appendix B</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Participation in International Challenges</head><p>The work carried out comprised also building several systems which were part of the submissions of the research group (GTTS, http://gtts.ehu.es) to several international evaluations. This Appendix provides details about the participation in some of them: a short description of the evaluation, specifications of the submitted systems, the development stages in which the author contributed and the attained results.</p><p>NIST evaluation rules prevent participants from commenting on the rank their systems have attained or sharing results from other participants. Therefore, for those challenges, this section will only provide system development and official results for our site, and comments will only be made comparing the different approaches presented by our group.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.1 NIST SRE 2010</head><p>Evaluation:</p><p>The NIST 2010 SRE followed the spirit of previous evaluations (see 6.1.1 for details). The evaluation focused on both, telephone and microphone speech, recorded over different types of channels. Further details can be found in <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b94">94]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Appendix B. International Challenges</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>System:</head><p>The EHU 1 system was built by discriminatively fusing four subsystems: a GMM-SVM sub-system, a Linearized Eigenchannel GMM (LE-GMM) subsystem, a GLDS-SVM subsystem and a JFA subsystem.</p><p>The Qualcomm-ICSI-OGI (QIO) noise reduction technique (based on Wiener filtering) was independently applied to the audio streams. The full audio stream was taken as input to estimate noise characteristics.</p><p>Features were obtained with the Sautrela toolkit 2 <ref type="bibr" target="#b105">[105]</ref>. Mel-Frequency Cepstral Coefficients (MFCC) were used as acoustic features, computed as described in Section 6.3. Two gender dependent UBMs consisting of 1024 mixture components were trained with the Sautrela toolkit.</p><p>• GMM-SVM &amp; LE-GMM subsystems: The GMM-SVM and LE-GMM (also known as dot-scoring) subsystems were built following the SUNSDV system description for NIST 2008 SRE <ref type="bibr" target="#b145">[145]</ref>. Channel compensation was trained for inter-telephone, inter-microphone and telephone-microphone variations, using 20, 20 and 40 eigenchannels, respectively. For the GMM-SVM subsystem, a linear kernel was trained using SMVTorch <ref type="bibr" target="#b36">[36]</ref>.</p><p>• GLDS-SVM subsystem: Sufficient statistics space compensation was projected to feature space by applying the following expression:</p><p>where f t is the feature vector at time t, γ k (t) is the posterior of Gaussian k at time t, n k = t γ k (t) is the zero-order statistic of Gaussian k, Σ Σ Σ k is the diagonal covariance matrix of Gaussian k and c S k is the first-order statistics shift (sufficient statistics space compensation factor) of Gaussian k given the input segment S. A polynomial expansion of degree 3 and a Generalized Linear Discriminant Sequence Kernel <ref type="bibr" target="#b28">[28]</ref> were then applied.</p><p>• JFA subsystem: The Joint Factor Analysis Matlab Demo from BUT <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b83">83]</ref> was applied to the MFCC + ∆ + ∆∆ features, using 200 eigenvoices and 100 eigenchannels.</p><p>Trials were conditioned on three channel types: no microphone sessions (0MIC), one microphone session (1MIC) and two microphone sessions (2MIC). Gender dependent</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Joint Factor Analysis Matlab Demo</title>
		<ptr target="http://speech.fit.vutbr.cz/en/software/jointfactor-analysis-matlab-demo" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
				<ptr target="https://www.ldc.upenn.edu/" />
		<title level="m">Linguistic Data Consortium</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Nist Lre</surname></persName>
		</author>
		<ptr target="http://www.itl.nist.gov/iad/mig/tests/lre/" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nist</forename><surname>The</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Year</surname></persName>
		</author>
		<ptr target="http://www.itl.nist.gov/iad/mig/tests/spk/2010/index.html" />
		<title level="m">Speaker Recognition Evaluation Plan</title>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">The NIST Year 2012 Speaker Recognition Evaluation Plan</title>
		<ptr target="http://www.nist.gov/itl/iad/mig/upload/NIST_SRE12_evalplan-v17-r1.pdf" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
				<ptr target="https://sites.google.com/site/gttspllrfeatures/home" />
		<title level="m">PLLR computation software</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
				<ptr target="http://www.nist.gov/itl/iad/mig/upload/LRE11_EvalPlan_releasev1.pdf" />
		<title level="m">The 2011 NIST Language Recognition Evaluation Plan (LRE11)</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title/>
		<ptr target="http://www.langsci.ucl.ac.uk/ipa/ipachart.html" />
	</analytic>
	<monogr>
		<title level="j">IPA Chart</title>
		<imprint>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Qualcomm-ICSI-OGI features for ASR</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Adami</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Burget</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Dupont</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Garudadri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Grezl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Hermansky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kajarekar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Morgan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Sivadas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ICSLP2002</title>
		<meeting>ICSLP2002</meeting>
		<imprint>
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Modeling prosodic differences for speaker recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">G</forename><surname>Adami</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Speech Communication</title>
		<imprint>
			<biblScope unit="volume">49</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="277" to="291" />
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Score normalization for text-independent speaker verification systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Auckenthaler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Carey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lloyd-Thomas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Digital Signal Processing</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="issue">1-3</biblScope>
			<biblScope unit="page" from="42" to="54" />
			<date type="published" when="2000-01" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Benesty</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">M</forename><surname>Sondhi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Huang</surname></persName>
		</author>
		<title level="m">Springer Handbook of Speech Processing</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Pattern Recognition and Machine Learning (Information Science and Statistics)</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">M</forename><surname>Bishop</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2006" />
			<publisher>Springer-Verlag New York, Inc</publisher>
			<pubPlace>Secaucus, NJ, USA</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Measuring, refining and calibrating speaker and language information extracted from speech</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Brümmer</surname></persName>
		</author>
		<imprint>
			<biblScope unit="page">7602</biblScope>
		</imprint>
		<respStmt>
			<orgName>Department of Electrical and Electronic Engineering, University of Stellenbosch, Private Bag X1</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">PhD thesis</note>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">South</forename><surname>Matieland</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Africa</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">The EM algorithm and Minimum Divergence applied to PLDA</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Brummer</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
	<note type="report_type">Technical report</note>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Fusion of heterogeneous speaker recognition systems in the STBU submission for the NIST speaker recognition evaluation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Brümmer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Burget</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Cernocky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Glembek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Grezl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Karafiat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Van Leeuwen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Matejka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Schwarz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Strasheim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Audio, Speech and Language Processing</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="2072" to="2084" />
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Description and analysis of the Brno 276 system for LRE2011</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Brümmer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Cumani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Glembek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Karafiát</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Matejka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pesán</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Plchot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Soufifar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Villiers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Cernocký</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Odyssey 2012: The Speaker and Language Recognition Workshop</title>
		<meeting><address><addrLine>Singapore</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2012-06" />
			<biblScope unit="page" from="216" to="223" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">The BOSARIS Toolkit: Theory, Algorithms and Code for Surviving the New DCF</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Brümmer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>De Villiers</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the NIST</title>
		<meeting>the NIST</meeting>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
				<ptr target="http://sites.google.com/site/bosaristoolkit/" />
		<title level="m">Speaker Recognition Workshop</title>
		<meeting><address><addrLine>Atlanta (GA), USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2011-12" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Application-Independent Evaluation of Speaker Detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Brümmer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Du Preez</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computer, Speech and Language</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">2-3</biblScope>
			<biblScope unit="page" from="230" to="275" />
			<date type="published" when="2006-07" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Discriminative Acoustic Language Recognition via Channel-Compensated GMM Statistics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Brümmer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Strasheim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Hubeika</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Matejka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Burget</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Glembek</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Interspeech</title>
		<meeting>Interspeech<address><addrLine>Brighton, UK</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2009-09" />
			<biblScope unit="page" from="2187" to="2190" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">On calibration of language recognition scores</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Brümmer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Van Leeuwen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Odyssey -The Speaker and Language Recognition Workshop</title>
		<meeting>Odyssey -The Speaker and Language Recognition Workshop</meeting>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">ABC System description for NIST SRE 2010</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Brummer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2010 NIST Speaker Recognition Evaluation (SRE)</title>
		<meeting><address><addrLine>Brno, Czech Republic</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Discriminatively trained probabilistic linear discriminant analysis for speaker verification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Burget</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Plchot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Cumani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Glembek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Matejka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Brummer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP) 2011</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2011" />
			<biblScope unit="page" from="4832" to="4835" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Speaker verification using support vector machines and high-level features. Audio, Speech, and Language Processing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Campbell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Campbell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Gleason</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Reynolds</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Shen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="2085" to="2094" />
			<date type="published" when="2007-09" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Advanced Language Recognition using Cepstra and Phonotactics: MITLL System Performance on the NIST 2005 Language Recognition Evaluation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Campbell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Gleason</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Navratil</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Reynolds</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Singer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Torres-Carrasquillo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Odyssey 2006 -The Speaker and Language Recognition Workshop</title>
		<meeting>Odyssey 2006 -The Speaker and Language Recognition Workshop<address><addrLine>San Juan, Puerto Rico</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2006-06" />
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">SVM Based Speaker Verification using a GMM Supervector Kernel and NAP Variability Compensation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Campbell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Sturim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Reynolds</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Solomonoff</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings. 2006 IEEE International Conference on</title>
		<meeting>2006 IEEE International Conference on</meeting>
		<imprint>
			<date type="published" when="2006-05" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="97" to="100" />
		</imprint>
	</monogr>
	<note>Acoustics, Speech and Signal Processing</note>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Generalized Linear Discriminant Sequence Kernels for Speaker Recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">M</forename><surname>Campbell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of IEEE ICASSP</title>
		<meeting>IEEE ICASSP</meeting>
		<imprint>
			<date type="published" when="2002" />
			<biblScope unit="volume">I</biblScope>
			<biblScope unit="page" from="161" to="164" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Language Recognition with Word Lattices and Support Vector Machines</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">M</forename><surname>Campbell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Richardson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">A</forename><surname>Reynolds</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of IEEE ICASSP</title>
		<meeting>IEEE ICASSP<address><addrLine>Honolulu, Hawaii, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page" from="15" to="20" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Language Recognition with Support Vector Machines</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">M</forename><surname>Campbell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Singer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">A</forename><surname>Torres-Carrasquillo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">A</forename><surname>Reynolds</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Odyssey 2004 -The Speaker and Language Recognition Workshop</title>
		<meeting>Odyssey 2004 -The Speaker and Language Recognition Workshop<address><addrLine>Toledo, Spain</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2004-06" />
			<biblScope unit="page" from="41" to="44" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Loquendo-Politecnico di Torino system for the 2009 NIST Language Recognition Evaluation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Castaldo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Colibro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Cumani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Dalmasso</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Laface</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Vair</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICASSP</title>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="5002" to="5005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Compensation of Nuisance Factors for Speaker and Language Recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Castaldo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Colibro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Dalmasso</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Laface</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Vair</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Audio, Speech and Language Processing</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="1969" to="1978" />
			<date type="published" when="2007-09" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Acoustic Language Identification Using Fast Discriminative Training</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Castaldo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Colibro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Dalmasso</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Laface</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Vair</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Interspeech</title>
		<meeting>Interspeech<address><addrLine>Antwerp, Belgium</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2007-08" />
			<biblScope unit="page" from="346" to="349" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Language Recognition Using Language Factors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Castaldo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Cumani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Laface</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Colibro</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Interspeech</title>
		<meeting>Interspeech<address><addrLine>Brighton, UK</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2009-09" />
			<biblScope unit="page" from="176" to="179" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Voice activity detection based on multiple statistical models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-H</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">S</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">K</forename><surname>Mitra</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on</title>
		<imprint>
			<biblScope unit="volume">54</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1965" to="1976" />
			<date type="published" when="2006-06" />
		</imprint>
	</monogr>
	<note>Signal Processing</note>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">SVMTorch: Support vector machines for largescale regression problems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Collobert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="143" to="160" />
			<date type="published" when="2001" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Automatic Language Identification: Performance vs Complexity</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Combrink</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Botha</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Sixth Annual South Africa Workshop on Pattern Recognition</title>
		<meeting>the Sixth Annual South Africa Workshop on Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Comparison of parametric representations for monosyllabic word recognition in continuously spoken sentences. Acoustics, Speech and Signal Processing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Davis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Mermelstein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="357" to="366" />
			<date type="published" when="1980-08" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Modeling prosodic features with joint factor analysis for speaker verification. Audio, Speech, and Language Processing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Dehak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dumouchel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Kenny</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="2095" to="2103" />
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Front-end factor analysis for speaker verification. Audio, Speech, and Language Processing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Dehak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Kenny</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Dehak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dumouchel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Ouellet</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="788" to="798" />
			<date type="published" when="2011-05" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Language Recognition via i-vectors and Dimensionality Reduction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Dehak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">A</forename><surname>Torres-Carrasquillo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">A</forename><surname>Reynolds</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Dehak</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Interspeech</title>
		<meeting>the Interspeech<address><addrLine>Florence, Italy</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2011-08-27" />
			<biblScope unit="page" from="857" to="860" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Maximum likelihood from incomplete data via the em algorithm</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">P</forename><surname>Dempster</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">M</forename><surname>Laird</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">B</forename><surname>Rubin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of the Royal statistical Society</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="38" />
			<date type="published" when="1977" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Phonotactic Language Recognition using i-vectors and Phoneme Posteriogram Counts</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Haro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Glembek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Plchot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Matejka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Soufifar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Cordoba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Cernocký</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Interspeech</title>
		<meeting>the Interspeech<address><addrLine>Portland, Oregon</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2012-09-09" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<monogr>
		<title level="m" type="main">Verificación de la Lengua Mediante Modelos Acústicos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Diez</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009-06" />
		</imprint>
	</monogr>
	<note>Final Year Project</note>
</biblStruct>

<biblStruct xml:id="b45">
	<monogr>
		<title level="m" type="main">Compensación de canal en procesamiento del habla</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Diez</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2010-06" />
		</imprint>
	</monogr>
	<note type="report_type">MSc Thesis</note>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">University of the Basque Country System for the 2011 NIST SRE Analysis Workshop</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Diez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Penagarikano</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">J</forename><surname>Rodriguez Fuentes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Varona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Bordel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIST 2011 Speaker Recognition Analysis Workshop</title>
		<meeting><address><addrLine>Atlanta (USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2011-12" />
			<biblScope unit="page" from="8" to="9" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">GTTS System for the Albayzin 2010 Speaker Diarization Evaluation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Diez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Penagarikano</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Varona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">J</forename><surname>Rodriguez Fuentes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Bordel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">VI Jornadas en Tecnologías del Habla and II Iberian SLTech Workshop</title>
		<meeting><address><addrLine>Vigo, Spain</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2010-11" />
			<biblScope unit="page" from="10" to="12" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">On the use of Dot Scoring for Speaker Diarization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Diez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Penagarikano</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Varona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">J</forename><surname>Rodriguez Fuentes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Bordel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Iberian Conference on Pattern Recognition and Image Analysis</title>
		<meeting><address><addrLine>Las Palmas de Gran Canaria. Spain</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2011-06" />
			<biblScope unit="page" from="8" to="10" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">University of the Basque Country Systems for the NIST 2012 Speaker Recognition Evaluation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Diez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Penagarikano</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Varona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">J</forename><surname>Rodríguez-Fuentes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Bordel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the NIST-SRE 2012</title>
		<meeting>the NIST-SRE 2012<address><addrLine>Orlando, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2012-12" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">On the Use of Log-Likelihood Ratios as Features in Spoken Language Recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Diez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Varona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Penagarikano</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">J</forename><surname>Rodriguez-Fuentes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Bordel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Workshop on Spoken Language Technology (SLT 2012)</title>
		<meeting><address><addrLine>Miami, Florida, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2012-12" />
			<biblScope unit="page" from="274" to="279" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Dimensionality Reduction of Phone Log-Likelihood Ratio Features for Spoken Language Recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Diez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Varona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Penagarikano</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">J</forename><surname>Rodríguez-Fuentes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Bordel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Interspeech 2013</title>
		<meeting>Interspeech 2013<address><addrLine>Lyon, France</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2013-08" />
			<biblScope unit="page" from="64" to="68" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Diez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Varona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Penagarikano</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">J</forename><surname>Rodriguez Fuentes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Bordel</surname></persName>
		</author>
		<title level="m">Language Recognition on Albayzin 2010 LRE using PLLR features. Procesamiento del Lenguaje Natural</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="153" to="160" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Using Phone Log-Likelihood Ratios as Features for Speaker Recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Diez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Varona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Penagarikano</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">J</forename><surname>Rodríguez-Fuentes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Bordel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Interspeech</title>
		<meeting>Interspeech<address><addrLine>Lyon, France</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2013-08" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<monogr>
		<title level="m" type="main">New Insight into the Use of Phone Log-Likelihood Ratios as Features for Language Recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Diez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Varona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Penagarikano</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">J</forename><surname>Rodriguez Fuentes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Bordel</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014-09-18" />
			<biblScope unit="page">14</biblScope>
			<pubPlace>Singapore</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">On the Complementarity of Phone Posterior Probabilities for Improved Speaker Recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Diez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Varona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Penagarikano</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">J</forename><surname>Rodriguez Fuentes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Bordel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Signal Processing Letters</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="649" to="652" />
			<date type="published" when="2014-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">On the Projection of PLLRs for Unbounded Feature Distributions in Spoken Language Recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Diez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Varona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Penagarikano</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">J</forename><surname>Rodriguez Fuentes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Bordel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Signal Processing Letters</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="1073" to="1077" />
			<date type="published" when="2014-09" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Optimizing PLLR Features for Spoken Language Recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Diez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Varona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Penagarikano</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">J</forename><surname>Rodriguez Fuentes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Bordel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">proceedings of the 22nd International Conference on Pattern Recognition (ICPR&apos;14)</title>
		<meeting>the 22nd International Conference on Pattern Recognition (ICPR&apos;14)<address><addrLine>Stockholm, Sweden</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2014-08" />
			<biblScope unit="page" from="24" to="28" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">Speaker recognition based on idiolectal differences between speakers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">R</forename><surname>Doddington</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Interspeech</title>
		<meeting>Interspeech</meeting>
		<imprint>
			<date type="published" when="2001" />
			<biblScope unit="page" from="2521" to="2524" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">LIBLIN-EAR: A Library for Large Linear Classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R.-E</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K.-W</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-J</forename><surname>Hsieh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X.-R</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-J</forename><surname>Lin</surname></persName>
		</author>
		<ptr target="http://www.csie.ntu.edu.tw/~cjlin/liblinear" />
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page" from="1871" to="1874" />
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">A noise-robust system for nist 2012 speaker recognition evaluation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Ferrer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mclaren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Scheffer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Graciarena</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Mitra</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">INTER-SPEECH</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="1981" to="1985" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<monogr>
		<title level="m" type="main">Tools for evaluation, calibration and fusion of, and decision-making with, multi-class statistical pattern recognition scores</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Focal</surname></persName>
		</author>
		<ptr target="https://sites.google.com/site/nikobrummer/focalmulticlass" />
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<monogr>
		<title level="m" type="main">Toolkit for Evaluation, Fusion and Calibration of statistical pattern recognizers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Focal</surname></persName>
		</author>
		<ptr target="http://sites.google.com/site/nikobrummer/focal" />
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<analytic>
		<title level="a" type="main">Feature Extraction Using 2-D Autoregressive Models For Speaker Recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ganapathy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Thomas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Hermansky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ISCA Speaker Odyssey</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<analytic>
		<title level="a" type="main">Analysis of I-vector Length Normalization in Speaker Recognition Systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Garcia-Romero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">Y</forename><surname>Epsy-Wilson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">INTERSPEECH</title>
		<meeting><address><addrLine>Florence, Italy</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2011-08" />
			<biblScope unit="page" from="249" to="252" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b65">
	<analytic>
		<title level="a" type="main">Maximum A Posteriori Estimation for Multivariate Gaussian Mixture Observations of Markov Chains</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">L</forename><surname>Gauvain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Speech and Audio Processing</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="291" to="298" />
			<date type="published" when="1994" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b66">
	<analytic>
		<title level="a" type="main">Language recognition using phone lattices</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">L</forename><surname>Gauvain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Messaoudi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Schwenk</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ICSLP</title>
		<meeting>ICSLP</meeting>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="page" from="1283" to="1286" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b67">
	<analytic>
		<title level="a" type="main">Robust voice activity detection using long-term signal variability. Audio, Speech, and Language Processing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Ghosh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Tsiartas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Narayanan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="600" to="613" />
			<date type="published" when="2011-03" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b68">
	<analytic>
		<title level="a" type="main">Some statistical issues in the comparison of speech recognition algorithms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Gillick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Cox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICASSP</title>
		<meeting>ICASSP</meeting>
		<imprint>
			<date type="published" when="1989" />
			<biblScope unit="page" from="532" to="535" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b69">
	<analytic>
		<title level="a" type="main">The 2011 NIST Language Recognition Evaluation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Greenberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Martin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Przybocki</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Interspeech</title>
		<meeting>Interspeech<address><addrLine>Portland, Oregon</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b70">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">S</forename><surname>Greenberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><forename type="middle">M</forename><surname>Stanford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">F</forename><surname>Martin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Yadagiri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">R</forename><surname>Doddington</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">J</forename><surname>Godfrey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hernandez-Cordero</surname></persName>
		</author>
		<title level="m">The 2012 NIST Speaker Recognition Evaluation. In INTERSPEECH</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="1971" to="1975" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b71">
	<analytic>
		<title level="a" type="main">On the use of windows for harmonic analysis with the discrete Fourier transform</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">J</forename><surname>Harris</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the IEEE</title>
		<meeting>of the IEEE</meeting>
		<imprint>
			<date type="published" when="1978" />
			<biblScope unit="volume">66</biblScope>
			<biblScope unit="page" from="51" to="83" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b72">
	<analytic>
		<title level="a" type="main">Crss systems for 2012 nist speaker recognition evaluation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Hasan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Sadjadi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Shokouhi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Boril</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hansen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Acoustics, Speech and Signal Processing (ICASSP), 2013 IEEE International Conference on</title>
		<imprint>
			<date type="published" when="2013-05" />
			<biblScope unit="page" from="6783" to="6787" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b73">
	<analytic>
		<title level="a" type="main">Perceptual linear predictive (PLP) analysis of speech</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Hermansky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Acoust. Soc. Am</title>
		<imprint>
			<biblScope unit="volume">57</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="1738" to="52" />
			<date type="published" when="1990-04" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b74">
	<analytic>
		<title level="a" type="main">RASTA Processing of Speech. Speech and Audio Processing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Hermansky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Morgan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="578" to="589" />
			<date type="published" when="1994-10" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b75">
	<analytic>
		<title level="a" type="main">Discriminative training and channel compensation for acoustic language recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Hubeika</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Burget</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Matejka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Schwarz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Interspeech</title>
		<meeting>Interspeech</meeting>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b76">
	<analytic>
		<title level="a" type="main">Language identification based on speech fundamental frequency</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Itahashi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Du</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of IEEE ICASSP</title>
		<meeting>IEEE ICASSP</meeting>
		<imprint>
			<date type="published" when="1995-09" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="1359" to="1362" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b77">
	<analytic>
		<title level="a" type="main">Data selection and calibration issues in automatic language recognition -investigation with BUT-AGNITIO NIST LRE 2009 system</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Jancík</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Plchot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Brummer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Burget</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Glembek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Hubeika</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Karafiát</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Matejka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Strasheim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Cernocký</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Odyssey 2010 -The Speaker and Language Recognition Workshop</title>
		<meeting>Odyssey 2010 -The Speaker and Language Recognition Workshop</meeting>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="215" to="221" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b78">
	<monogr>
		<title level="m" type="main">Learning the Melscale and Optimal VTN Mapping</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Kamm</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Hermansky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">G</forename><surname>Andreou</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1997" />
			<publisher>CSLP</publisher>
		</imprint>
	</monogr>
	<note type="report_type">Technical report</note>
</biblStruct>

<biblStruct xml:id="b79">
	<analytic>
		<title level="a" type="main">Language Identification: Insights from the Classification of Hand Annotated Phone Transcripts</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Kempton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">K</forename><surname>Moore</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Odyssey 2008 -The Speaker and Language Recognition Workshop</title>
		<meeting>Odyssey 2008 -The Speaker and Language Recognition Workshop<address><addrLine>Stellenbosch, South Africa</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b80">
	<analytic>
		<title level="a" type="main">Joint factor analysis of speaker and session variability: Theory and algorithms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Kenny</surname></persName>
		</author>
		<idno>CRIM-06/08-13</idno>
		<ptr target="http://www.crim.ca/perso/patrick.kenny/" />
	</analytic>
	<monogr>
		<title level="j">CRIM</title>
		<imprint>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
	<note type="report_type">Technical Report Technical Report</note>
</biblStruct>

<biblStruct xml:id="b81">
	<analytic>
		<title level="a" type="main">Bayesian speaker verification with heavy-tailed priors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Kenny</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Odyssey, The Speaker and Language Tecognition Workshop</title>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b82">
	<analytic>
		<title level="a" type="main">Experiments in speaker verification using factor analysis likelihood ratios</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Kenny</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dumouchel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Odyssey: The Speaker and Language Recognition Workshop</title>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="page" from="219" to="226" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b83">
	<analytic>
		<title level="a" type="main">A study of interspeaker variability in speaker verification. Audio, Speech, and Language Processing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Kenny</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Ouellet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Dehak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dumouchel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="980" to="988" />
			<date type="published" when="2008-07" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b84">
	<analytic>
		<title level="a" type="main">The 2013 speaker recognition evaluation in mobile environment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Khoury</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Vesnicer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Franco-Pedroso</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Violato</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Boulkenafet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">M</forename><surname>Fernandez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Diez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kosmala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Khemiri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Cipr</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">G R</forename><surname>Saeidi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zganec-Gros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">Z</forename><surname>Candil</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Simoes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bengherabi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">A</forename><surname>Marquina</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Penagarikano</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Abad</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Boulayemen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Schwarz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">V</forename><surname>Leeuwen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gonzalez-Domínguez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">U</forename><surname>Neto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Boutellaa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">G</forename><surname>Vilda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Varona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Petrovska-Delacretaz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Matejka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gonzalez-Rodríguez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Pereira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Harizi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">J</forename><surname>Rodriguez Fuentes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">E</forename><surname>Shafey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Angeloni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Bordel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Chollet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Marcel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The 6th IAPR International Conference on Biometrics (ICB-2013)</title>
		<meeting><address><addrLine>Madrid, Spain</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2013-07" />
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b85">
	<analytic>
		<title level="a" type="main">Long-term f0 modeling for textindependent speaker recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Kinnunen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>González-Hautamäki</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 10th International Conference Speech and Computer (SPECOM)</title>
		<meeting>the 10th International Conference Speech and Computer (SPECOM)<address><addrLine>Patras, Greece</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="page" from="567" to="570" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b86">
	<analytic>
		<title level="a" type="main">An overview of text-independent speaker recognition: From features to supervectors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Kinnunen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Speech Communication</title>
		<imprint>
			<biblScope unit="volume">52</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="12" to="40" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b87">
	<analytic>
		<title level="a" type="main">ivector fusion of prosodic and cepstral features for speaker verification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Kockmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Ferrer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Burget</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Cernocký</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">INTERSPEECH</title>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="265" to="268" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b88">
	<analytic>
		<title level="a" type="main">Tuning Phone Decoders For Language Identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">S</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Tong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Matejka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Burget</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Cernocký</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the workshop on Human Language Technology</title>
		<meeting>the workshop on Human Language Technology<address><addrLine>Stroudsburg, PA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="4861" to="4864" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b89">
	<analytic>
		<title level="a" type="main">Efficient cepstral normalization for robust speech recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F.-H</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">M</forename><surname>Stern</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Acero</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the workshop on Human Language Technology, HLT &apos;93</title>
		<meeting>the workshop on Human Language Technology, HLT &apos;93<address><addrLine>Stroudsburg, PA, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="1993" />
			<biblScope unit="page" from="69" to="74" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b90">
	<analytic>
		<title level="a" type="main">Robust Speaker Recognition Using Spectro-Temporal Autoregressive Models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">H</forename><surname>Mallidi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ganapathy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Hermansky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Interspeech 2013</title>
		<meeting>Interspeech 2013</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="3689" to="3693" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b91">
	<analytic>
		<title level="a" type="main">On the results of the first mobile biometry (mobio) face and speaker verification evaluation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Marcel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Mccool</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Matejka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Ahonen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Cernocky</surname></persName>
		</author>
		<idno>RR-30-2010</idno>
	</analytic>
	<monogr>
		<title level="m">Idiap-RR Idiap</title>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b92">
	<analytic>
		<title level="a" type="main">The DET Curve in Assessment of Detection Task Performance</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Martin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Doddington</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Kamm</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ordowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Przybocki</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Eurospeech</title>
		<meeting>Eurospeech</meeting>
		<imprint>
			<date type="published" when="1997" />
			<biblScope unit="page" from="1985" to="1988" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b93">
	<analytic>
		<title level="a" type="main">The 2009 NIST Language Recognition Evaluation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Martin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Greenberg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Odyssey 2010 -The Speaker and Language Recognition Workshop, paper 030</title>
		<meeting><address><addrLine>Brno, Czech Republic</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="165" to="171" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b94">
	<analytic>
		<title level="a" type="main">The NIST 2010 Speaker Recognition Evaluation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">F</forename><surname>Martin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">S</forename><surname>Greenberg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Eleventh Annual Conference of the International Speech Communication Association</title>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b95">
	<analytic>
		<title level="a" type="main">The Current State of Language Recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">F</forename><surname>Martin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">N</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Odyssey 2006 -The Speaker and Language Recognition Workshop</title>
		<meeting>Odyssey 2006 -The Speaker and Language Recognition Workshop</meeting>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b96">
	<analytic>
		<title level="a" type="main">Language Recognition Evaluation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">F</forename><surname>Martin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">N</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Nist</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Odyssey 2008 -The Speaker and Language Recognition Workshop</title>
		<meeting>Odyssey 2008 -The Speaker and Language Recognition Workshop</meeting>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b97">
	<analytic>
		<title level="a" type="main">iVector-based Prosodic System for Language Identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Martínez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Burget</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Ferrer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Scheffer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ICASSP</title>
		<meeting>ICASSP<address><addrLine>Japan</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="4861" to="4864" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b98">
	<analytic>
		<title level="a" type="main">Language Recognition in iVectors Space</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Martínez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Plchot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Burget</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Glembek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Matejka</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Interspeech</title>
		<meeting>Interspeech<address><addrLine>Firenze, Italy</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="861" to="864" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b99">
	<analytic>
		<title level="a" type="main">Patrol Team Language Identification System for DARPA RATS P1 Evaluation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Matejka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Plchot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Soufifar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Glembek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">F</forename><surname>Haro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Veselý</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Grézl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">Z</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Matsoukas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Dehak</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Interspeech</title>
		<meeting>the Interspeech<address><addrLine>Portland, Oregon</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2012-09-09" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b100">
	<analytic>
		<title level="a" type="main">Full-covariance UBM and heavy-tailed PLDA in i-vector speaker verification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Matějka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Glembek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Castaldo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Alam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Plchot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Kenny</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Burget</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Černocký</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICASSP</title>
		<meeting>ICASSP</meeting>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b101">
	<analytic>
		<title level="a" type="main">Bi-modal person recognition on a mobile phone: using mobile phone data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Mccool</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Marcel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Hadid</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Pietikainen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Matejka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Cernocky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Poh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kittler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Larcher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Matrouf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-F</forename><surname>Bonastre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Tresadern</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Cootes</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE ICME Workshop on Hot Topics in Mobile Multimedia</title>
		<imprint>
			<date type="published" when="2012-07" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b102">
	<analytic>
		<title level="a" type="main">Databases For Speaker Recognition: Activities In COST250 Working Group 2</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Melin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings COST250 Workshop on Speaker Recognition in Telephony</title>
		<meeting>COST250 Workshop on Speaker Recognition in Telephony</meeting>
		<imprint>
			<date type="published" when="1999" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b103">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">W M</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-C</forename><surname>Leung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<title level="m">Spoken Language Recognition With Prosodic Features. IEEE Transactions on Audio, Speech &amp; Language Processing</title>
		<imprint>
			<date type="published" when="2013-09" />
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="page" from="1841" to="1853" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b104">
	<analytic>
		<title level="a" type="main">Feature Warping for Robust Speaker Verification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pelecanos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Sridharan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2001: A Speaker Odyssey -The Speaker Recognition Workshop</title>
		<imprint>
			<date type="published" when="2001" />
			<biblScope unit="page" from="213" to="218" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b105">
	<analytic>
		<title level="a" type="main">Sautrela: a highly modular open source speech recognition framework</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Penagarikano</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Bordel</surname></persName>
		</author>
		<ptr target="http://gtts.ehu.es/TWiki/bin/view/Sautrela" />
	</analytic>
	<monogr>
		<title level="m">Automatic Speech Recognition and Understanding</title>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="page" from="386" to="391" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b106">
	<analytic>
		<title level="a" type="main">Bordel. A speaker recognition system based on sufficient-statistics-space channelcompensation and dot-scoring</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Penagarikano</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Varona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Diez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">J</forename><surname>Rodriguez Fuentes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">VI Jornadas en Tecnologías del Habla and II Iberian SLTech Workshop</title>
		<meeting><address><addrLine>Vigo, Spain</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2010-11" />
			<biblScope unit="page" from="10" to="12" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b107">
	<analytic>
		<title level="a" type="main">University of the Basque Country System for NIST 2010 Speaker Recognition Evaluation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Penagarikano</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Varona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Diez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">J</forename><surname>Rodriguez Fuentes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Bordel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2010 NIST Speaker Recognition Evaluation (SRE) Workshop</title>
		<meeting><address><addrLine>Brno, Czech Republic</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2010-06" />
			<biblScope unit="page" from="24" to="25" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b108">
	<analytic>
		<title level="a" type="main">University of the Basque Country System for NIST 2010 Speaker Recognition Evaluation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Penagarikano</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Varona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Diez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">J</forename><surname>Rodriguez Fuentes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Bordel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">V Jornadas de Reconocimiento Biométrico de Personas</title>
		<meeting><address><addrLine>Huesca, Spain</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2010-09" />
			<biblScope unit="page" from="2" to="3" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b109">
	<monogr>
		<title level="m" type="main">Study of Different Backends in a State-Of-the-Art Language Recognition System</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Penagarikano</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Varona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Diez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">J</forename><surname>Rodriguez Fuentes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Bordel</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2012-09" />
			<biblScope unit="page" from="9" to="13" />
			<pubPlace>Portland, Oregon, USA</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b110">
	<analytic>
		<title level="a" type="main">A dynamic approach to the selection of high-order n-grams in phonotactic language recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Penagarikano</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Varona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Rodríguez-Fuentes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Bordel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ICASSP</title>
		<meeting>ICASSP<address><addrLine>Prague, Czech Republic</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="4412" to="4415" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b111">
	<analytic>
		<title level="a" type="main">Dimensionality Reduction for Using High-Order n-grams in SVM-Based Phonotactic Language Recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Penagarikano</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Varona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Rodriguez-Fuentes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Bordel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Interspeech 2011</title>
		<meeting>Interspeech 2011<address><addrLine>Florence, Italy</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="853" to="856" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b112">
	<analytic>
		<title level="a" type="main">Improved Modeling of Cross-Decoder Phone Co-occurrences in SVM-based Phonotactic Language Recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Penagarikano</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Varona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">J</forename><surname>Rodríguez-Fuentes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Bordel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Audio, Speech and Language Processing</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="2348" to="2363" />
			<date type="published" when="2011-11" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b113">
	<analytic>
		<title level="a" type="main">University of the Basque Country (EHU) Systems for the 2011 NIST Language Recognition Evaluation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Penagarikano</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Varona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">J</forename><surname>Rodriguez-Fuentes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Diez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Bordel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the NIST 2011 LRE Workshop</title>
		<meeting>the NIST 2011 LRE Workshop<address><addrLine>Atlanta (USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2011-12" />
			<biblScope unit="page" from="6" to="7" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b114">
	<monogr>
		<title level="m" type="main">The EHU Systems for the NIST 2011 Language Recognition Evaluation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Penagarikano</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Varona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">J</forename><surname>Rodriguez Fuentes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Diez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Bordel</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2012-09" />
			<biblScope unit="page" from="9" to="13" />
			<pubPlace>Portland, Oregon, USA</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b115">
	<monogr>
		<title level="m" type="main">Spectral Analysis for Physical Applications</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Walden</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1993" />
			<publisher>Cambridge University Press</publisher>
			<pubPlace>Cambridge, UK</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b116">
	<analytic>
		<title level="a" type="main">PLLR Features in Language Recognition System for RATS</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Plchot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Diez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">S L</forename><surname>Burget</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Interspeech</title>
		<meeting>Interspeech<address><addrLine>Singapore</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b117">
	<analytic>
		<title level="a" type="main">Speaker vectors from Subspace Gaussian Mixture Model as complementary features for Language Identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Plchot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Karafiát</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Brümmer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Glembek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Matejka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>De Villiers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Cernocký</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Odyssey 2012: The Speaker and Language Recognition Workshop</title>
		<meeting><address><addrLine>Singapore</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2012-06" />
			<biblScope unit="page" from="330" to="333" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b118">
	<analytic>
		<title level="a" type="main">Probabilistic Linear Discriminant Analysis for Inferences About Identity</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">J D</forename><surname>Prince</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">H</forename><surname>Elder</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b119">
	<analytic>
		<title level="a" type="main">The supersid project: exploiting high-level information for highaccuracy speaker recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Reynolds</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Andrews</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Campbell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Navratil</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Peskin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Adami</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Klusacek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Abramson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Mihaescu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Godfrey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Xiang</surname></persName>
		</author>
		<idno>IV-784-7</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings. (ICASSP &apos;03). 2003 IEEE International Conference on</title>
		<meeting>(ICASSP &apos;03). 2003 IEEE International Conference on</meeting>
		<imprint>
			<date type="published" when="2003-04" />
			<biblScope unit="volume">4</biblScope>
		</imprint>
	</monogr>
	<note>Acoustics, Speech, and Signal Processing</note>
</biblStruct>

<biblStruct xml:id="b120">
	<analytic>
		<title level="a" type="main">Speaker Verification Using Adapted Gaussian Mixture Models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">A</forename><surname>Reynolds</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">F</forename><surname>Quatieri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">B</forename><surname>Dunn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Digital Signal Processing</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="issue">1-3</biblScope>
			<biblScope unit="page" from="19" to="41" />
			<date type="published" when="2000-01" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b121">
	<analytic>
		<title level="a" type="main">Language recognition with discriminative keyword selection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Richardson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Campbell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ICASSP</title>
		<meeting>ICASSP</meeting>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="4145" to="4148" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b122">
	<analytic>
		<title level="a" type="main">NAP for high level language identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">S</forename><surname>Richardson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">M</forename><surname>Campbell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Acoustics, Speech, and Signal Processing</title>
		<meeting>the IEEE International Conference on Acoustics, Speech, and Signal Processing<address><addrLine>Prague, Czech Republic</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="4392" to="4395" />
		</imprint>
	</monogr>
	<note>ICASSP 2011</note>
</biblStruct>

<biblStruct xml:id="b123">
	<monogr>
		<title level="m" type="main">The Albayzin 2012 Language Recognition Evaluation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">J</forename><surname>Rodriguez Fuentes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Brümmer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Penagarikano</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Varona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Bordel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Diez</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2013-08" />
			<biblScope unit="page" from="25" to="29" />
			<pubPlace>Lyon, France</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b124">
	<monogr>
		<title level="m" type="main">The Albayzin 2012 Language Recognition Evaluation Plan</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">J</forename><surname>Rodriguez Fuentes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Brümmer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Penagarikano</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Varona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Diez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Bordel</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2012-11-23" />
			<pubPlace>madrid, spain.</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b125">
	<analytic>
		<title level="a" type="main">The Albayzin 2008 Language Recognition Evaluation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">J</forename><surname>Rodriguez-Fuentes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Penagarikano</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Bordel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Varona</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Odyssey 2010: The Speaker and Language Recognition Workshop</title>
		<meeting>Odyssey 2010: The Speaker and Language Recognition Workshop<address><addrLine>Brno, Czech Republic</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2010-07-01" />
			<biblScope unit="page" from="172" to="179" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b126">
	<analytic>
		<title level="a" type="main">KALAKA: A TV Broadcast Speech Database for the Evaluation of Language Recognition Systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">J</forename><surname>Rodriguez-Fuentes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Penagarikano</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Bordel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Varona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Diez</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the LREC 2010</title>
		<meeting>the LREC 2010<address><addrLine>Valleta, Malta</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2010-05" />
			<biblScope unit="page" from="17" to="23" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b127">
	<analytic>
		<title level="a" type="main">Overview of the Albayzin 2010 Language Recognition Evaluation: database design, evaluation plan and preliminary analysis of results</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">J</forename><surname>Rodriguez Fuentes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Penagarikano</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Varona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Diez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Bordel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">VI Jornadas en Tecnologías del Habla and II Iberian SLTech Workshop</title>
		<meeting><address><addrLine>Vigo, Spain</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2010-11" />
			<biblScope unit="page" from="10" to="12" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b128">
	<analytic>
		<title level="a" type="main">Bordel. The Albayzin 2010 Language Recognition Evaluation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">J</forename><surname>Rodriguez-Fuentes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Penagarikano</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Varona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Diez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Interspeech</title>
		<meeting>Interspeech<address><addrLine>Firenze, Italia</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="1529" to="1532" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b129">
	<analytic>
		<title level="a" type="main">KALAKA-2: a TV broadcast speech database for the recognition of Iberian languages in clean and noisy environments</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">J</forename><surname>Rodriguez-Fuentes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Penagarikano</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Varona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Diez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Bordel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the LREC</title>
		<meeting>the LREC<address><addrLine>Turkey</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2012-05" />
			<biblScope unit="page" from="23" to="25" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b130">
	<analytic>
		<title level="a" type="main">KALAKA-3: a database for the recognition of spoken European languages on YouTube audios</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">J</forename><surname>Rodriguez Fuentes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Penagarikano</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Varona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Diez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Bordel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 9th international conference on language resources and evaluation (LREC&apos;14)</title>
		<meeting>the 9th international conference on language resources and evaluation (LREC&apos;14)<address><addrLine>Reykjavik, Iceland</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2014-05" />
			<biblScope unit="page" from="26" to="31" />
		</imprint>
	</monogr>
	<note>European Language Resources Association (ELRA</note>
</biblStruct>

<biblStruct xml:id="b131">
	<analytic>
		<title level="a" type="main">The BLZ Systems for the 2011 NIST Language Recognition Evaluation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">J</forename><surname>Rodriguez Fuentes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Penagarikano</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Varona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Diez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Bordel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Abad</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Martinez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Villalba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Ortega</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Lleida</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIST 2011 Language Recognition Evaluation Workshop</title>
		<meeting><address><addrLine>Atlanta (USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2011-12" />
			<biblScope unit="page" from="6" to="7" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b132">
	<monogr>
		<title level="m" type="main">The BLZ Submission to the NIST 2011 LRE: Data Collection, System Development and Performance</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">J</forename><surname>Rodriguez Fuentes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Penagarikano</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Varona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Diez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Bordel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Abad</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Martinez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Villalba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Ortega</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Lleida</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2012-09" />
			<biblScope unit="page" from="9" to="13" />
			<pubPlace>Portland, Oregon, USA</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b133">
	<analytic>
		<title level="a" type="main">Multi-site Heterogeneous System Fusions for the Albayzin 2010 Language Recognition Evaluation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">J</forename><surname>Rodriguez-Fuentes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Penagarikano</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Varona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Diez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Bordel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Martínez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Villalba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Miguel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Ortega</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Lleida</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Abad</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Koller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Trancoso</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Lopez-Otero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Docio-Fernandez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Garcia-Mateo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Saeidi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Soufifar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Kinnunen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Svendsen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Franti</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Automatic Speech Recognition and Understanding Workshop (ASRU)</title>
		<meeting>the IEEE Automatic Speech Recognition and Understanding Workshop (ASRU)<address><addrLine>Hawaii, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2011-12" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b134">
	<analytic>
		<title level="a" type="main">Evaluation of Spoken Language Recognition Technology Using Broadcast Speech: Performance and Challenges</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">J</forename><surname>Rodriguez Fuentes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Varona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Diez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Penagarikano</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Bordel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Odyssey 2012: The Speaker and Language Recognition Workshop</title>
		<meeting><address><addrLine>Singapore</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b135">
	<analytic>
		<title level="a" type="main">Bordel. Spoken language recognition in conversational telephone speech and TV broadcast news (GLOSA)</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">J</forename><surname>Rodriguez Fuentes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Varona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Penagarikano</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Diez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">XXVI Congreso de la Sociedad Española para el Procesamiento de Lenguaje Natural (SEPLN)</title>
		<meeting><address><addrLine>Huelva, Spain</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2011-09" />
			<biblScope unit="page" from="5" to="7" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b136">
	<analytic>
		<title level="a" type="main">Cepstral Channel Normalization Techniques for HMM-based Speaker Verification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Rosenberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Soong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ICSPL</title>
		<meeting>ICSPL</meeting>
		<imprint>
			<date type="published" when="1994" />
			<biblScope unit="page" from="1835" to="1838" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b137">
	<monogr>
		<title level="m" type="main">Phoneme recognition based on long temporal context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Schwarz</surname></persName>
		</author>
		<ptr target="http://www.fit.vutbr.cz/" />
		<imprint>
			<date type="published" when="2008" />
			<pubPlace>Brno, Czech Republic</pubPlace>
		</imprint>
		<respStmt>
			<orgName>Faculty of Information Technology, Brno University of Technology</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">PhD thesis</note>
</biblStruct>

<biblStruct xml:id="b138">
	<analytic>
		<title level="a" type="main">Modeling prosodic feature sequences for speaker recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Shriberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Ferrer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kajarekar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Venkataraman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Stolcke</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Speech Communication</title>
		<imprint>
			<biblScope unit="volume">46</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="455" to="472" />
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b139">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Singer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">A</forename><surname>Torres-Carrasquillo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">A</forename><surname>Reynolds</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Mccree</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Richardson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Dejak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Sturim</surname></persName>
		</author>
		<title level="m">Odyssey 2012: The Speaker and Language Recognition Workshop</title>
		<meeting><address><addrLine>Singapore</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2012-06" />
			<biblScope unit="page" from="209" to="215" />
		</imprint>
	</monogr>
	<note>The MITLL NIST LRE 2011 Language Recognition System</note>
</biblStruct>

<biblStruct xml:id="b140">
	<analytic>
		<title level="a" type="main">A statistical model-based voice activity detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sohn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">S</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Sung</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Signal Processing Letters</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="3" />
			<date type="published" when="1999-01" />
			<publisher>IEEE</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b141">
	<analytic>
		<title level="a" type="main">Advances In Channel Compensation For SVM Speaker Recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Solomonoff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Campbell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Boardman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Acoustics, Speech, and Signal Processing</title>
		<imprint>
			<date type="published" when="2005-03" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="629" to="632" />
		</imprint>
	</monogr>
	<note>IEEE International Conference on</note>
</biblStruct>

<biblStruct xml:id="b142">
	<analytic>
		<title level="a" type="main">Regularized subspace n-gram model for phonotactic ivector extraction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Soufifar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Burget</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Plchot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Cumani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Cernocky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Interspeech</title>
		<meeting>Interspeech<address><addrLine>Lyon, France</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b143">
	<analytic>
		<title level="a" type="main">Discriminative Classifiers for Phonotactic Language Recognition with iVectors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Soufifar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Cumani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Burget</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">H</forename><surname>Cernocky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ICASSP</title>
		<meeting>ICASSP<address><addrLine>Kyoto, Japan</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="4853" to="4856" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b144">
	<analytic>
		<title level="a" type="main">SRILM -An extensible language modeling toolkit</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Stolcke</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Interspeech</title>
		<meeting>Interspeech</meeting>
		<imprint>
			<date type="published" when="2002-11" />
			<biblScope unit="page" from="257" to="286" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b145">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Strasheim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Brümmer</surname></persName>
		</author>
		<title level="m">NIST 2008 Speaker Recognition Evaluation Workshop Booklet</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
	<note>SUNSDV system description: NIST SRE</note>
</biblStruct>

<biblStruct xml:id="b146">
	<analytic>
		<title level="a" type="main">A Target-Oriented Phonotactic Front-End for Spoken Language Recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Tong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">S</forename><surname>Chng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Audio, Speech and Language Processing</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="1335" to="1347" />
			<date type="published" when="2009-09" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b147">
	<analytic>
		<title level="a" type="main">Selecting Phonotactic Features for Language Recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Tong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">S</forename><surname>Chng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Interspeech</title>
		<meeting>Interspeech</meeting>
		<imprint>
			<date type="published" when="2010-09" />
			<biblScope unit="page" from="737" to="740" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b148">
	<analytic>
		<title level="a" type="main">The MITLL NIST LRE 2007 language recognition system</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Torres-Carrasquillo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Singer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Campbell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Gleason</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Mccree</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Reynolds</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Richardson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Sturim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Interspeech</title>
		<meeting>Interspeech</meeting>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="719" to="722" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b149">
	<analytic>
		<title level="a" type="main">Approaches to Language Identification using Gaussian Mixture Models and Shifted Delta Cepstral Features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">A</forename><surname>Torres-Carrasquillo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Singer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">A</forename><surname>Kohler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">J</forename><surname>Greene</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">A</forename><surname>Reynolds</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">R</forename><surname>Deller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ICSLP</title>
		<meeting>ICSLP</meeting>
		<imprint>
			<date type="published" when="2002" />
			<biblScope unit="page" from="89" to="92" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b150">
	<analytic>
		<title level="a" type="main">Channel-dependent gmm and multi-class logistic regression models for language recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">A</forename><surname>Van Leeuwen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Brummer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Speaker and Language Recognition Workshop</title>
		<meeting><address><addrLine>Odyssey</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2006" />
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b151">
	<analytic>
		<title level="a" type="main">The COST278 pan-european broadcast news database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vandecatseye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-P</forename><surname>Martens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">P</forename><surname>Neto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Meinedo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Garcia-Mateo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dieguez-Tirado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Mihelic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zibert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Nouza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>David</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Pleva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Cizmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Papageorgiou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Alexandris</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of LREC</title>
		<meeting>LREC<address><addrLine>Lisbon, Portugal</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b152">
	<analytic>
		<title level="a" type="main">A Spoken Document Retrieval System for TV Broadcast News in Spanish and Basque</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Varona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Nieto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">J</forename><surname>Rodriguez Fuentes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Penagarikano</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Bordel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Diez</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">XXVI Congreso de la Sociedad Española para el Procesamiento de Lenguaje Natural (SEPLN)</title>
		<meeting><address><addrLine>Huelva, Spain</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2011-09" />
			<biblScope unit="page" from="5" to="7" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b153">
	<analytic>
		<title level="a" type="main">Handling i-vectors from different recording conditions using multi-channel simplified PLDA in speaker recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">A</forename><surname>Villalba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Lleida</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICASSP</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="6763" to="6767" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b154">
	<analytic>
		<title level="a" type="main">Shifted-Delta MLP Features for Spoken Language Recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-C</forename><surname>Leung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Signal Process. Lett</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="15" to="18" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b155">
	<analytic>
		<title level="a" type="main">Language Identification Using Efficient Gaussian Mixture Model Analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Wong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pelecanos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Myers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Sridharan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Australian International Conference on Speech Scienc and Tehcnology</title>
		<meeting>the Australian International Conference on Speech Scienc and Tehcnology</meeting>
		<imprint>
			<date type="published" when="2000" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b156">
	<monogr>
		<title level="m" type="main">The HTK Book (for HTK Version 3.4). Entropic, Ltd</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Young</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Evermann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Gales</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Hain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Kershaw</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Lui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Moore</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Odell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ollason</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Povey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Valtchev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Woodland</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2006" />
			<pubPlace>Cambridge, UK</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b157">
	<analytic>
		<title level="a" type="main">Data-driven generation of phonetic broad classes, based on phoneme confusion matrix similarity</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zgank</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Horvat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Kacic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Speech Comunication</title>
		<imprint>
			<biblScope unit="volume">47</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="379" to="393" />
			<date type="published" when="2005-09" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b158">
	<analytic>
		<title level="a" type="main">Robust Language Recogntion Based on Diverse Features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">H</forename><surname>Hansen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Odyssey 2014: The Speaker and Language Recognition Workshop</title>
		<meeting><address><addrLine>Joensuu, Finland</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2014-06" />
			<biblScope unit="page" from="152" to="157" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b159">
	<analytic>
		<title level="a" type="main">Comparison of Four Approaches to Automatic Language Identification of Telephone Speech</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zissman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Speech and Audio Processing</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="31" to="44" />
			<date type="published" when="1996-01" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b160">
	<analytic>
		<title level="a" type="main">Automatic language identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">A</forename><surname>Zissman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">M</forename><surname>Berkling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Speech Communication</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="115" to="124" />
			<date type="published" when="2001" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b161">
	<analytic>
		<title level="a" type="main">Speech database development at MIT: TIMIT and beyond</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Zue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Seneff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">R</forename><surname>Glass</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Speech Communication</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="351" to="356" />
			<date type="published" when="1990" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
