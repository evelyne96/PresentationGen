<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /mnt/sda1/Dissertation/grobid/grobid-0.5.6/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Context and motivation The language detection task Test conditions Data Organization Results Conclusions and current work The Albayzin 2008 Language Recognition Evaluation Odyssey 2010, Brno, Czech Republic</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2010-06-30">June 30, 2010</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luis</forename><forename type="middle">J</forename><surname>Rodríguez-Fuentes</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Electricity and Electronics</orgName>
								<orgName type="laboratory">Software Technologies Working Group Basque Country Barrio Sarriena s/n</orgName>
								<orgName type="institution">University</orgName>
								<address>
									<postCode>48940</postCode>
									<settlement>Leioa</settlement>
									<country key="ES">Spain</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mikel</forename><surname>Penagarikano</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Electricity and Electronics</orgName>
								<orgName type="laboratory">Software Technologies Working Group Basque Country Barrio Sarriena s/n</orgName>
								<orgName type="institution">University</orgName>
								<address>
									<postCode>48940</postCode>
									<settlement>Leioa</settlement>
									<country key="ES">Spain</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Germán</forename><surname>Bordel</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Electricity and Electronics</orgName>
								<orgName type="laboratory">Software Technologies Working Group Basque Country Barrio Sarriena s/n</orgName>
								<orgName type="institution">University</orgName>
								<address>
									<postCode>48940</postCode>
									<settlement>Leioa</settlement>
									<country key="ES">Spain</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amparo</forename><surname>Varona</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Electricity and Electronics</orgName>
								<orgName type="laboratory">Software Technologies Working Group Basque Country Barrio Sarriena s/n</orgName>
								<orgName type="institution">University</orgName>
								<address>
									<postCode>48940</postCode>
									<settlement>Leioa</settlement>
									<country key="ES">Spain</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Context and motivation The language detection task Test conditions Data Organization Results Conclusions and current work The Albayzin 2008 Language Recognition Evaluation Odyssey 2010, Brno, Czech Republic</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2010-06-30">June 30, 2010</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.5.6" ident="GROBID" when="2019-12-23T23:49+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>		<profileDesc>
			<abstract/>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Context and motivation</head><p>The language detection task Test conditions Data Organization Results</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Conclusions and current work</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Motivation</head><p>To promote collaboration between research groups from Spain and Portugal interested in language recognition</p><p>To provide a speech database specifically designed for language recognition applications featuring the official languages in Spain as target languages</p><p>To measure the accuracy that state-of-the-art systems can attain for the task of recognizing four target languages that have been in close contact from long time ago: will this task be more challenging than expected?</p><p>To measure the performance of systems developed on a limited amount of data </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Results: CF-30</head><p>Best performance in CF-30: Cavg = 0, 0552, meaning around 5% EER 5.45% EER obtained in independent experiments carried out with our own state-of-the-art system. The same system yielded below 3% EER in the general language recognition task defined in NIST 2007 LRE. Performance worse for this task than for the general task defined in NIST 2007 LRE Possible issues... Not the same task, not the same data (are results comparable?) Statistical significance (few errors, not many trials)</p><p>...and possible explanations:</p><p>Acoustic variability (speakers, channel, background noise)? Phonetic and lexical similarity among target languages?</p><p>In any case, the task seems to be challenging enough to allow further research in language recognition technology   </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>J. Rodríguez-Fuentes et al. The Albayzin 2008 LRE (Odyssey 2010, Brno, CZ)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" validated="false"><head></head><label></label><figDesc>Name: KALAKA (see paper at LREC 2010 for details) Four target languages: Spanish, Catalan, Basque and Galician Other languages (just to allow open-set tests): French, Portuguese, German and English Audio files: 16 kHz, single channel, 16 bits/sample, uncompressed PCM (WAV) Speech signals extracted from TV shows, including both planned and spontaneous speech in diverse environment conditions involving a varying number of speakers. Disjoint subsets of TV shows assigned to train, development and evaluation Size: around 50 hours (distributed in 3 DVD) Only high SNR speech: fragments containing medium-high level noise, music, speech overlaps, etc. filtered out Segments for training had no length restrictions Segments for development and evaluation: enclosed by a certain amount of low-energy frames 3-second subset ⊂ 10-second subset ⊂ 30-second subset length tolerance: 3-5, 10-12 and 30-33 seconds Proportions of unknown languages made deliberately different for development and evaluation, to avoid tuning systems to reject specific languages Proportion of French and Portuguese twice the proportion of German and English Distribution of unknown languages in development and evaluation For each test condition: single primary + any number of contrastive systems Results in NIST LRE format (text file with one line per trial and 6 fields per line) Participants committed to specify whether or not their scores may be interpreted as log-likelihood ratios Participants committed to send descriptions of their systems and present them at the Albayzin 2008 LRE workshop Systems ranked in each track according to Cavg Award: system yielding the least Cavg in the CR-30 condition Luis J. Rodríguez-Fuentes et al. September 2008 (for additional evaluation data) Luis J. Rodríguez-Fuentes et al.ResultsParticipation: 4 teams, 13 systems Two teams (T1: 6 systems and T2: 4 systems) applying state-of-the-art language recognition technology Average performance in the four test conditions (OF, OR, CF and CR) on the subset of 30-second segments Pooled DET curves of systems in the CR-30 test condition</figDesc><table>Context and motivation 
The language detection task 
Test conditions 
Data 
Organization 
Results 
Conclusions and current work 

Database features 

Context and motivation 
The language detection task 
Test conditions 
Data 
Organization 
Results 
Conclusions and current work 

Database design issues (I) 

Development dataset (same structure for evaluation): 

Total: 1800 segments 
600 segments per duration 

120 segments per target language and duration 
120 segments of unknown languages per duration 

Luis J. Rodríguez-Fuentes et al. 
The Albayzin 2008 LRE (Odyssey 2010, Brno, CZ) 

Context and motivation 
The language detection task 
Test conditions 
Data 
Organization 
Results 
Conclusions and current work 

Database design issues (II) 

# segments 
French 
Portuguese 
English 
German 
Devel 
70 
10 
40 
0 
Eval 
10 
70 
0 
40 

Luis J. Rodríguez-Fuentes et al. 
The Albayzin 2008 LRE (Odyssey 2010, Brno, CZ) 

Context and motivation 
The language detection task 
Test conditions 
Data 
Organization 
Results 
Conclusions and current work 

Evaluation rules (in brief) 

4 test conditions (OF, OR, CF, CR) × 3 durations: 12 tracks 

The Albayzin 2008 LRE (Odyssey 2010, Brno, CZ) 

Context and motivation 
The language detection task 
Test conditions 
Data 
Organization 
Results 
Conclusions and current work 

Schedule (as finally executed) 

Database production 

April-June 2008 

The Albayzin 2008 LRE (Odyssey 2010, Brno, CZ) 

Context and motivation 
The language detection task 
Test conditions 
Data 
Organization 
Results 
Conclusions and current work 

CR-30 (mandatory condition) 
CF-30 
OF-30 
Performance per target language 
Segment length 
Development conditions 

Cavg 
OF-30 
OR-30 
CF-30 
CR-30 
pri 
pri 
con 
pri 
pri 
con 
T1 
0,0946 
0,1313 0,1110 
0,0552 
0,0778 0,0656 
T2 
0,1204 
0,2787 
0,0556 
0,2420 
T3 
0,2597 0,5389 
T4 
0,5035 

Luis J. Rodríguez-Fuentes et al. 
The Albayzin 2008 LRE (Odyssey 2010, Brno, CZ) 

Context and motivation 
The language detection task 
Test conditions 
Data 
Organization 
Results 
Conclusions and current work 

CR-30 (mandatory condition) 
CF-30 
OF-30 
Performance per target language 
Segment length 
Development conditions 

Results: CR-30 (mandatory condition) 

0.2 
0.5 
1 
2 
5 
10 
20 
40 

0.2 

0.5 

1 

2 

5 

10 

20 

40 

False Alarm probability (in %) 

Miss probability (in %) 

Albayzin 2008 LRE CR−30 

Team4 
Team3 
Team2 
Team1 
Team1 alt 

Best primary (award winner): 
T1, Cavg = 0, 0778 

Best of all: T1-contrastive, 
Cavg = 0, 0656 

Luis J. Rodríguez-Fuentes et al. 
The Albayzin 2008 LRE (Odyssey 2010, Brno, CZ) 

Context and motivation 
The language detection task 
Test conditions 
Data 
Organization 
Results 
Conclusions and current work 

CR-30 (mandatory condition) 
CF-30 
OF-30 
Performance per target language 
Segment length 
Development conditions 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" validated="false"><head></head><label></label><figDesc>Luis J. Rodríguez-Fuentes et al. Best performance in OF-30: Cavg = 0, 0946, meaning around 9% EER Almost two times the EER in CF-30: impostor trials corresponding to unknown languages introduce a sizeable number of false alarms Some unknown languages are being confused with target languages, maybe Portuguese and French?Results: performance per target language DET curves for target languages (best systems in </figDesc><table>The Albayzin 2008 LRE (Odyssey 2010, Brno, CZ) 

Context and motivation 
The language detection task 
Test conditions 
Data 
Organization 
Results 
Conclusions and current work 

CR-30 (mandatory condition) 
CF-30 
OF-30 
Performance per target language 
Segment length 
Development conditions 

Results: OF-30 

Luis J. Rodríguez-Fuentes et al. 
The Albayzin 2008 LRE (Odyssey 2010, Brno, CZ) 

Context and motivation 
The language detection task 
Test conditions 
Data 
Organization 
Results 
Conclusions and current work 

CR-30 (mandatory condition) 
CF-30 
OF-30 
Performance per target language 
Segment length 
Development conditions 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" validated="false"><head></head><label></label><figDesc>Results: segment lengthPooled DET curves in the OF-30, OF-10 and OF-3 test conditions(best system) as expected, worse performance for shorter segments EER in OF-3 (around 20%) two times the EER in OF-30 (around 10%) similar results for other systems and conditions Luis J. Rodríguez-Fuentes et al. The Albayzin 2008 LRE (Odyssey 2010, Brno, CZ) Results: development conditions Pooled DET curves in the CF-30 and CR-30 test conditions for T1 and T2 systems</figDesc><table>Context and motivation 
The language detection task 
Test conditions 
Data 
Organization 
Results 
Conclusions and current work 

CR-30 (mandatory condition) 
CF-30 
OF-30 
Performance per target language 
Segment length 
Development conditions 

Results: performance per target language 

Error rates: Pmiss(i) in the diagonal, P f a (i, j) outside the diagonal (best 
system in CF-30) 

Target 

Spanish 
Catalan 
Basque 
Galician 

Spanish 
0.0750 
0.0167 
0.1250 
0.0833 

Catalan 
0.0083 
0.1167 
0.0083 
0.0000 

Basque 
0.0083 
0.0000 
0.0083 
0.0000 
Segment 

Galician 
0.1167 
0.0500 
0.0083 
0.1000 

Note. In the paper, error rates were mistaken as costs. An updated version can be 

downloaded from http://gtts.ehu.es (go to research and then to publications). 

Luis J. Rodríguez-Fuentes et al. 
The Albayzin 2008 LRE (Odyssey 2010, Brno, CZ) 

Context and motivation 
The language detection task 
Test conditions 
Data 
Organization 
Results 
Conclusions and current work 

CR-30 (mandatory condition) 
CF-30 
OF-30 
Performance per target language 
Segment length 
Development conditions 

Results: performance per target language 

Error rates: Pmiss(i) in the diagonal, P f a (i, j) outside the diagonal (best 
system in OF-30) 

Target 

Spanish 
Catalan 
Basque 
Galician 

Spanish 
0.0833 
0.0083 
0.0667 
0.0083 

Catalan 
0.0083 
0.1750 
0.0000 
0.0000 

Basque 
0.0083 
0.0000 
0.0250 
0.0000 

Galician 
0.1083 
0.0417 
0.0000 
0.1083 
Segment 

Unknown 
0.0667 
0.4333 
0.1083 
0.1417 

Luis J. Rodríguez-Fuentes et al. 
The Albayzin 2008 LRE (Odyssey 2010, Brno, CZ) 

Context and motivation 
The language detection task 
Test conditions 
Data 
Organization 
Results 
Conclusions and current work 

CR-30 (mandatory condition) 
CF-30 
OF-30 
Performance per target language 
Segment length 
Development conditions 

0.2 
0.5 
1 
2 
5 
10 
20 
40 

0.2 

0.5 

1 

2 

5 

10 

20 

40 

False Alarm probability (in %) 

Miss probability (in %) 

Albayzin 2008 LRE 
Best System at OF−30,10,3 

Context and motivation 
The language detection task 
Test conditions 
Data 
Organization 
Results 
Conclusions and current work 

CR-30 (mandatory condition) 
CF-30 
OF-30 
Performance per target language 
Segment length 
Development conditions 

0.2 
0.5 
1 
2 
5 
10 
20 
40 

0.2 

0.5 

1 

2 

5 

10 

20 

40 

False Alarm probability (in %) 

Miss probability (in %) 

Albayzin 2008 LRE Closed−Set, 30sec 

Team2 − Restricted 
Team2 − Free 
Team1 − Restricted 
Team1 − Free 

Better performance in 
free-development conditions 

Performance of T1 (blue) 
and T2 (red) systems not 
significantly different in 
CF-30, but... 

T1 restricted system yields 
40% worse Cavg 

T2 restricted system yields 
400% worse Cavg 
</table></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0" />
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Context and motivation The language detection task Test conditions Data Organization Results</p><p>Context and motivation The language detection task Test conditions Data Organization Results Conclusions and current work</p></div>			</div>
			<div type="references">

				<listBibl/>
			</div>
		</back>
	</text>
</TEI>
