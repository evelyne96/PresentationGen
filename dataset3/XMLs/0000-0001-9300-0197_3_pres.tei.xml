<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /mnt/sda1/Dissertation/grobid/grobid-0.5.6/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">RECURRENT NEURAL NETWORK IN NATURAL LANGUAGE PROCESSING</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sütő</forename><surname>Evelyne</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Babeş-Bolyai University</orgName>
								<address>
									<addrLine>Computer Science</addrLine>
									<settlement>Cluj-Napoca</settlement>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">RECURRENT NEURAL NETWORK IN NATURAL LANGUAGE PROCESSING</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.5.6" ident="GROBID" when="2019-12-23T23:47+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>		<profileDesc>
			<abstract/>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>PARAPHRASE GENERATION</head><p>• Paraphrasing, the act to express the same meaning in different possible ways.</p><p>• Paraphrase generation has been researched in <ref type="bibr">[Prakash et al., 2016]</ref> in 2016.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>• Conclusions</head><p>• Encoder-Decoder model with deep LSTM networks.</p><p>• They have reached state of the art results. <ref type="bibr">Graves, A. and Jaitly, N. (2014)</ref>.</p><p>Towards end-to-end speech recognition with recurrent neural networks.</p><p>In International Conference on Machine Learning, pages 1764-1772.</p><p>Nallapati, R., Zhou, B., Gulcehre, C., <ref type="bibr">Xiang, B., et al. (2016)</ref>.</p><p>Abstractive text summarization using sequence-to-sequence rnns and beyond. arXiv preprint arXiv:1602.06023.</p><p>Prakash, A., Hasan, S. A., Lee, K., <ref type="bibr">Datla, V., Qadir, A., Liu, J., and Farri, O. (2016)</ref>.</p><p>Neural paraphrase generation with stacked residual lstm networks. arXiv preprint arXiv:1610.03098. Sequence to sequence learning with neural networks.</p><p>In Advances in neural information processing systems, pages 3104-3112.</p><p>Wu, Y., Schuster, M., Chen, Z., Le, Q. V., Norouzi, M., Macherey, W., <ref type="bibr">Krikun, M., Cao, Y., Gao, Q., Macherey, K., et al. (2016)</ref>.</p><p>Google's neural machine translation system: Bridging the gap between human and machine translation. arXiv preprint arXiv:1609.08144.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 2 :Figure 3 :</head><label>23</label><figDesc>Image source: https://www.packtpub.com/mapt/book/Image source: https://geekyisawesome.blogspot.com/2016/10/</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>•</head><label></label><figDesc>Encoder-Decoder model with a bi-directional GRU-RNN as the encoder and a uni-directional GRU-RNN.• Address the problem of rarewords • They have reached state of the art results.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>Sutskever, I., Vinyals, O., and Le, Q. V. (2014).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" validated="false"><head></head><label></label><figDesc>NEURAL MACHINE TRANSLATION• Researchers have applied this model toEnglish-French   translation [Sutskever et al., 2014]  in 2014 • Conclusions • LSTM sequence to sequence models are able to map even very long sentences to the translation language • Deep LSTMs can significantly outperform shallow LSTMs • Unoptimized model was able to produce state of the art results with relatively short training • State of the art result: 37 BLEU score • This model's result: 36.5 • Researchers improved this unoptimized version [Wu et al., 2016] in 2016 to use for Google's translation system SPEECH RECOGNITION • Another application is shown in [Graves and Jaitly, 2014] done in 2014 for speech recognition system. • Conclusions • Bi-Directional LSTMs are needed to exploit future context as well. • Connectionist Temporal Classification • Model that does not need data preprocessing. • Their best score on 81 hour dataset (Wall street journal) is 8.2 while the baseline was 7.8 (word error rate/character error rate)TEXT SUMMARIZATION• Sequence to sequence model was used for text summarization in[Nallapati et al., 2016]  in 2016</figDesc><table>• Conclusions 

</table></figure>
		</body>
		<back>
			<div type="references">

				<listBibl/>
			</div>
		</back>
	</text>
</TEI>
