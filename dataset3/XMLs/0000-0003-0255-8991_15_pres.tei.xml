<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /mnt/sda1/Dissertation/grobid/grobid-0.5.6/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Introduction An Overview of the Albayzin LREs Albayzin LRE datasets SLR system Performance analysis Conclusions and future work Evaluation of Spoken Language Recognition Technology Using Broadcast Speech: Performance and Challenges Odyssey 2012, Singapore Odyssey 2012 Evaluation of SLR Technology Using Broadcast Speech</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2012-06-27">June 27, 2012</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luis</forename><forename type="middle">J</forename><surname>Rodríguez-Fuentes</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Electricity and Electronics</orgName>
								<orgName type="laboratory">Software Technologies Working Group Basque Country Barrio Sarriena s/n</orgName>
								<orgName type="institution">University</orgName>
								<address>
									<postCode>48940</postCode>
									<settlement>Leioa</settlement>
									<country key="ES">Spain</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amparo</forename><surname>Varona</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Electricity and Electronics</orgName>
								<orgName type="laboratory">Software Technologies Working Group Basque Country Barrio Sarriena s/n</orgName>
								<orgName type="institution">University</orgName>
								<address>
									<postCode>48940</postCode>
									<settlement>Leioa</settlement>
									<country key="ES">Spain</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mireia</forename><surname>Diez</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Electricity and Electronics</orgName>
								<orgName type="laboratory">Software Technologies Working Group Basque Country Barrio Sarriena s/n</orgName>
								<orgName type="institution">University</orgName>
								<address>
									<postCode>48940</postCode>
									<settlement>Leioa</settlement>
									<country key="ES">Spain</country>
								</address>
							</affiliation>
						</author>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mikel</forename><surname>Penagarikano</surname></persName>
							<email>mikel.penagarikano@ehu.es</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Electricity and Electronics</orgName>
								<orgName type="laboratory">Software Technologies Working Group Basque Country Barrio Sarriena s/n</orgName>
								<orgName type="institution">University</orgName>
								<address>
									<postCode>48940</postCode>
									<settlement>Leioa</settlement>
									<country key="ES">Spain</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Germán</forename><surname>Bordel</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Electricity and Electronics</orgName>
								<orgName type="laboratory">Software Technologies Working Group Basque Country Barrio Sarriena s/n</orgName>
								<orgName type="institution">University</orgName>
								<address>
									<postCode>48940</postCode>
									<settlement>Leioa</settlement>
									<country key="ES">Spain</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Introduction An Overview of the Albayzin LREs Albayzin LRE datasets SLR system Performance analysis Conclusions and future work Evaluation of Spoken Language Recognition Technology Using Broadcast Speech: Performance and Challenges Odyssey 2012, Singapore Odyssey 2012 Evaluation of SLR Technology Using Broadcast Speech</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2012-06-27">June 27, 2012</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.5.6" ident="GROBID" when="2019-12-23T23:49+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>		<profileDesc>
			<abstract/>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>(1) Different amount of training data to estimate models (purple vs. green)</p><p>(2) Portuguese and English (2010 LRE) less confused with the other languages than the average (green vs. blue)</p><p>(3) Task intrinsically more difficult in 2008 than in 2010, probably due to higher acoustic variability related to background noise (red vs. purple)</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Introduction An Overview of the Albayzin LREs</head><p>Albayzin LRE datasets SLR system Performance analysis Conclusions and future work Closed-set Clean-speech (CC) Open-set Clean-speech (OC) Noisy speech (Albayzin 2010 LRE)</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Closed-set Clean-speech (CC): confusion of languages with each other</head><p>Miss probabilities (diagonal) and false alarm probabilities (out of the diagonal) on the CC-3s condition of the Albayzin 2010 LRE </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Open-set Clean-speech (OC): comparison across evaluations</head><p>Again, performance on the 2008 LRE dataset much worse than on the 2010 LRE dataset (red dotted vs. blue dotted) -see details here   (1) SLR system built on clean and noisy speech signals: not specially optimized to deal with noisy speech</p><p>(2) Performance on the noisy-speech condition far worse than on the clean-speech condition (dotted vs. continuous) -see details here</p><p>(3) Moving from clean to noisy (continuous red to dotted red) produced higher degradation than moving from closed-set to open-set (continuous red to continuous blue)</p><p>(4) Performance on the Open-set Noisy-speech (ON) condition: between 2 and 6 times worse than in the Closed-set Clean-speech (CC) condition, depending on the nominal duration (the shorter the segments the smaller the differences in performance)</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>/NN phone decoders for Czech, Hungarian and Russian providing phone posteriors Phone lattices built on posteriors by means of HTK (BUT recipe) Expected counts of phone n-grams computed by means of SRILM (up to 3-grams, weighted counts) L2-regularized L1-loss SVM classification by means of LIBLINEAR Backend and Fusion Parameters optimized on the development set of Albayzin 2010 LRE and then applied to both 2008 and 2010 evaluation sets Gaussian backend applied only in the open-set condition Fusion/Calibration parameters estimated by linear logistic regression under a multiclass paradigm Minimum expected cost Bayes decisions based on the calibrated scores FoCal toolkit by Niko the 2008 LRE dataset much worse than on the 2010 LRE dataset (red vs. blue) -see details here</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>False</head><label></label><figDesc>Alarm probability (in %) Miss probability (in %) Albayzin 2010 LRE − closed Albayzin 2010 LRE − open Albayzin 2008 LRE − closed Albayzin 2008 LRE − open (1) Difference in performance for equivalent tasks (clean-speech, 30s) in 2008 and 2010 LRE: around 5 points in terms of EER (2) Albayzin 2010 LRE: larger training dataset, less confusable languages (on average)... (3) Similar differences in performance between open-set and closed-set for both datasets (dotted vs. continuous) Clean-speech (OC): confusion of languages with each other Miss probabilities (diagonal) and false alarm probabilities (out of the diagonal) on the OC-3s condition of the Albayzin 2010 LRE (including OOS segments)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" validated="false"><head></head><label></label><figDesc>Albayzin LRE: things that were different Two separate tracks depending on the data used to build systems: Albayzin 2008 LRE: KALAKA Segments containing background noise, music, speech overlaps, etc. filtered out OOS languages: French, Portuguese, English, GermanTraining: more than 8 hours per target language KALAKA fully recycled for KALAKA-2 New recordings, specially for Portuguese, English and OOS languages Noisy segments collected from existing and newly recorded materials Evaluation dataset completely new and independent of KALAKA OOS languages: Arabic, French, German, Romanian Training: more than 10 hours of clean speech and more than 2 hours of noisy speech per target language SLR system: phonotactic subsystems + backend/fusion</figDesc><table>Introduction 
An Overview of the Albayzin LREs 
Albayzin LRE datasets 
SLR system 
Performance analysis 
Conclusions and future work 

Albayzin 2008 LRE 

Target languages: Basque, Catalan, 
Galician, Spanish 

-restricted (only train and dev data 
provided for the evaluation) 
-free (any available data) 

Only clean speech 

Albayzin 2010 LRE 

Target languages: Basque, Catalan, 
Galician, Spanish, Portuguese, 
English 

Free development 
Two separate tracks depending on 
the background noise: 

-clean: only clean-speech test 
segments were considered 
-noisy: all the test segments 
(containing either clean or 
noisy/overlapped speech) were 
considered 

Separate sets of clean and 
noisy/overlapped speech segments 
provided for training 
Introduction 
An Overview of the Albayzin LREs 
Albayzin LRE datasets 
SLR system 
Performance analysis 
Conclusions and future work 

Spanish Catalan Basque 
Galician 

#segments 
282 
278 
342 
401 

time (minutes) 
529 
538 
531 
532 

Development and evaluation: 1800 segments each (600 per 
nominal duration, 120 per target language and 120 containing 
OOS languages) 

More than 50 hours of speech: 36 hours for training 
+ 7.7 hours for development + 7.7 hours for evaluation 
Introduction 
An Overview of the Albayzin LREs 
Albayzin LRE datasets 
SLR system 
Performance analysis 
Conclusions and future work 
Clean speech 
Noisy speech 
#segments 
time (minutes) 
#segments 
time (minutes) 
Basque 
406 
644 
112 
135 
Catalan 
341 
687 
107 
131 
English 
249 
731 
136 
152 
Galician 
464 
644 
125 
134 
Portuguese 
387 
665 
160 
197 
Spanish 
342 
625 
133 
222 

Development and evaluation: more than 150 segments per target language 
and nominal duration (4950 and 4992 segments, respectively) 
125 hours of speech: 82 hours for training + 21.24 hours for 
development + 21.43 hours for evaluation 
Introduction 
An Overview of the Albayzin LREs 
Albayzin LRE datasets 
SLR system 
Performance analysis 
Conclusions and future work 

Phonotactic subsystems 

</table></figure>
		</body>
		<back>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Conclusions (I)</head><p>Tasks defined for Albayzin 2008 LRE more challenging than those defined for Albayzin 2010 LRE, due to:</p><p>(1) Amount of training and development data (2) Average confusability of languages with each other (3) Intrinsic features of the evaluation datasets (acoustic variability) Closely related languages (e.g. Romance languages in Spain) the most confused OOS segments producing a strong impact on false alarm rates for all the target languages Highest degradation found when dealing with noisy speech</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Conclusions (II)</head><p>Most challenging conditions:</p><p>Background noise, conversations, etc. (outdoor environments)</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Similarity of target languages (dialects)</head><p>Amount of speech available to make decisions (short segments)</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Lack of training/development data (low-resource target languages)</head><p>Three possible setups proposed for future evaluations:</p><p>(1) Dialect recognition: intrinsically difficult, already addressed in NIST LRE</p><p>(2) Large-scale European language recognition: many closely related languages, collaboration of research groups throughout Europe required for data collection</p><p>(3) Language recognition in the wild: uncontrolled resources in the internet, small set of target languages, many/few/no training data</p></div>			</div>
			<div type="references">

				<listBibl/>
			</div>
		</back>
	</text>
</TEI>
