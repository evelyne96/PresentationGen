<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /mnt/sda1/Dissertation/grobid/grobid-0.5.6/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">AN ONLINE SPEAKER TRACKING SYSTEM FOR AMBIENT INTELLIGENCE ENVIRONMENTS</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maider</forename><surname>Zamalloa</surname></persName>
							<email>maider.zamalloa@ehu.es</email>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Department of Electricity and Electronics</orgName>
								<orgName type="department" key="dep2">Ikerlan -Technological Research Centre</orgName>
								<orgName type="institution" key="instit1">GTTS</orgName>
								<orgName type="institution" key="instit2">University of the Basque Country</orgName>
								<address>
									<settlement>Leioa, Mondragón</settlement>
									<country>Spain, Spain</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mikel</forename><surname>Penagarikano</surname></persName>
							<email>mikel.penagarikano@ehu.es</email>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Department of Electricity and Electronics</orgName>
								<orgName type="department" key="dep2">Ikerlan -Technological Research Centre</orgName>
								<orgName type="institution" key="instit1">GTTS</orgName>
								<orgName type="institution" key="instit2">University of the Basque Country</orgName>
								<address>
									<settlement>Leioa, Mondragón</settlement>
									<country>Spain, Spain</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luis</forename><surname>Javier Rodríguez-Fuentes</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Department of Electricity and Electronics</orgName>
								<orgName type="department" key="dep2">Ikerlan -Technological Research Centre</orgName>
								<orgName type="institution" key="instit1">GTTS</orgName>
								<orgName type="institution" key="instit2">University of the Basque Country</orgName>
								<address>
									<settlement>Leioa, Mondragón</settlement>
									<country>Spain, Spain</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Germán</forename><surname>Bordel</surname></persName>
							<email>german.bordel@ehu.es</email>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Department of Electricity and Electronics</orgName>
								<orgName type="department" key="dep2">Ikerlan -Technological Research Centre</orgName>
								<orgName type="institution" key="instit1">GTTS</orgName>
								<orgName type="institution" key="instit2">University of the Basque Country</orgName>
								<address>
									<settlement>Leioa, Mondragón</settlement>
									<country>Spain, Spain</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Juan</forename><forename type="middle">Pedro</forename><surname>Uribe</surname></persName>
							<email>jpuribe@ikerlan.es</email>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Department of Electricity and Electronics</orgName>
								<orgName type="department" key="dep2">Ikerlan -Technological Research Centre</orgName>
								<orgName type="institution" key="instit1">GTTS</orgName>
								<orgName type="institution" key="instit2">University of the Basque Country</orgName>
								<address>
									<settlement>Leioa, Mondragón</settlement>
									<country>Spain, Spain</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">AN ONLINE SPEAKER TRACKING SYSTEM FOR AMBIENT INTELLIGENCE ENVIRONMENTS</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.5.6" ident="GROBID" when="2019-12-23T23:49+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>		<profileDesc>
			<textClass>
				<keywords>
					<term>Speaker Tracking</term>
					<term>Ambient Intelligence</term>
					<term>AMI Corpus</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Ambient intelligence is an interdisciplinary paradigm which envisages smart spaces that provide services and adapt transparently to the user. As the most natural interface for human interaction, speech can be exploited for adaptation purposes in such scenarios. Low latency is required, since adaptation must be continuous. Most speaker tracking approaches found in the literature work offline, fully processing prerecorded audio files by a two-stage procedure: (1) performing acoustic segmentation and (2) assigning each segment a speaker label. In this work a real-time low-latency speaker tracking system is presented, which deals with continuous audio streams. Experimental results are reported on the AMI Corpus of meeting conversations, revealing the effectiveness of the proposed approach when compared to an offline speaker tracking system developed for reference.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>Ambient Intelligence (AmI) is an interdisciplinary applied research field, aiming to create smart spaces which provide services featuring user and context adaptation capabilities <ref type="bibr">(ISTAG, 2001)</ref>  <ref type="bibr" target="#b4">(Cook, 2009)</ref>. It was originally devised as Ubiquitous Computing in <ref type="bibr" target="#b14">(Weiser, 1991)</ref> where it was suggested the interaction of consumer electronics, telecommunications and computing devices to support people carrying out everyday life activities in a natural way. In such environment, daily objects feature computing and telecommunication capabilities. Transparency is critical, so natural and intelligent interfaces are needed for human-computer interaction <ref type="bibr" target="#b0">(Abowd, 2005)</ref>. Speech is a natural interface for human interaction and the most suitable means to support user interaction and adaptation. Speech streams can be exploited to extract user related information such as location, identity, etc. But in such environments, user adaptation must be continuous, and low-latency online processing is needed.</p><p>Speaker diarization and speaker tracking are well known tasks which aim to answer the question Who spokes when?. Speaker tracking aims to detect segments corresponding to a known set of target speakers <ref type="bibr" target="#b9">(Martin, 2001)</ref>. Speaker diarization consists of detecting speaker turns without any prior knowledge about the target speakers <ref type="bibr" target="#b13">(Tranter, 2006)</ref>  <ref type="bibr" target="#b10">(Meignier, 2006)</ref>. Speaker diarization and tracking primary applications domains assume that audio recordings are fully available before processing. Common approaches to these tasks consist of two uncoupled steps: (1) audio segmentation and (2) speaker detection. In speaker diarization, segments hypothetically uttered by the same speaker are clustered together. In speaker tracking, however, once the audio stream is segmented, speaker detection is carried out through classical speaker recognition techniques <ref type="bibr" target="#b11">(Moraru, 2005)</ref>  <ref type="bibr" target="#b5">(Istrate, 2005</ref><ref type="bibr" target="#b0">) (Bonastre, 2000</ref>. In any case, these methodologies are not suitable for low-latency online speaker detection.</p><p>Few works related to real-time speaker segmentation and tracking can be found in the literature <ref type="bibr" target="#b15">(Wu, 2003)</ref>  <ref type="bibr" target="#b7">(Lu, 2005)</ref>  <ref type="bibr" target="#b6">(Liu, 2005)</ref>. Most of the speaker segmentation approaches are based on metrics measuring spectral changes, such as Bayesian Information Criterion <ref type="bibr" target="#b3">(Chen, 1998) and</ref><ref type="bibr">Generalized Likelihood Ratio (Bonastre, 2000)</ref>  <ref type="bibr" target="#b6">(Liu, 2005)</ref>. These procedures are robust but computationally expensive since two or three Gaussian models must be estimated for scoring each possible change point in each analysis window, and there can be between 100 and 1000 analysis windows per second. In <ref type="bibr" target="#b15">(Wu, 2003)</ref>, a real-time model-based speaker change detection system is proposed, where a Universal Background Model (UBM) is taken as reference to classify speech segments, and a distance between two adjacent windows is computed which accounts for the spectral change. In <ref type="bibr" target="#b7">(Lu, 2005)</ref>, an unsupervised speaker segmentation and tracking algorithm is presented. Once the speaker change boundaries are determined, each segment is scored with a set of incremental quasi Gaussian Mixture Models corresponding to unknown target speakers. In <ref type="bibr" target="#b6">(Liu, 2005)</ref>, an online speaker adaptation methodology is applied for real-time speaker tracking, with unknown target speakers. This approach combines a phonotactic speaker change detection module with an online speaker clustering algorithm. Speaker adaptation is based on feature transformation. The transformation matrix is incrementally adapted as labeled segments become available.</p><p>In this paper, a real-time low-latency online speaker tracking approach is presented, designed for an AmI scenario (for example, an intelligent home environment), where the system continuously tracks known speakers. The expected number of target speakers is low (i.e. the members of a family). This scenario requires taking almost instantaneous (low latency) speaker tracking decisions. A very simple speaker tracking algorithm is proposed, where audio segmentation and speaker detection are jointly accomplished by defining and processing fixedlength audio segments and scoring each of them to decide whether it belongs to a target speaker or to an impostor. Audio segments are scored by means of acoustic models (corresponding to target speakers) estimated via Maximum a Posteriori (MAP) adaptation of a UBM <ref type="bibr" target="#b12">(Reynolds, 2000)</ref>. The MAP-UBM methodology yields good speaker recognition performance and allows for a fast scoring technique which speeds up the score computation. Finally, detection scores are calibrated (i.e linearly mapped to likelihood ratios) and then, based on the scores obtained for a development corpus, an optimal application-dependent decision threshold (that minimizes the expected error cost) is established <ref type="bibr" target="#b1">(Brummer, 2006)</ref>. The performance of the proposed approach is compared to that of an offline system developed for reference, which follows the classical two-stage approach: audio segmentation is done over the whole input stream, and MAP-UBM speaker detection is performed on the resulting segments. Speaker tracking experiments applying both systems were carried out on the AMI Corpus <ref type="bibr" target="#b2">(Carletta, 2007)</ref>, which contains human conversations in the context of smart meeting rooms, close to the AmI scenario described above.</p><p>The rest of the paper is organized as follows. In section 2 the main features of the speaker tracking systems are described, including the acoustic frontend, the audio segmentation (required for the offline reference system) and the speaker detection and calibration stages. Section 3 gives details about the experimental corpus and the UBM estimation. Speaker tracking results using the proposed and the reference systems are presented in section 4. Finally, conclusions and guidelines for future work are given in Section 5.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">SPEAKER TRACKING SYSTEMS</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Acoustic Front-End</head><p>In this work, 16 kHz audio streams are analyzed in frames of 20 milliseconds, at intervals of 10 milliseconds. A Hamming window is applied and a 512-point FFT computed. The FFT amplitudes are then averaged in 24 overlapped triangular filters, with central frequencies and bandwidths defined according to the Mel scale. A Discrete Cosine Transform is finally applied to the logarithm of the filter amplitudes, obtaining 12 Mel-Frequency Cepstral Coefficients (MFCC). To increase robustness against channel distortion, Cepstral Mean Normalization (CMN) is applied. When the audio stream is processed on-the-fly, a dynamic CMN approach is applied, the cepstral mean being updated at each time t as follows:</p><formula xml:id="formula_0"> t =C t 1− t −1<label>(1)</label></formula><p>where α is a time constant, C(t) is the vector of cepstral coefficients at time t and µt-1 is the dynamic cepstral mean at time t-1. After CMN, the first derivatives of the MFCC are also computed, yielding a 24-dimensional feature vector.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Audio Segmentation</head><p>Audio segmentation, also known as acoustic change detection, is required by most speaker tracking systems as a previous step to the detection of target speakers. A simple algorithm is applied in this work, which segments the audio signal in a fully unsupervised way, by locating the most likely change points from a purely acoustic point of view. The algorithm considers a sliding window W of N acoustic vectors and computes the likelihood of change at the center of that window, then moves the window K vectors ahead and repeats the process until the end of the vector sequence. To compute the likelihood of change, each window is divided in two halves, Wleft and Wright, then a Gaussian distribution (with diagonal covariance matrix) is estimated for each half and finally the cross-likelihood ratio is computed and stored as likelihood of change. This yields a sequence of cross-likelihood ratios which must be post-processed to get the hypothesized segment boundaries. This involves applying a threshold τ and forcing a minimum segment size δ.</p><p>In practice, a boundary t is validated when its crosslikelihood ratio exceeds τ and there is no candidate boundary with greater ratio in the interval [t-δ,t+δ] (see <ref type="bibr">(Rodriguez, 2007)</ref> for details).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Speaker Detection</head><p>The real-time speaker tracking system proposed in this work computes a detection score per target speaker and outputs a speaker identification decision at fixed-length intervals. That length has been empirically set to one second, which provides relatively good time resolution and spectral richness, and a reasonably small latency for most online speaker tracking scenarios. The offline system developed for reference does the same computation, but using the segments produced by the algorithm described in Section 2.2. Regardless the way audio segments are obtained, they are scored with the same set of MAP-UBM target speaker models <ref type="bibr" target="#b12">(Reynolds, 2000)</ref>.</p><p>In the adaptation of a speaker model from the UBM, only non-overlapped training segments (i.e. those segments containing only speech from that speaker, according to the time references of manual annotations) are used. This way, component densities related to the acoustic classes strongly observed in training data will change, whereas component densities that correspond to weaker or missing acoustic units (such as silence or impostors) will remain un-adapted. Therefore, it is assumed that the resulting MAP-UBM system should be able to detect speech from target speakers and reject silence, noise and speech from impostors. Given the acoustic model λs for the target speaker s and λUBM for the UBM, the detection score ∆s(X) is computed as follows:</p><formula xml:id="formula_1">∆s(X)= L(X|λs) -L(X|λUBM)<label>(2)</label></formula><p>where L(X|λ) is the log-likelihood of X given λ.</p><p>Once the detection scores are computed for all the target speakers, a unique identification decision is made per segment: X is marked as coming from the most likely target speaker s* = arg maxsϵ[1,S]{∆s(X)}, if ∆s*(X) &gt; θ. Otherwise X is marked as coming from an impostor. The decision threshold θ can be heuristically established to optimize the discrimination. Note that, for any given segment X, there could actually be two or more speakers speaking at the same time. However, the detection approach described above cannot inform of speaker overlaps, because only the most likely speaker can be detected.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4">Calibration of scores</head><p>Calibration maps detection scores {∆s | s ∈ [1,S]} to likelihood ratios {C(∆s) | s ∈ [1,S]} without any specific application in mind. The scaling parameters are computed over a development corpus by maximizing Mutual Information, which is equivalent to minimizing the so called CLLR (a metric defined in <ref type="bibr" target="#b1">(Brummer, 2006)</ref>), which integrates the expected cost over a wide range of operation points (representing specific applications) in the Detection Error Tradeoff (DET) curve <ref type="bibr" target="#b8">(Martin, 1997)</ref>. The final decision is taken by applying the minimum expected cost Bayes decision threshold to calibrated scores C(∆). The target speaker is accepted only if the following inequality holds:</p><formula xml:id="formula_2">C ≥ln  C fa 1− P target  C miss P target <label>(3)</label></formula><p>where Cmiss and Cfa are miss and false-acceptance error costs, and Ptarget the prior probability of target speakers. Scores are calibrated by means of the FoCal toolkit, applying a linear mapping strategy (see http://www.dsp.sun.ac.za/~nbrummer/focal/).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">EXPERIMENTAL SET-UP</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">The AMI Corpus</head><p>Experiments are carried out over the AMI Corpus of meeting conversations, available as a public resource (see http://corpus.amiproject.org/). The AMI Corpus is a multimodal dataset concerned with real-time human interaction in the context of smart meeting rooms. Data, collected in three instrumented meeting rooms, include a range of synchronized audio and video recordings. Meetings contain speech in English, mostly from non native speakers. In this work, the development and evaluation of speaker tracking systems is based on a subset of the AMI Corpus, the Edinburgh scenario meetings, including 15 sessions: ES2002-ES2016, with four meetings per session, each meeting being half an hour long on average. Training data are taken from meetings recorded at the three AMI sites. The audio stream is obtained by mixing the signals from the headset microphones of the participating speakers. Three of the four speakers participating in each session are taken as target speakers, the remaining one being assigned the role of impostor. Careful impostor selection -not random-is made to account for gender unbalanced sessions. In sessions containing just one female speaker, the impostor is forced to be male (and vice versa), in order to avoid that gender favors impostor discrimination.</p><p>In order to assess the speaker tracking performance in realistic conditions, two independent subsets are defined, consisting of different sessions (and therefore different speakers), for development and evaluation purposes, respectively. The development set, consisting of 8 sessions (32 meetings), is used to tune the configuration parameters of the speaker tracking systems. The evaluation set, including the remaining 7 sessions (28 meetings), is used only to evaluate the performance of the previously tuned speaker tracking systems.</p><p>Both the development and evaluation subsets are further divided into train and test datasets. Two meetings per session are randomly selected for training speaker models, and the remaining two are left for testing purposes. Time references are based on manual annotations provided in the AMI Corpus.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">UBM estimation</head><p>Two speaker detection systems have been developed based on the MAP-UBM approach. They only differ in the data used to estimate the UBM: UBM-g uses 15 gender-balanced AMI meetings from all sites except Edinburgh (so, a kind of room mismatch may be expected), whereas UBM-t uses only speech from target speakers in training meetings. UBM-g is estimated once and can be applied to whatever evaluation data and target speakers, whereas UBM-t must be estimated specifically for each set of target speakers.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Performance measures</head><p>The performance of speaker tracking systems is commonly analyzed by means of Detection Error Tradeoff (DET) plots <ref type="bibr" target="#b8">(Martin, 1997)</ref>. Performance is measured in terms of time that is correctly or incorrectly classified as belonging to a target. Therefore, miss and false alarm rates are computed as a function of time <ref type="bibr" target="#b9">(Martin, 2001)</ref> and not as a function of trial number, like in speaker detection experiments.</p><p>DET performance can be summarized in a single figure by means of the Equal Error Rate (EER), the point of the DET curve at which miss and false alarm rates are equal. Obviously, the lower the EER, the higher the accuracy of a speaker tracking system.</p><p>Another way to summarize in a single figure the performance of a speaker tracking system is the so called F-measure, defined as follows:</p><formula xml:id="formula_3">F = 2 . 0 * PRC * RCL PRC RCL<label>(4)</label></formula><p>where precision (PRC) and recall (RCL) are related to false alarm and miss rates respectively. PRC measures the correctly detected target time from the total target time detected. RCL computes the correctly detected target time from the actual target time. The F-measure ranges from 0 to 1, with higher values indicating better performance. Collar periods of 250 milliseconds at the end of speaker turns are ignored for scoring purposes. Thus, speaker turns of less than 0.5 seconds are not scored. <ref type="figure" target="#fig_0">Figures 1 and 2</ref> show the performance of the online and offline speaker tracking systems, using UBM-g and UBM-t as background models, for the development and evaluation sets, respectively. Since the speaker detection strategy followed in this work cannot detect speaker overlaps, all the segments containing speech from two or more speakers are removed when scoring test meetings.  As expected, the classical offline system outperforms the proposed low-latency online system, but the performance of the latter is quite good. Taking the performance of the offline system as reference, in speaker tracking experiments over the evaluation set, the EER increases from 25.80 to 26.20 (1.55% relative degradation) when using UBM-g, and from 19.07 to 21.14 (10.85% relative degradation) when using UBM-t. On the other hand, UBM-t systems outperform UBM-g systems, maybe due to the aforementioned room mismatch in UBMg and the limited amount of training data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">EXPERIMENTAL RESULTS</head><p>In addition, performance degradation from development to evaluation is small (from around 20% to 21% EER) in MAP-UBM-t, which means that system configuration (based on the development set) was also suitable for the evaluation set. The MAP-UBM-g system suffers a bigger degradation from development to evaluation. It seems that estimating the UBM from unknown speakers in mismatched conditions (different rooms) degrades acoustic coverage and reduces the robustness of system configuration with regard to using a roomspecific UBM estimated from target speakers. The UBM-t system might be getting advantage not only from matching the room, but also from the consistency between the speakers in the UBM and the target speakers. In fact, 100% of the target speakers appearing in the test corpus contribute data to the UBM-t, increasing the consistency of speaker models estimated through MAP adaptation (because a perfect match exists between the adaptation data corresponding to any target speaker and some of the component densities of the UBM). <ref type="table">Table 1</ref> shows precision (PRC), recall (RCL) and F-measure performance of the speaker tracking systems, for both the calibrated and uncalibrated speaker detection scores. These results correspond to the operation point (threshold) considered optimal in the DET curve. The threshold used for calibrated scores is based on application-dependent costs and target priors, adjusted on the development corpus. For uncalibrated scores, the threshold is fixed to zero, i.e. a target speaker is detected if the likelihood of the null hypothesis is higher than that of the alternative hypothesis.  Results in <ref type="table">Table 1</ref> demonstrate the usefulness of the calibration stage, which leads to better performance in all cases. Finally, note that the realtime (online, low-latency) system provides only slightly worse performance than the reference (offline) system: 1.7% average relative degradation in F-measure. Though speaker tracking actually takes advantage from an offline acoustic segmentation of the audio stream, depending on the scenario and the required latency, offline audio segmentation would not be feasible. In such a situation, the proposed approach provides real-time low-latency online speaker tracking at the cost of little performance degradation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">CONCLUSIONS AND FUTURE WORK</head><p>In this paper, an online speaker tracking system, designed for an Ambient Intelligence scenario, is presented an evaluated. The system processes continuous audio streams and outputs a speaker identification decision for fixed-length (one second) segments. Speaker detection is done by means of a MAP-UBM speaker verification backend. A calibration stage is applied which linearly maps detection scores to likelihood ratios. Calibration parameters are estimated beforehand based on development data, yielding significant performance improvements without increasing the computational cost, which is crucial for a real-time low-latency system. An alternative speaker tracking system, based on an offline segmentation of the audio stream has been developed and evaluated for reference.</p><p>Experiments have been carried out on a subset of the AMI Corpus of meeting conversations. Results demonstrate that better results can be attained when the UBM is estimated from data matching test conditions (same room, same speakers), instead of using general but unrelated data. The calibration stage provides performance improvements in all cases. Finally, offline segmentation of audio streams actually improves speaker tracking performance with regard to using fixed-length segments. However, depending on the scenario and the required latency, offline audio segmentation would not be feasible. The proposed system provides realtime low-latency online speaker tracking with little performance degradation.</p><p>Current work involves increasing the robustness of detection scores (and decisions) by using information from past segments. Future work includes using detection scores in a speaker verification framework (thus allowing the detection of multiple speakers), and making a smart use of all the available data through new UBM estimation strategies.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>DET performance of speaker tracking systems on the development set defined on the AMI corpus.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>DET performance of speaker tracking systems on the evaluation set defined on the AMI corpus.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>1 :</head><label>1</label><figDesc>Precision (PRC), Recall (RCL) and F-measure performance of the real-time (rt) and reference (ref) speaker tracking systems, using UBM-g and UBM-t, on the development (Dev) and evaluation (Eval) sets.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" validated="false"><head>Table</head><label></label><figDesc></figDesc><table></table></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ACKNOWLEDGEMENTS</head><p>This work has been partially funded by the Government of the Basque Country, under program SAIOTEK, project S-PE07IK03; and the Spanish MICINN, under Plan Nacional de I+D+i, project TIN2009-07446.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">A Speaker Tracking System based on Speaker Turn Detection for NIST Evaluation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">D</forename><surname>Abowd</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">D</forename><surname>Mynatt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">F</forename><surname>Bonastre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Delacourt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Fredouille</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Merlin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Wellekens</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceeding of the IEEE International Conference on Acoustics, Speech and Signal Processing</title>
		<editor>D.J. Cook and S.K. Das</editor>
		<meeting>eeding of the IEEE International Conference on Acoustics, Speech and Signal essing<address><addrLine>Istanbul, Turkey</addrLine></address></meeting>
		<imprint>
			<publisher>Wiley</publisher>
			<date type="published" when="2000" />
			<biblScope unit="page" from="153" to="174" />
		</imprint>
	</monogr>
	<note>Smart Environments: Technology, Protocols, and Applications</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Application Independent Evaluation of Speaker Detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Brummer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Preez</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computer Speech and Language</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="page" from="230" to="275" />
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Unleashing the killer corpus: experiences in creating the multi-everything AMI Meeting Corpus</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Carletta</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Language Resources and Evaluation Journal</title>
		<imprint>
			<biblScope unit="volume">41</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="181" to="190" />
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Speaker, Environment and Channel Change Detection and Clustering via the Bayesian Information Criterion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">S</forename><surname>Gopalakrishnan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of DARPA Broadcast News Transcription and Understanding Workshop</title>
		<meeting>DARPA Broadcast News Transcription and Understanding Workshop</meeting>
		<imprint>
			<date type="published" when="1998" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Scenarios for Ambient Intelligence in</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">J</forename><surname>Cook</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">C</forename><surname>Augusto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><forename type="middle">R</forename><surname>Jakkula</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Ambient Intelligence: Technologies, Applications, and Opportunities</title>
		<imprint>
			<date type="published" when="2001" />
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page" from="277" to="298" />
		</imprint>
	</monogr>
	<note>European Commission Report</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Broadcast News Speaker Tracking for ESTER 2005 Campaign</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Istrate</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Scheffer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Fredouille</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">F</forename><surname>Bonastre</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Speech and Language Processing</title>
		<meeting>the International Conference on Speech and Language Processing<address><addrLine>Lisboa</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Online Speaker Adaptation and Tracking for Real-time Speech Recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Kiecza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Kubala</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Speech and Language Processing</title>
		<meeting>the International Conference on Speech and Language Processing<address><addrLine>Lisboa</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Unsupervised Speaker Segmentation and Tracking in Real-time Audio Content Analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">J</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Multimedia Systems</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page" from="332" to="343" />
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">The DET curve in assessment of detection task performance</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Martin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Doddington</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Kamm</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ordowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Przybocki</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 5th European Conference on Speech Communication and Technology</title>
		<meeting>the 5th European Conference on Speech Communication and Technology</meeting>
		<imprint>
			<date type="published" when="1997" />
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page" from="1895" to="1898" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Speaker Recognition in a Multi-Speaker Environment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">F</forename><surname>Martin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">A</forename><surname>Przybocki</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 7th European Conference on Speech Communication and Technology (Eurospeech)</title>
		<meeting>the 7th European Conference on Speech Communication and Technology (Eurospeech)<address><addrLine>Denmark</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2001" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Step-by-step and Integrated Approaches in Broadcast News Speaker Diarization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Meignier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Moraru</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Fredouille</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">F</forename><surname>Bonastre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Besacier</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computer Speech and Language</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="page" from="303" to="330" />
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Experiments on Speaker Tracking and Segmentation in Radio Broadcast News</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Moraru</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ben</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Gravier</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Speech and Language Processing</title>
		<meeting>the International Conference on Speech and Language Processing<address><addrLine>Lisboa</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">A Simple But Effective Approach to Speaker Tracking in Broadcast News</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">A</forename><surname>Reynolds</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">F</forename><surname>Quatieri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">B</forename><surname>Dunn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">J</forename><surname>Rodríguez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Peñagarikano</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Bordel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Pattern Recognition and Image Analysis</title>
		<imprint>
			<publisher>Springer-Verlag</publisher>
			<date type="published" when="2000" />
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page" from="48" to="55" />
		</imprint>
	</monogr>
	<note>LNCS</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">An Overview of Automatic Speaker Diarization Systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">E</forename><surname>Tranter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">A</forename><surname>Reynolds</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Audio, Speech and Language Processing</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="1557" to="1565" />
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">The Computer for the Twenty-First Century</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Weiser</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Scientific American</title>
		<imprint>
			<biblScope unit="page" from="94" to="104" />
			<date type="published" when="1991" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">UBM-based Real-time Speaker Segmentation for Broadcasting News</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">J</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Acoustics, Speech and Signal Processing</title>
		<meeting>the IEEE International Conference on Acoustics, Speech and Signal Processing<address><addrLine>China</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
