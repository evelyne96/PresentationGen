<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /mnt/sda1/Dissertation/grobid/grobid-0.5.6/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Multi-agent reinforcement learning</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Schnebli</forename><surname>Zoltán</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Faculty of Mathematics and Computer Science</orgName>
								<orgName type="institution">Babeş-Bolyai University</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Multi-agent reinforcement learning</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.5.6" ident="GROBID" when="2019-12-23T23:44+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>		<profileDesc>
			<abstract/>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Exploration vs. exploitation</head><p>• ϵ -greedy strategy  </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>•</head><label></label><figDesc>⟨S, A, P . (·, ·), R . (·, ·), γ⟩ • S -set of states • A -set of actions • Pa(s, s ′ ) -probability of reaching state s' • Ra(s, s ′ ) -value of the reward if we go to s' • γ -discount factorPartially observable Markov decision process• ⟨S, A, P . (·, ·), R . (·, ·), γ, Ω, O(·, ·)⟩ • S, A, T, R, γ • Ω -set of all observations • Oa(o,s′ ) -probability of getting observation o Markov games • N agents • A := {A1, A2, ..., An} • O := {O1, O2, ..., On} • It is the most general modell Deep deterministic policy gradient algorithm with generative cooperative policy network • Every agent has 3 policies • Q-network -&gt; optimal action during execution • Greedy policy network -&gt; maximizes the global objective based on the local actions • Generative cooperative policy newtork -&gt; learn other agents policies</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" validated="false"><head></head><label></label><figDesc>• opponent sampling -random old opponent better • exploration curriculum -dense reward at the beginning to learn basic motor skills faster • interesting behaviors: blocking, rising arms, charging, kicking high, etc.</figDesc><table>Emergent Complexity via Multi-Agent Competition 

• goal: get complex agent behavior from simple environments 

• ideea: self-play 
Environments 

• Run to Goal 

• You Shall Not Pass 

• Sumo 

• Kick and Defend 
</table></figure>
		</body>
		<back>
			<div type="references">

				<listBibl/>
			</div>
		</back>
	</text>
</TEI>
