<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /mnt/sda1/Dissertation/grobid/grobid-0.5.6/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">KALAKA: a TV Broadcast Speech Database for the Evaluation of Language Recognition Systems</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luis</forename><forename type="middle">J</forename><surname>Rodríguez-Fuentes</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Electricity and Electronics</orgName>
								<orgName type="laboratory">Software Technologies Working Group Basque Country Barrio Sarriena s/n</orgName>
								<orgName type="institution">University</orgName>
								<address>
									<postCode>48940</postCode>
									<settlement>Leioa</settlement>
									<country key="ES">Spain</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mikel</forename><surname>Penagarikano</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Electricity and Electronics</orgName>
								<orgName type="laboratory">Software Technologies Working Group Basque Country Barrio Sarriena s/n</orgName>
								<orgName type="institution">University</orgName>
								<address>
									<postCode>48940</postCode>
									<settlement>Leioa</settlement>
									<country key="ES">Spain</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Germán</forename><surname>Bordel</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Electricity and Electronics</orgName>
								<orgName type="laboratory">Software Technologies Working Group Basque Country Barrio Sarriena s/n</orgName>
								<orgName type="institution">University</orgName>
								<address>
									<postCode>48940</postCode>
									<settlement>Leioa</settlement>
									<country key="ES">Spain</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amparo</forename><surname>Varona</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Electricity and Electronics</orgName>
								<orgName type="laboratory">Software Technologies Working Group Basque Country Barrio Sarriena s/n</orgName>
								<orgName type="institution">University</orgName>
								<address>
									<postCode>48940</postCode>
									<settlement>Leioa</settlement>
									<country key="ES">Spain</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mireia</forename><surname>Díez</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Electricity and Electronics</orgName>
								<orgName type="laboratory">Software Technologies Working Group Basque Country Barrio Sarriena s/n</orgName>
								<orgName type="institution">University</orgName>
								<address>
									<postCode>48940</postCode>
									<settlement>Leioa</settlement>
									<country key="ES">Spain</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">KALAKA: a TV Broadcast Speech Database for the Evaluation of Language Recognition Systems</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.5.6" ident="GROBID" when="2019-12-24T15:33+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>A speech database, named KALAKA, was created to support the Albayzin 2008 Evaluation of Language Recognition Systems, organized by the Spanish Network on Speech Technologies from May to November 2008. This evaluation, designed according to the criteria and methodology applied in the NIST Language Recognition Evaluations, involved four target languages: Basque, Catalan, Galician and Spanish (official languages in Spain), and included speech signals in other (unknown) languages to allow open-set verification trials. In this paper, the process of designing, collecting data and building the train, development and evaluation datasets of KALAKA is described. Results attained in the Albayzin 2008 LRE are presented as a means of evaluating the database. The performance of a state-of-the-art language recognition system on a closed-set evaluation task is also presented for reference. Future work includes extending KALAKA by adding Portuguese and English as target languages and renewing the set of unknown languages needed to carry out open-set evaluations.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>A speech database, named KALAKA, was designed, collected and built with the aim to support the Albayzin 2008 Evaluation of Language Recognition Systems (http://jth2008.ehu.es/en/albayzin.html) organized by the Spanish Network on Speech Technologies from May to November 2008. Hereafter, we will refer to this evaluation as Albayzin 2008 LRE. This evaluation was designed according to the criteria and methodology applied in NIST Language Recognition Evaluations (http://www.itl.nist.gov/iad/mig/tests/lre/). In particular, the Evaluation Plan for the 2007 NIST LRE was taken as reference <ref type="bibr" target="#b9">(Martin and Le, 2008)</ref>. There is, however, a significant difference: NIST LRE materials were extracted from spontaneous conversations through telephone (narrow-band) channels involving two speakers, whereas those of KALAKA were extracted from (wide-band) TV shows, including both planned and spontaneous speech in diverse environment conditions involving a varying number of speakers. Various types of TV shows were recorded, with prevalence of broadcast news, talk shows and debates. The database was named after the talk show Kalaka (which could be translated to English as offensive or annoying talk), broadcast by the Basque channel ETB1. Training data provided in KALAKA allows to build language recognition systems with four target languages: Basque, Catalan, Galician and Spanish. These are all official languages in Spain, though only Spanish is spoken in the whole territory, whereas the other three are spoken (with different usage levels) in specific regions. In any case, remarkable differences have been observed between planned speech produced by professional speakers in broadcast news and spontaneous speech produced by peo-This work has been supported by the Government of the Basque Country, under program SAIOTEK (project S-PE09UN47), and the Spanish MICINN, under Plan Nacional de I+D+i (project TIN2009-07446, partially financed by FEDER funds). ple in interviews. In particular, Spanish features several regional dialects, some of them reflecting features <ref type="bibr">(pronunciation, intonation, words, syntactic forms, etc.)</ref> inherited from yet extinct Iberian languages, and others reflecting features imported from Basque, Catalan or Galician, which at the same time have historically received a strong influence from Spanish. So, the task of recognizing these four target languages could be more challenging than expected. In fact, one of the goals of the evaluation was to measure the accuracy that state-of-the-art language recognition systems could attain for this task.</p><p>Development and evaluation data include utterances not only in target languages but also in other languages (unknown from the point of view of the application), so that open-set evaluations can be carried out. Those unknown languages (English, French, Portuguese and German) have been chosen attending to the availability of TV channels, with a higher presence of French and Portuguese, which may increase task difficulty, since these two languages are assumed to share some features with Catalan and Galician, respectively.</p><p>The training set contains around 9 hours of speech per target language, which amounts to around 36 hours of training data. The development and evaluation sets contains around 7.7 hours of speech each one, with the same distribution: more than 90 minutes of speech per target language and more than 90 minutes of speech in other (unknown) languages. The whole database amounts to around 50 hours of speech and is distributed (after direct request to the authors) in three DVD.</p><p>The rest of the paper is organized as follows. The design of the database and the recording setup are addressed in Sections 2 and 3, respectively. Section 4 describes how the recorded materials were processed and organized, including classification of recordings, selection of speech materials, extraction of fixed (nominal) length segments and encoding of filenames. Section 5 summarizes results attained in Albayzin 2008 LRE and presents a state-of-the-art language recognition system developed and evaluated on KALAKA. Finally, conclusions and future work are outlined in Section 6.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Design issues</head><p>We started from the following two considerations:</p><p>1. In order to avoid undesired variabilities, recording conditions (channel and audio processing) should be the same for all the languages, meaning that a single recording setup (devices, connectors, audio conversions, etc.) should be defined.</p><p>2. With regard to other sources of variability <ref type="bibr">(environment, speaker, etc.)</ref>, each language should contain as much diversity as possible, thus minimizing (or at least, averaging) the effect of such factors in the evaluation.</p><p>We chose cable TV, in particular, that provided by Euskaltel (http://www.euskaltel.com) in the Basque Country, because it gives easy access to audio in different languages: Spanish from several generalist and regional channels; Basque, Catalan and Galician from the corresponding regional channels in the Basque Country, Catalonia, Valencia (a region in eastern Spain where a variation of catalan is spoken) and Galicia; and English, German, French, Portuguese, etc. from international channels.</p><p>In order to foster data independence and make the evaluation as robust as possible, disjoint subsets of TV shows were assigned to train, development and evaluation. This way, each subset still contains a diverse choice of speakers, but the probability of finding the same speaker in two subsets (and therefore, the risk of modeling, optimizing for or matching the speaker and not the language) is very low.</p><p>Training materials had no constraints regarding duration, whereas development and evaluation data followed the guidelines of NIST LRE, by defining three evaluation subsets according to the nominal duration of speech segments: 30, 10 and 3 seconds, respectively. This allowed to measure the recognition performance as a function of the available amount of speech. Obviously, it was expected that the shorter the speech segment, the lower the accuracy in recognizing the spoken language.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Recording setup</head><p>Recordings were done through a home connection to cable TV, by means of a digital audio recorder.</p><p>A Roland Edirol R-09 ultra-light audio recorder was chosen, with the following features (see http://www.roland.com/products/en/R-09 for further details): up to 24 bit / 48 kHz linear PCM and up to 320 kbps MP3 recording, SD card direct storage, built-in stereo microphone, mic and line audio inputs and high speed file transfer through USB 2.0. CD quality (16 bit / 44.1 kHz / stereo) recordings were done by connecting the audio output of the cable TV decoder to the line input of the R-09. The resulting files were stored in WAV format for further processing.</p><p>Audio signals were downsampled to 16 kHz, left and right channels being averaged into one single channel, by means of SoX (Sound eXchange: http://sox.sourceforge.net/). This way, storage requirements were reduced in a factor of 5.51, while keeping an acceptable (wide-band) quality for speech processing applications.</p><p>Filenames were given according to the following pattern: Most recordings were done from April 18th to May 2nd 2008. After that time, in order to complete the evaluation dataset, a few additional recordings were done from August 15th to September 13th 2008. As explained in next section, recordings were filtered and many segments discarded because of high noise levels, speech overlaps, etc. So, the size of recorded materials was around 3 times the size of speech segments finally used in KALAKA. <ref type="table" target="#tab_1">Table 1</ref> shows the TV channels used for the recordings and the recorded time for each language. Recorded time for all languages amounts to 138 hours.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Creating the database</head><p>Audio files were processed in four steps: (1) classification (according to contents); (2) selection of speech segments;</p><p>(3) automatic extraction of 30-, 10-and 3-second speech segments (needed for the development and evaluation datasets); and (4) generation of encoded filenames (hiding language information). The last step was necessary because information about language (present in conventional filenames) had to be hidden to sites participating in the Albayzin 2008 LRE.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Classification of recordings</head><p>The recording process included taking notes about each TV show: type (news, talk show, debate, etc.), duration, environment conditions, rate of speech overlaps, etc. This information was used to distribute TV shows into train, development and evaluation datasets, keeping in mind the diversity and independence conditions: (1) the three datasets should contain similar proportions of show types; and (2) all the recordings of a given TV show should be posted to the same dataset.</p><p>TV shows were classified in six categories: (1) debates and interviews;</p><p>(2) talk-shows; (3) news; (4) sports; (5) entertaining (contests, reality shows, etc.); and (6) documentaries. Recorded time (absolute and relative) of the six types of TV shows for the target languages is presented in <ref type="table" target="#tab_2">Table  2</ref>. To allow a good characterization of target languages, debates and interviews (which feature a high rate of clean non-overlapped speech from many speakers) were most of them posted to the train dataset.</p><p>As noted above, speech data were recorded not only for the four target languages, but also for other languages (unknown from the point of view of the application), with the only aim to carry out open-set evaluations, not to train models for them. Obviously, training models on other languages would improve acoustic coverage and help verification, but we cannot assume that such data will be available in practice. On the other hand, training (and using) models for languages actually appearing in the evaluation dataset would violate the assumption that they were unknown. And finally, training (and using) models for languages not appearing in the evaluation dataset may help but may also damage verification. So, TV shows in English, German, French and Portuguese were posted only to the development and evaluation datasets. Their relative distribution was designed according to the percentages given in <ref type="table" target="#tab_3">Table 3</ref>. Proportions of unknown languages were deliberately different for development and evaluation, to avoid tuning systems to reject specific languages. On the other hand, in order to make things even more difficult, due to the relative proximity of French to Catalan, and Portuguese to Galician, the proportion of these languages was twice the proportion of English and German.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Selection of speech segments</head><p>Fragments containing noisy speech, music, speech overlaps, etc. were discarded.</p><p>Only speech segments with a low level of background noise were validated for KALAKA. This task was performed by means of Wavesurfer <ref type="bibr" target="#b15">(Sjolander and Beskow, 2000)</ref> (http://www.speech.kth.se/wavesurfer/), which allows listening to and looking at audio signals, selecting segments and storing them. As a result, speech segments of indefinite length (each segment spoken in a single language) were extracted from recorded materials and stored in WAV files.</p><p>The main part of this task was performed by three members of the research team, from May to July 2008. Additional materials were also processed by one of the researchers in September 2008. After discussing and determining the selection criteria (for the resulting segments to be as homogeneous as possible), each researcher selected materials in a fully autonomous way, and the resulting files were pooled for further processing. For intermediate storage, filenames were generated by adding a three-digit number to the name of the source file. This way, the first speech segment extracted from the file LaNitAlDia 20080317 ca.wav was named LaNitAl-Dia 20080317 ca 001.wav, the second speech segment was named LaNitAlDia 20080317 ca 002.wav, etc.</p><p>No further processing was applied to speech segments posted to the train dataset. The number of training segments per target language, as well as their total approximate du-  <ref type="table" target="#tab_4">Table 4</ref>. Speech segments posted to development and evaluation datasets were taken as source to extract spech segments of fixed (nominal) durations of 30, 10 and 3 seconds, according to the criteria given in Section 4.3. The resulting WAV files were stored in the corresponding folders (train, devel and eval), with conventional names consisting of the sequence LLCDDXXX.wav, where LL is the international language code (es, ca, eu, gl, de, fr, en, pt), C is the dataset identifier (t, d, e), DD is the duration code (00: undefined, 03: 3 seconds, 10: 10 seconds, 30: 30 seconds), and XXX is a three-digit number. This way, cat00023.wav represents the 23th speech segment of undefined duration in Catalan in the train dataset; ptd30011.wav represents the 11th 30-second speech segment in Portuguese in the development dataset; and eue10143.wav represents the 143rd 10-second speech segment in Basque in the evaluation dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Automatic extraction of 30-, 10-and 3-second segments</head><p>As noted above, speech segments posted to development and evaluation were taken as source to extract segments of fixed duration (30, 10 and 3 seconds), according to the following criteria:</p><p>1. Speech segments must be enclosed by a certain amount of silence (i.e. low-energy frames), which is included as part of the segments. This way, it is expected to catch natural segments and to avoid cutting words.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">A 30-second segment is validated if and only if it</head><p>contains a valid 10-second segment. Similarly, a 10second segment is validated if and only if it contains a 3-second segment.</p><p>3. Segments can be slightly longer (but not shorter) than their nominal duration: 3-second segments are allowed to last up to 5 seconds; 10-second segments are allowed to last up to 12 seconds; and 30-second segments are allowed to last up to 33 seconds.</p><p>A single-pass greedy approach was applied, which looked for 30-second segments fulfilling the three conditions given above. First, the whole file was taken as input and nonoverlaped 30-second speech segments were searched and stored for further processing. Next, each 30-second segment found in the above search was validated, as follows: (1) non-overlapped 10-second segments inside the 30-second segment were searched and stored;</p><p>(2) each 10second speech segment was processed the same way, by looking for non-overlapped 3-second speech segments; and (3) as soon as a 3-second speech segment was found inside a 10-second segment, the validation procedure ended, all the intermediate files were deleted and time marks of the 30-, 10-and 3-second segments were stored.</p><p>The search for d-second speech segments works as follows: (1) the input signal is processed in overlapped frames of 100 milliseconds, with a frame step (time resolution) of 10 milliseconds; (2) frame energies are computed and stored; (3) low-energy fragments are implicitly defined by two heuristically fixed energy thresholds (for start and end) and are required to last more than 100 milliseconds; and (4) a greedy search is applied which extracts non-overlapped segments lasting from d to d + k seconds, forced to begin and end at low-energy fragments, k being a tolerance parameter.</p><p>On average, this algorithm retrieved 65% of the input speech. Note that two additional files of 10 and 3 seconds were produced for each 30-second segment located by the algorithm: each 3-second segment was part of a 10-second segment, which in turn was part of a 30-second segment.</p><p>Since 30-, 10-and 3-second evaluation subsets were built on the same materials, performance differences measured on these subsets should be attributed, almost exclusively, to the varying amount of available speech.</p><p>The development dataset consists of 1800 speech segments, distributed in three subsets, each containing 600 segments of 30, 10 and 3 seconds, respectively. Each subset consists of 120 segments per target language and 120 additional segments spoken in unknown languages, following the distribution shown in <ref type="table" target="#tab_3">Table 3</ref>: 70 segments spoken in French, 10 in Portuguese and 40 in English. The evaluation dataset has the same structure, except for the distribution of unknown languages, which, according to <ref type="table" target="#tab_3">Table 3</ref>, consists of 10 segments spoken in French, 70 in Portuguese and 40 in German.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">Encoding filenames</head><p>For the sake of completeness, we briefly address here the algorithm applied to encode conventional filenames. The algorithm was designed according to the following conditions:</p><p>1. Encoding must be reversible: the conventional filename must be recoverable from the encoded filename.</p><p>2. The encoded filename will be generated from three data: the conventional filename, file contents and a password.</p><p>3. The conventional filename will be recovered from three data: the encoded filename, file contents and the same password used to produce the encoded filename.</p><p>4. The conventional filename must match the structure described in section 4.2.</p><p>5. The encoded filename will consist of a seemingly random string of 8 hexadecimal digits (followed by the .wav extension).</p><p>All the speech files of KALAKA (around 5000), also including those of the train dataset, were given an encoded filename. Encoding consisted on applying an exclusive or (XOR) to a 4-byte string derived from the conventional filename (through a bidirectional translation table), the password and a SHA-1 hash computed on file contents. The same algorithm was applied to recover the conventional filename, taking advantage of the reversibility property of the XOR (if c = a XOR b, then b = a XOR c) and inverting the correspondence between 4-byte strings and conventional filenames. Using a SHA-1 hash implies that there is no one-to-one correspondence between conventional and encoded filenames, i.e. two conventional filenames may produce the same encoded filename. However, the probability of such an event is very low in a set of 5000 filenames, since there are (2 4 ) 8 = 2 32 potential encodings. In any case, different passwords could be applied until no collision was found. In practice, no collision was detected when applying this algorithm with various passwords.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Using the database</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.">The Albayzin 2008 LRE</head><p>Following NIST evaluations <ref type="bibr" target="#b9">(Martin and Le, 2008)</ref>, the Albayzin 2008 LRE involved independent language verification trials for a set of 4 target languages: Basque, Catalan, Galician and Spanish. Given a test utterance S and a target language L, the task consisted on deciding whether or not L was actually spoken in S. Besides the decision, a score should be provided for each trial, the higher the score the greater the confidence that the target language was spoken in the segment. Three evaluation subsets of 30-, 10-and 3-second speech segments, two development conditions (restricted vs. free) and two evaluation modes (closed-set vs. open-set) were considered. Restricted development implied that only those materials provided in KALAKA could be used to build the system, and external materials could be used neither directly nor indirectly. For instance, acoustic models trained on an external acoustic database were not allowed. Free development allowed using any kind and amount of materials. Closed-set evaluation assumed that only target languages could be spoken in test utterances. Open-set evaluation relaxed that assumption by allowing any (known or unknown) language to be spoken in test utterances. System performance was measured by presenting a set of trials, each trial consisting of a pair (test utterance, target language), and then computing the C avg cost function <ref type="bibr" target="#b9">(Martin and Le, 2008)</ref>, which depends on the miss and false alarm error rates, language priors (P target , P non−target and P Out−Of −Set ) and application dependent costs (C miss and C f a ). The C avg function was computed separately for the three evaluation subsets of 30-, 10-and 3-second segments, for the restricted and free development conditions and for the closed-set and open-set evaluation modes. For those sites indicating that their scores could be interpreted as log-likelihood ratios, an alternative peformance measure was also computed, the so called C LLR <ref type="bibr" target="#b2">(Brümmer and du Preez, 2006)</ref>, which does not depend on application costs. Finally, Detection Error Tradeoff (DET) curves <ref type="bibr" target="#b10">(Martin et al., 1997)</ref> were computed (using NIST software 1 ) to visualize and compare global performance of systems, including both actual and minimum C avg operation points. The Albayzin 2008 LRE presented an award for the system yielding the least C avg in the restricted-condition closed-set evaluation on the subset of 30-second speech segments. DET curves of four primary (continuous line) and one contrastive (dotted line) systems participating in that competition are shown in <ref type="figure" target="#fig_0">Figure 1</ref>. The C avg attained by the most competitive systems (corresponding to two undisclosed participants) are shown in <ref type="table" target="#tab_5">Table 5</ref>. Note that, though state-of-the-art technology was employed, results reveal that the proposed task was more challenging than expected, the best systems yielding a C avg of around 0.05 (roughly corresponding to 5% EER) in the free-development closed-set evaluation on 30-second segments, and around 0.09 (roughly corresponding to 9% EER) in the free-development open-set evaluation on 30second segments. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.">Developing language recognition technology</head><p>NIST evaluations have promoted the creation and use of large multilingual narrow-band (telephone channel) speech databases, supporting the development of language recognition technology as a preprocessing step for the automatic transcription of telephone conversations in some interesting languages. Few multilingual wide-band speech databases are available, and none of them includes the official languages in Spain. Creating KALAKA was moti- vated just by the lack of a multilingual speech database featuring the official languages in Spain. Using wideband (16 kHz, single channel) broadcast recordings made sense since we were primarily interested in building a language recognition module for the backend of an audio indexing and retrieval system dealing with wide-band broadcast news in Spanish and Basque (which is likely to be extended to broadcast news in Catalan and Galician) <ref type="bibr" target="#b1">(Bordel et al., 2009)</ref>. Open-set language recognition was needed because broadcast news often include fragments in foreign languages (French, English, Arabic, etc.) that must be discarded for automatic transcription and indexing. Therefore, KALAKA was designed and is being used in our research group for both basic and applied research on language recognition.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.1.">The main language recognition system</head><p>A language recognition system has been built starting from the train and development sets of KALAKA and the materials implicitly used to built phone decoders (see below). Performance has been measured, in terms of C avg and DET curves, on the evaluation set of KALAKA. The system consists of a hierarchical fusion of 7 individual subsystems: an acoustic GMM-SVM subsystem using 7-2-3-7 SDC-MFCC, three Phone-SVM subsystems and three Phone-LM subsystems (see descriptions below). In order to make it easier for other researchers to verify our results, open software resources were used to build all the subsystems.</p><p>For the GMM-SVM subsystem, acoustic models were estimated using the Sautrela toolbox <ref type="bibr" target="#b12">(Penagarikano and Bordel, 2005)</ref>. The phonotactic systems were based on the phone decoders developed and made available by the Brno University of Thechnology (BUT) for Czech, Hungarian and Russian <ref type="bibr" target="#b14">(Schwarz, 2008)</ref>. BUT decoders have been previously used by other groups −besides BUT , the MIT Lincoln Laboratory <ref type="bibr" target="#b16">(Torres-Carrasquillo et al., 2008)</ref>− as the backend for phonotactic language recognition, yielding high recognition accuracies. Each BUT decoder runs its own acoustic front-end, so it can be seen as a black box which takes a speech signal as input and gives the 1-best phone decoding as output. The Phone-LM subsystems applied the SRI Language Model toolkit <ref type="bibr">(Stolcke, 2002)</ref> to estimate phone sequence n-gram models. Finally, all the subsystems based on Support Vector Machines (SVM) <ref type="bibr" target="#b5">(Campbell et al., 2006a)</ref>  <ref type="bibr" target="#b6">(Campbell et al., 2006b)</ref> were developed using either SVMTorch <ref type="bibr" target="#b8">(Collobert and Bengio, 2001)</ref> or libSVM <ref type="bibr" target="#b7">(Chang and Lin, 2001)</ref>, for dense and sparse vectors, respectively.</p><p>Scores produced by language recognition subsystems were first normalized, by means of a t-norm <ref type="bibr" target="#b0">(Auckenthaler et al., 2000)</ref>, and then calibrated, by means of a Gaussian backend. Finally, normalized and calibrated scores were fused by applying linear logistic regression optimization. A minimum expected cost Bayes decision threshold was then established, according to the application-dependent language priors and costs −see  and <ref type="bibr" target="#b4">(Brümmer et al., 2007)</ref> for details.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.2.">The GMM-SVM subsystem</head><p>The GMM-SVM subsystem applies a SVM classifier on the vector space defined by Gaussian Mixture Model (GMM) parameters. The GMM corresponding to a target language is constructed by using training samples of that language to adapt the means of a Universal Background Model (UBM) consisting of 1024 mixture components. Maximum A Posteriori (MAP) adaptation is performed using a relevance factor of τ = 16. The adapted means are normalized and stacked to construct the so called GMM supervectors which feed the SVM classifier <ref type="bibr" target="#b6">(Campbell et al., 2006b)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.3.">Phonotactic subsystems</head><p>As noted above, the phonotactic subsystems were based on the Brno University of Technology (BUT) TRAPS/NN decoders for Czech, Hungarian and Russian. These decoders were designed to process 8 kHz raw PCM signals. Therefore, the original 16 kHz signals were downsampled to 8 kHz. Prior to phone tokenization, an energy based Voice Activity Detector (VAD) was used to split and remove lowenergy (presumably non-speech) segments from the signals. Non-phonetic units appearing in phone sequences were all mapped to silence, leading to inventories of 43, 59 and 49 phonetic units for Czech, Hungarian and Russian, respectively. Two different phone sequence modeling techniques were applied:</p><p>• Phone-LM: 4-gram language models with Witten-Bell smoothing.</p><p>• Phone-SVM: SVM (with a linear kernel), built on bag-of-N-gram vectors (including up to 3-grams), weighted as proposed in . <ref type="table" target="#tab_6">Table 6</ref> shows the performance (C avg ) of single and fused systems on the closed-set evaluation subset of 30-second speech segments. DET curves for the GMM-SVM subsystem, the Phone-LM fused system, the Phone-SVM fused system and the main system (fusing all the previous systems) are shown in <ref type="figure" target="#fig_1">Figure 2</ref>. The performance of the main  system (C avg = 0.0576) is similar to that of the most competitive systems submitted to the Albayzin 2008 LRE (in the free-development condition). In any case, taking into account that state-of-the-art technology has been applied, these results confirm that the task defined on KALAKA is quite challenging and may help further developments in language recognition technology.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.4.">Results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Conclusions and future work</head><p>In this paper, we have addressed the design, data collection and evaluation of KALAKA, a database consisting of wideband (16 kHz) audio signals taken from TV broadcasts, created and used specifically for the Albayzin 2008 Language Recognition Evaluation, which was carried out from May to November 2008. The database includes train, development and evaluation materials for four target languages: Basque, Catalan, Galician and Spanish (official languages in Spain). It also includes speech signals in other languages to allow open-set verification trials. Results attained in the Albayzin 2008 LRE have been presented as a means of evaluating the database. Preliminary results using various state-of-the-art language recognition sub-systems and the system resulting from their fusion have been also presented to provide more evidences of the difficulty of the task. Taking into account the performance attained in both cases, we can conclude that tasks defined on KALAKA can be challenging enough to support further developments in language recognition technology. Future work will focus on preparing an extended version of KALAKA to support a second evaluation this year, the Albayzin 2010 LRE, using again wide-band (16 kHz) TV broadcast speech signals, but including also Portuguese and English as target languages and renewing the set of unknown languages. We hope this new feature will make the evaluation more appealing for research teams from outside Spain. The evaluation would be held from June to October 2010 and results would be presented at the 6th Biennial Workshop on Speech Technology, to be held in Vigo (Spain) in November 2010. If things go as we expect, the evaluation plan would be posted through ISCA in June 2010.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Pooled DET curves of systems participating in the restricted-development closed-set evaluation on 30-second speech segments. Operation points corresponding to actual (X) and minimum (O) C avg are marked on the curves.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Pooled DET curves of various systems: GMM-SVM (blue), Phone-LM (green), Phone-SVM (red) and the system fusing all of them (black), on the closed-set evaluation subset of 30-second speech segments. Operation points corresponding to actual (X) and minimum (O) C avg are marked on the curves.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" validated="false"><head>&lt;TVshow&gt; [</head><label>&lt;TVshow&gt;</label><figDesc>TVchannel] &lt;date&gt; &lt;language&gt;.wav</figDesc><table>Date consisted of a sequence of numbers of the form 
yyyymmdd (year, month and day), left padded with zeros 
if necessary (for instance, 20080503 represents May 3rd, 
2008). International codes were used to denote language: 
es (Spanish), ca (Catalan), eu (Basque), gl (Galician), en 
(English), de (German), fr (French) and pt (Portuguese). 
The TV channel was added only when, for a given lan-
guage, TV shows from different channels were recorded, 
or when the name of the show was not descriptive enough 
(as in the case of news). Here we present some examples: 

NoticiasTVCanaria 20080319 es.wav 
ElTiempoAndaluciaTV 20080421 es.wav 
ElTiempoTeleMadrid 20080423 es.wav 
EuromaxxDWTV 20080331 en.wav 
HardTalkBBCWorld 20080317 en.wav 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" validated="true"><head>Table 1 :</head><label>1</label><figDesc>TV channels and recorded time (in minutes) for each language in KALAKA.</figDesc><table>Language TV Channels 
Recorded time 

Spanish 

TVE1, La 2, 
La Sexta, Cuatro, 
Tele5, Antena3, ETB2, 
TV Canaria Sat, 
AndalucíaTV, 
TeleMadrid 

1818 

Catalan 
TVCi 
1777 
Basque 
ETB1 
1905 
Galician 
TVG 
1731 
German 
DWTV 
275 
French 
TV5Monde Europe 
320 
English 
DWTV, BBCWorld 
257 
Portuguese RTPi 
218 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" validated="true"><head>Table 2 :</head><label>2</label><figDesc>Recorded time, absolute (minutes) and relative (%), of the six types of TV shows for the target languages.</figDesc><table>Spanish 
Catalan 
Basque 
Galician 
Debates and interviews 
495 -27.23 
499 -28.08 
631 -33.12 
515 -29.75 
Talk-shows 
500 -27.50 
428 -24.09 
498 -26.14 
642 -37.09 
News 
353 -19.42 
336 -18.91 
341 -17.90 
405 -23.40 
Sports 
126 -6.93 
120 -6.75 
120 -6.30 
17 -0.98 
Entertaining 
230 -12.65 
249 -14.01 
153 -8.03 
83 -4.79 
Documentaries 
114 -6.27 
145 -8.16 
162 -8.50 
69 -3.99 
Total 
1818 -100.00 1777 -100.00 1905 -100.00 1731 -100.00 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" validated="true"><head>Table 3 :</head><label>3</label><figDesc>Planned distribution of data (%) for unknown languages.</figDesc><table>Dev 
Eval 
Total 
German 
0.00 16.67 16.67 
French 
29.17 4.16 
33.33 
English 
16.67 0.00 
16.67 
Portuguese 4.16 29.17 33.33 
Total 
50.00 50.00 100.00 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" validated="true"><head>Table 4 :</head><label>4</label><figDesc>Number and total duration of training segments for the target languages in KALAKA.</figDesc><table>Spanish Catalan Basque Galician 
All 
Number of segments 
282 
278 
342 
401 
1303 
Total duration (minutes) 
529 
538 
531 
532 
2130 

ration (in minutes), are shown in </table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5" validated="true"><head>Table 5 :</head><label>5</label><figDesc>C avg yielded by the most competitive systems presented to Albayzin 2008 LRE.</figDesc><table>C avg (30-second segments) 
F: free development, R: restricted development, O: open-set, C: closed-set 
Competition 
FO 
RO 
FC 
RC 
System 
primary primary contrastive primary primary 
contrastive 
Site A 
0.0946 
0.1313 
0.1110 
0.0552 
0.0778 
0.0656 
Site B 
0.1204 
0.2787 
-
0.0556 
0.2420 
-

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6" validated="true"><head>Table 6 :</head><label>6</label><figDesc>Performance (C avg ) of single and fused language recognition systems on the closed-set evaluation subset of 30-second speech segments of KALAKA.</figDesc><table>C avg 
GMM-SVM 
0.1611 
PHONE (CH) -LM 
0.1545 
PHONE (HU) -LM 
0.1427 
Single systems PHONE (RU) -LM 
0.1305 
PHONE (CH) -SVM 0.0940 
PHONE (HU) -SVM 0.1017 
PHONE (RU) -SVM 0.1215 
PHONE -LM 
0.0892 
PHONE -SVM 
0.0774 
Fused systems 
PHONE 
0.0691 
ALL 
0.0576 

</table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">http://www.itl.nist.gov/iad/mig/tools/DETware v2.1.targz.htm</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Score normalization for text-independent speaker verification systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Auckenthaler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Carey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lloyd-Thomas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Digital Signal Processing</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="issue">1-3</biblScope>
			<biblScope unit="page" from="42" to="54" />
			<date type="published" when="2000-01" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Hearch: A Multilingual Spoken Document Retrieval System</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Bordel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Diez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Landera</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Nieto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Penagarikano</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">J</forename><surname>Rodriguez-Fuentes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Varona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zamalloa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Automatic Speech Recognition and Understanding Workshop</title>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
	<note>demo</note>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Application-Independent Evaluation of Speaker Detection. Computer, Speech and Language</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Brümmer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Du Preez</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2006-04" />
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="page" from="230" to="275" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">On calibration of language recognition scores</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Brümmer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">A</forename><surname>Van Leeuwen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Odyssey -The Speaker and Language Recognition Workshop</title>
		<meeting>Odyssey -The Speaker and Language Recognition Workshop</meeting>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Fusion of heterogeneous speaker recognition systems in the STBU submission for the NIST speaker recognition evaluation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Brümmer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Burget</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Cernocky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Glembek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Grezl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Karafiat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Van Leeuwen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Matejka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Schwarz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Strasheim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Audio, Speech, and Language Processing</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="2072" to="2084" />
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Support vector machines for speaker and language recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">M</forename><surname>Campbell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">P</forename><surname>Campbell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">A</forename><surname>Reynolds</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Singer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">A</forename><surname>Torres-Carrasquillo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computer Speech and Language</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">2-3</biblScope>
			<biblScope unit="page" from="210" to="229" />
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">SVM Based Speaker Verification Using a GMM Supervector Kernel and NAP Variability Compensation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Campbell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">E</forename><surname>Sturim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">A</forename><surname>Reynolds</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Solomonoff</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ICASSP</title>
		<meeting>ICASSP</meeting>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="97" to="100" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">LIBSVM: a library for support vector machines</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">C</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">J</forename><surname>Lin</surname></persName>
		</author>
		<ptr target="http://www.csie.ntu.edu.tw/∼cjlin/libsvm" />
		<imprint>
			<date type="published" when="2001" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">SVM Torch: Support Vector Machines for Large-Scale Regression Problems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Collobert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="143" to="160" />
			<date type="published" when="2001" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Language Recognition Evaluation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">F</forename><surname>Martin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">N</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Odyssey 2008 -The Speaker and Language Recognition Workshop</title>
		<meeting>Odyssey 2008 -The Speaker and Language Recognition Workshop<address><addrLine>Stellenbosch, South Africa</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="volume">16</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">The DET Curve in Assessment of Detection Task Performance</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Martin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Doddington</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Kamm</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ordowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Przybocki</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Eurospeech</title>
		<meeting>Eurospeech</meeting>
		<imprint>
			<date type="published" when="1997" />
			<biblScope unit="page" from="1985" to="1988" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">BUT system description for NIST LRE</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Matejka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Burget</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Glembek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Schwarz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Hubeika</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Fapso</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Plchot</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 2007 NIST Language Recognition Evaluation Workshop</title>
		<meeting>2007 NIST Language Recognition Evaluation Workshop<address><addrLine>Orlando, US</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page" from="1" to="5" />
		</imprint>
		<respStmt>
			<orgName>National Institute of Standards and Technology</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Sautrela: A Highly Modular Open Source Speech Recognition Framework</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Penagarikano</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Bordel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the ASRU Workshop</title>
		<meeting>the ASRU Workshop<address><addrLine>San Juan, Puerto Rico</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2005-12" />
			<biblScope unit="page" from="386" to="391" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Language recognition with discriminative keyword selection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Richardson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Campbell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ICASSP</title>
		<meeting>ICASSP</meeting>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="4145" to="4148" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Phoneme recognition based on long temporal context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Petr</forename><surname>Schwarz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">BUT</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
		<respStmt>
			<orgName>Faculty of Information Technology</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Ph.D. thesis</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">SRILM -an extensible language modeling toolkit</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kare</forename><surname>Sjolander</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonas</forename><surname>Beskow</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Intl. Conf. on Spoken Language Processing</title>
		<meeting>Intl. Conf. on Spoken Language essing<address><addrLine>Beijing, China. A. Stolcke</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2000" />
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page" from="257" to="286" />
		</imprint>
	</monogr>
	<note>Proceedings of ICSLP 2000</note>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">The MITLL NIST LRE 2007 language recognition system</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Torres-Carrasquillo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Singer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">M</forename><surname>Campbell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Gleason</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Mccree</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">A</forename><surname>Reynolds</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Richardson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">E</forename><surname>Sturim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Interspeech</title>
		<meeting>Interspeech</meeting>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="719" to="722" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
